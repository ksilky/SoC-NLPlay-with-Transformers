{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert based Sentiment Classifier",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fdb9db1859e04f2c916a6a73c9e44165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a22a54fc6b904e008fc24a7e45c466db",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18910e15249140a9a6bd72bf0888c1d0",
              "IPY_MODEL_350345db655b498a8f52657a549d2f7d"
            ]
          }
        },
        "a22a54fc6b904e008fc24a7e45c466db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18910e15249140a9a6bd72bf0888c1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9b5ca6944006431b8a1992fecbbb22b8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_41bb0d76461a4f2ba542fc8f2bc893f5"
          }
        },
        "350345db655b498a8f52657a549d2f7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1bb51229d50c491987e04082774ae517",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:02&lt;00:00, 218B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b80abb9569044a6a5e29cb1b5a83fdb"
          }
        },
        "9b5ca6944006431b8a1992fecbbb22b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "41bb0d76461a4f2ba542fc8f2bc893f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bb51229d50c491987e04082774ae517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b80abb9569044a6a5e29cb1b5a83fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb5b720549c44dcc8600c5610453698e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7281155f2c4e44e2bef60364496ab4e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0407279a52e44427855b206f52e329fc",
              "IPY_MODEL_144862e917ab4bf0ad038687fb3d1206"
            ]
          }
        },
        "7281155f2c4e44e2bef60364496ab4e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0407279a52e44427855b206f52e329fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cdeb0111397f4051842aafb3217ec60c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb274800880c4d479d0af97112cb6aa1"
          }
        },
        "144862e917ab4bf0ad038687fb3d1206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bbbcc3493137464586723e4d893b268e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 899k/899k [00:00&lt;00:00, 2.38MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52358a81fe31417990df31ae5fc22d73"
          }
        },
        "cdeb0111397f4051842aafb3217ec60c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb274800880c4d479d0af97112cb6aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbbcc3493137464586723e4d893b268e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52358a81fe31417990df31ae5fc22d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "005a6935ba94461fb3573445207f12c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1a1360c60ed6417ea071d38221d8e2d1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6254e90052a14ec5981998d04589cf65",
              "IPY_MODEL_38b0375a328942e2b8ce56f711de2c02"
            ]
          }
        },
        "1a1360c60ed6417ea071d38221d8e2d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6254e90052a14ec5981998d04589cf65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c8c1a732a2e14320a47a2531760f4a51",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38204927c1864f08a9dbf747107b4edb"
          }
        },
        "38b0375a328942e2b8ce56f711de2c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5cf7390048bb43cd8b726cdf61fa18fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:01&lt;00:00, 287kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3dc88618c1ce4442848411530d0ffdf0"
          }
        },
        "c8c1a732a2e14320a47a2531760f4a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38204927c1864f08a9dbf747107b4edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5cf7390048bb43cd8b726cdf61fa18fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3dc88618c1ce4442848411530d0ffdf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bfe82ade4fe4553b054d02021ca1e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_213529fe17c2432c8b65bf669cd09b8b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e74ec74405b41e1a8ef0693a8a96bc3",
              "IPY_MODEL_670d5674e56e4296baa8814894088d28"
            ]
          }
        },
        "213529fe17c2432c8b65bf669cd09b8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e74ec74405b41e1a8ef0693a8a96bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2fdcfbbd8897420f929d0ce1c12b3c78",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7181aa0425b948af8ee3ed909885e4d9"
          }
        },
        "670d5674e56e4296baa8814894088d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db4377b557804da1b9f821cc566736a5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 1.69MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00d3bfcbc1e448659758d0e45bc86cc5"
          }
        },
        "2fdcfbbd8897420f929d0ce1c12b3c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7181aa0425b948af8ee3ed909885e4d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db4377b557804da1b9f821cc566736a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00d3bfcbc1e448659758d0e45bc86cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eij714pJ9xl9"
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf\n",
        "from sklearn import feature_extraction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYgQ_ue6IamE",
        "outputId": "77cfb7bb-523a-4df0-96de-398300f384db"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "uXy1z_ho96qd",
        "outputId": "112592d7-93b6-41a0-a289-92acec241387"
      },
      "source": [
        "dataset = pd.read_csv('/content/gdrive/My Drive/NLPlay with Transformers/Sentiment analysis/IMDB Dataset.csv')\n",
        "dataset.info()\n",
        "dataset.head(n=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ML1dJPucZfca",
        "outputId": "bd79edf9-f922-4a72-b5f1-c50247fd134e"
      },
      "source": [
        "one_hot = pd.get_dummies(dataset['sentiment'])\n",
        "dataset = dataset.drop('sentiment', axis=1) \n",
        "dataset = dataset.join(one_hot)\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>negative</th>\n",
              "      <th>positive</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  negative  positive\n",
              "0  One of the other reviewers has mentioned that ...         0         1\n",
              "1  A wonderful little production. <br /><br />The...         0         1\n",
              "2  I thought this was a wonderful way to spend ti...         0         1\n",
              "3  Basically there's a family where a little boy ...         1         0\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...         0         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_kTxhzDWh4Z"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X =dataset.iloc[:, 0].values\n",
        "Y = dataset.iloc[:, 2].values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ayodEOV5IS"
      },
      "source": [
        "Tranformers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XnLPMZiB9aj",
        "outputId": "8ef2e3ec-27da-461c-bae4-b999f52b1698"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/d5/c6c23ad75491467a9a84e526ef2364e523d45e2b0fae28a7cbe8689e7e84/transformers-4.8.1-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.5MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 901kB 33.0MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3MB 42.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmWI3KaLWI0r"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78yOZOiUvLQM"
      },
      "source": [
        "dataset=[]\n",
        "for i in range(len(X)):\n",
        "  encoding = tokenizer.encode_plus(\n",
        "  X[i],\n",
        "  truncation=True,\n",
        "  max_length=64,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "  dataset.append({\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'targets': torch.tensor(Y[i], dtype=torch.long)})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuEsTwrt7kpb",
        "outputId": "0b447251-3970-43c0-8556-d55980263002"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test= train_test_split(dataset, shuffle=True, random_state=60, test_size=0.20)\n",
        "print(\"Number of train samples: \", len(X_train))\n",
        "print(\"Number of test samples: \", len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train samples:  40000\n",
            "Number of test samples:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtjM9cOU5rZL"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(X_train, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(X_test, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfeq3ad406Q0"
      },
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(BertClassifier, self).__init__()\n",
        "    self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "    self.drop = nn.Dropout(0.5)\n",
        "    self.x= nn.Linear(self.bert.config.hidden_size, 1)\n",
        "    self.out=nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,return_dict=False)\n",
        "    out1= self.drop(pooled_output)\n",
        "    out2=self.x(out1)\n",
        "    return self.out(out2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC85MPxu1cCT",
        "outputId": "a655b192-3381-485e-8347-9dbcc164dab7"
      },
      "source": [
        "model = BertClassifier()\n",
        "n_epocs=3\n",
        "n_samples=40000\n",
        "batch_size=32\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6ZQLtG218q_",
        "outputId": "a22f820a-16f6-4f9c-b730-c00264c0f09a"
      },
      "source": [
        "for n in range(1,n_epocs):\n",
        "  correct=0\n",
        "  train_loss=0\n",
        "  batch=0\n",
        "  for d in train_dataloader:\n",
        "    batch+=1\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = d[\"input_ids\"]\n",
        "    attention_mask = d[\"attention_mask\"]\n",
        "    target = d[\"targets\"]\n",
        "    target=target.to(torch.float32)\n",
        "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = criterion(output[:,0],target)\n",
        "    with torch.no_grad():\n",
        "      train_loss+=loss\n",
        "      output = (output>0.5).float()\n",
        "      correct += ((output[:,0] == target).float()).sum()\n",
        "      print(\"batches_trained: \" + str(batch)+\" total_loss: \"+str(train_loss/(batch*batch_size))+\" acc: \"+str(correct/(batch*batch_size)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch completed: \" + str(n)+\" total_loss: \"+str(train_loss)+\" acc: \"+str(correct/n_samples))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batches_trained: 1 total_loss: tensor(0.0234) acc: tensor(0.4062)\n",
            "batches_trained: 2 total_loss: tensor(0.0228) acc: tensor(0.4844)\n",
            "batches_trained: 3 total_loss: tensor(0.0229) acc: tensor(0.4271)\n",
            "batches_trained: 4 total_loss: tensor(0.0227) acc: tensor(0.4531)\n",
            "batches_trained: 5 total_loss: tensor(0.0225) acc: tensor(0.4625)\n",
            "batches_trained: 6 total_loss: tensor(0.0223) acc: tensor(0.4792)\n",
            "batches_trained: 7 total_loss: tensor(0.0223) acc: tensor(0.4777)\n",
            "batches_trained: 8 total_loss: tensor(0.0223) acc: tensor(0.4922)\n",
            "batches_trained: 9 total_loss: tensor(0.0224) acc: tensor(0.5000)\n",
            "batches_trained: 10 total_loss: tensor(0.0224) acc: tensor(0.4938)\n",
            "batches_trained: 11 total_loss: tensor(0.0224) acc: tensor(0.5000)\n",
            "batches_trained: 12 total_loss: tensor(0.0225) acc: tensor(0.4922)\n",
            "batches_trained: 13 total_loss: tensor(0.0223) acc: tensor(0.5024)\n",
            "batches_trained: 14 total_loss: tensor(0.0223) acc: tensor(0.4978)\n",
            "batches_trained: 15 total_loss: tensor(0.0223) acc: tensor(0.4958)\n",
            "batches_trained: 16 total_loss: tensor(0.0223) acc: tensor(0.4863)\n",
            "batches_trained: 17 total_loss: tensor(0.0224) acc: tensor(0.4798)\n",
            "batches_trained: 18 total_loss: tensor(0.0225) acc: tensor(0.4705)\n",
            "batches_trained: 19 total_loss: tensor(0.0224) acc: tensor(0.4770)\n",
            "batches_trained: 20 total_loss: tensor(0.0224) acc: tensor(0.4797)\n",
            "batches_trained: 21 total_loss: tensor(0.0223) acc: tensor(0.4851)\n",
            "batches_trained: 22 total_loss: tensor(0.0222) acc: tensor(0.4901)\n",
            "batches_trained: 23 total_loss: tensor(0.0222) acc: tensor(0.4918)\n",
            "batches_trained: 24 total_loss: tensor(0.0222) acc: tensor(0.4987)\n",
            "batches_trained: 25 total_loss: tensor(0.0221) acc: tensor(0.5063)\n",
            "batches_trained: 26 total_loss: tensor(0.0221) acc: tensor(0.5072)\n",
            "batches_trained: 27 total_loss: tensor(0.0220) acc: tensor(0.5174)\n",
            "batches_trained: 28 total_loss: tensor(0.0219) acc: tensor(0.5201)\n",
            "batches_trained: 29 total_loss: tensor(0.0219) acc: tensor(0.5205)\n",
            "batches_trained: 30 total_loss: tensor(0.0218) acc: tensor(0.5271)\n",
            "batches_trained: 31 total_loss: tensor(0.0217) acc: tensor(0.5353)\n",
            "batches_trained: 32 total_loss: tensor(0.0215) acc: tensor(0.5449)\n",
            "batches_trained: 33 total_loss: tensor(0.0215) acc: tensor(0.5483)\n",
            "batches_trained: 34 total_loss: tensor(0.0214) acc: tensor(0.5515)\n",
            "batches_trained: 35 total_loss: tensor(0.0213) acc: tensor(0.5562)\n",
            "batches_trained: 36 total_loss: tensor(0.0212) acc: tensor(0.5642)\n",
            "batches_trained: 37 total_loss: tensor(0.0211) acc: tensor(0.5642)\n",
            "batches_trained: 38 total_loss: tensor(0.0210) acc: tensor(0.5666)\n",
            "batches_trained: 39 total_loss: tensor(0.0210) acc: tensor(0.5697)\n",
            "batches_trained: 40 total_loss: tensor(0.0209) acc: tensor(0.5734)\n",
            "batches_trained: 41 total_loss: tensor(0.0208) acc: tensor(0.5777)\n",
            "batches_trained: 42 total_loss: tensor(0.0207) acc: tensor(0.5804)\n",
            "batches_trained: 43 total_loss: tensor(0.0208) acc: tensor(0.5807)\n",
            "batches_trained: 44 total_loss: tensor(0.0207) acc: tensor(0.5859)\n",
            "batches_trained: 45 total_loss: tensor(0.0206) acc: tensor(0.5896)\n",
            "batches_trained: 46 total_loss: tensor(0.0205) acc: tensor(0.5944)\n",
            "batches_trained: 47 total_loss: tensor(0.0204) acc: tensor(0.5984)\n",
            "batches_trained: 48 total_loss: tensor(0.0203) acc: tensor(0.5990)\n",
            "batches_trained: 49 total_loss: tensor(0.0202) acc: tensor(0.6027)\n",
            "batches_trained: 50 total_loss: tensor(0.0203) acc: tensor(0.6012)\n",
            "batches_trained: 51 total_loss: tensor(0.0202) acc: tensor(0.6048)\n",
            "batches_trained: 52 total_loss: tensor(0.0200) acc: tensor(0.6100)\n",
            "batches_trained: 53 total_loss: tensor(0.0198) acc: tensor(0.6162)\n",
            "batches_trained: 54 total_loss: tensor(0.0197) acc: tensor(0.6221)\n",
            "batches_trained: 55 total_loss: tensor(0.0196) acc: tensor(0.6233)\n",
            "batches_trained: 56 total_loss: tensor(0.0195) acc: tensor(0.6267)\n",
            "batches_trained: 57 total_loss: tensor(0.0194) acc: tensor(0.6305)\n",
            "batches_trained: 58 total_loss: tensor(0.0194) acc: tensor(0.6320)\n",
            "batches_trained: 59 total_loss: tensor(0.0193) acc: tensor(0.6351)\n",
            "batches_trained: 60 total_loss: tensor(0.0191) acc: tensor(0.6396)\n",
            "batches_trained: 61 total_loss: tensor(0.0190) acc: tensor(0.6414)\n",
            "batches_trained: 62 total_loss: tensor(0.0191) acc: tensor(0.6421)\n",
            "batches_trained: 63 total_loss: tensor(0.0190) acc: tensor(0.6443)\n",
            "batches_trained: 64 total_loss: tensor(0.0190) acc: tensor(0.6470)\n",
            "batches_trained: 65 total_loss: tensor(0.0188) acc: tensor(0.6510)\n",
            "batches_trained: 66 total_loss: tensor(0.0187) acc: tensor(0.6544)\n",
            "batches_trained: 67 total_loss: tensor(0.0186) acc: tensor(0.6567)\n",
            "batches_trained: 68 total_loss: tensor(0.0185) acc: tensor(0.6604)\n",
            "batches_trained: 69 total_loss: tensor(0.0185) acc: tensor(0.6617)\n",
            "batches_trained: 70 total_loss: tensor(0.0184) acc: tensor(0.6629)\n",
            "batches_trained: 71 total_loss: tensor(0.0184) acc: tensor(0.6642)\n",
            "batches_trained: 72 total_loss: tensor(0.0184) acc: tensor(0.6658)\n",
            "batches_trained: 73 total_loss: tensor(0.0184) acc: tensor(0.6674)\n",
            "batches_trained: 74 total_loss: tensor(0.0183) acc: tensor(0.6689)\n",
            "batches_trained: 75 total_loss: tensor(0.0182) acc: tensor(0.6696)\n",
            "batches_trained: 76 total_loss: tensor(0.0182) acc: tensor(0.6719)\n",
            "batches_trained: 77 total_loss: tensor(0.0181) acc: tensor(0.6737)\n",
            "batches_trained: 78 total_loss: tensor(0.0181) acc: tensor(0.6755)\n",
            "batches_trained: 79 total_loss: tensor(0.0181) acc: tensor(0.6752)\n",
            "batches_trained: 80 total_loss: tensor(0.0180) acc: tensor(0.6762)\n",
            "batches_trained: 81 total_loss: tensor(0.0180) acc: tensor(0.6775)\n",
            "batches_trained: 82 total_loss: tensor(0.0179) acc: tensor(0.6784)\n",
            "batches_trained: 83 total_loss: tensor(0.0179) acc: tensor(0.6796)\n",
            "batches_trained: 84 total_loss: tensor(0.0178) acc: tensor(0.6812)\n",
            "batches_trained: 85 total_loss: tensor(0.0178) acc: tensor(0.6820)\n",
            "batches_trained: 86 total_loss: tensor(0.0178) acc: tensor(0.6831)\n",
            "batches_trained: 87 total_loss: tensor(0.0178) acc: tensor(0.6839)\n",
            "batches_trained: 88 total_loss: tensor(0.0177) acc: tensor(0.6839)\n",
            "batches_trained: 89 total_loss: tensor(0.0177) acc: tensor(0.6857)\n",
            "batches_trained: 90 total_loss: tensor(0.0176) acc: tensor(0.6872)\n",
            "batches_trained: 91 total_loss: tensor(0.0175) acc: tensor(0.6892)\n",
            "batches_trained: 92 total_loss: tensor(0.0175) acc: tensor(0.6906)\n",
            "batches_trained: 93 total_loss: tensor(0.0175) acc: tensor(0.6915)\n",
            "batches_trained: 94 total_loss: tensor(0.0175) acc: tensor(0.6918)\n",
            "batches_trained: 95 total_loss: tensor(0.0174) acc: tensor(0.6934)\n",
            "batches_trained: 96 total_loss: tensor(0.0174) acc: tensor(0.6924)\n",
            "batches_trained: 97 total_loss: tensor(0.0174) acc: tensor(0.6936)\n",
            "batches_trained: 98 total_loss: tensor(0.0174) acc: tensor(0.6936)\n",
            "batches_trained: 99 total_loss: tensor(0.0174) acc: tensor(0.6941)\n",
            "batches_trained: 100 total_loss: tensor(0.0173) acc: tensor(0.6963)\n",
            "batches_trained: 101 total_loss: tensor(0.0173) acc: tensor(0.6968)\n",
            "batches_trained: 102 total_loss: tensor(0.0173) acc: tensor(0.6970)\n",
            "batches_trained: 103 total_loss: tensor(0.0172) acc: tensor(0.6972)\n",
            "batches_trained: 104 total_loss: tensor(0.0172) acc: tensor(0.6983)\n",
            "batches_trained: 105 total_loss: tensor(0.0172) acc: tensor(0.6997)\n",
            "batches_trained: 106 total_loss: tensor(0.0172) acc: tensor(0.6999)\n",
            "batches_trained: 107 total_loss: tensor(0.0171) acc: tensor(0.7009)\n",
            "batches_trained: 108 total_loss: tensor(0.0171) acc: tensor(0.7028)\n",
            "batches_trained: 109 total_loss: tensor(0.0171) acc: tensor(0.7030)\n",
            "batches_trained: 110 total_loss: tensor(0.0171) acc: tensor(0.7026)\n",
            "batches_trained: 111 total_loss: tensor(0.0171) acc: tensor(0.7033)\n",
            "batches_trained: 112 total_loss: tensor(0.0170) acc: tensor(0.7045)\n",
            "batches_trained: 113 total_loss: tensor(0.0170) acc: tensor(0.7049)\n",
            "batches_trained: 114 total_loss: tensor(0.0170) acc: tensor(0.7050)\n",
            "batches_trained: 115 total_loss: tensor(0.0170) acc: tensor(0.7054)\n",
            "batches_trained: 116 total_loss: tensor(0.0170) acc: tensor(0.7053)\n",
            "batches_trained: 117 total_loss: tensor(0.0169) acc: tensor(0.7059)\n",
            "batches_trained: 118 total_loss: tensor(0.0169) acc: tensor(0.7060)\n",
            "batches_trained: 119 total_loss: tensor(0.0169) acc: tensor(0.7067)\n",
            "batches_trained: 120 total_loss: tensor(0.0169) acc: tensor(0.7073)\n",
            "batches_trained: 121 total_loss: tensor(0.0169) acc: tensor(0.7082)\n",
            "batches_trained: 122 total_loss: tensor(0.0169) acc: tensor(0.7085)\n",
            "batches_trained: 123 total_loss: tensor(0.0168) acc: tensor(0.7099)\n",
            "batches_trained: 124 total_loss: tensor(0.0168) acc: tensor(0.7102)\n",
            "batches_trained: 125 total_loss: tensor(0.0168) acc: tensor(0.7110)\n",
            "batches_trained: 126 total_loss: tensor(0.0167) acc: tensor(0.7121)\n",
            "batches_trained: 127 total_loss: tensor(0.0167) acc: tensor(0.7131)\n",
            "batches_trained: 128 total_loss: tensor(0.0167) acc: tensor(0.7134)\n",
            "batches_trained: 129 total_loss: tensor(0.0167) acc: tensor(0.7139)\n",
            "batches_trained: 130 total_loss: tensor(0.0167) acc: tensor(0.7142)\n",
            "batches_trained: 131 total_loss: tensor(0.0166) acc: tensor(0.7149)\n",
            "batches_trained: 132 total_loss: tensor(0.0166) acc: tensor(0.7161)\n",
            "batches_trained: 133 total_loss: tensor(0.0166) acc: tensor(0.7169)\n",
            "batches_trained: 134 total_loss: tensor(0.0165) acc: tensor(0.7178)\n",
            "batches_trained: 135 total_loss: tensor(0.0165) acc: tensor(0.7185)\n",
            "batches_trained: 136 total_loss: tensor(0.0164) acc: tensor(0.7192)\n",
            "batches_trained: 137 total_loss: tensor(0.0165) acc: tensor(0.7188)\n",
            "batches_trained: 138 total_loss: tensor(0.0165) acc: tensor(0.7197)\n",
            "batches_trained: 139 total_loss: tensor(0.0164) acc: tensor(0.7212)\n",
            "batches_trained: 140 total_loss: tensor(0.0164) acc: tensor(0.7217)\n",
            "batches_trained: 141 total_loss: tensor(0.0164) acc: tensor(0.7221)\n",
            "batches_trained: 142 total_loss: tensor(0.0164) acc: tensor(0.7223)\n",
            "batches_trained: 143 total_loss: tensor(0.0164) acc: tensor(0.7225)\n",
            "batches_trained: 144 total_loss: tensor(0.0164) acc: tensor(0.7233)\n",
            "batches_trained: 145 total_loss: tensor(0.0164) acc: tensor(0.7237)\n",
            "batches_trained: 146 total_loss: tensor(0.0163) acc: tensor(0.7247)\n",
            "batches_trained: 147 total_loss: tensor(0.0163) acc: tensor(0.7247)\n",
            "batches_trained: 148 total_loss: tensor(0.0163) acc: tensor(0.7251)\n",
            "batches_trained: 149 total_loss: tensor(0.0163) acc: tensor(0.7253)\n",
            "batches_trained: 150 total_loss: tensor(0.0163) acc: tensor(0.7252)\n",
            "batches_trained: 151 total_loss: tensor(0.0162) acc: tensor(0.7256)\n",
            "batches_trained: 152 total_loss: tensor(0.0162) acc: tensor(0.7262)\n",
            "batches_trained: 153 total_loss: tensor(0.0162) acc: tensor(0.7269)\n",
            "batches_trained: 154 total_loss: tensor(0.0162) acc: tensor(0.7269)\n",
            "batches_trained: 155 total_loss: tensor(0.0162) acc: tensor(0.7272)\n",
            "batches_trained: 156 total_loss: tensor(0.0161) acc: tensor(0.7278)\n",
            "batches_trained: 157 total_loss: tensor(0.0161) acc: tensor(0.7281)\n",
            "batches_trained: 158 total_loss: tensor(0.0161) acc: tensor(0.7294)\n",
            "batches_trained: 159 total_loss: tensor(0.0160) acc: tensor(0.7298)\n",
            "batches_trained: 160 total_loss: tensor(0.0160) acc: tensor(0.7309)\n",
            "batches_trained: 161 total_loss: tensor(0.0160) acc: tensor(0.7314)\n",
            "batches_trained: 162 total_loss: tensor(0.0160) acc: tensor(0.7321)\n",
            "batches_trained: 163 total_loss: tensor(0.0160) acc: tensor(0.7327)\n",
            "batches_trained: 164 total_loss: tensor(0.0160) acc: tensor(0.7325)\n",
            "batches_trained: 165 total_loss: tensor(0.0160) acc: tensor(0.7337)\n",
            "batches_trained: 166 total_loss: tensor(0.0159) acc: tensor(0.7346)\n",
            "batches_trained: 167 total_loss: tensor(0.0159) acc: tensor(0.7348)\n",
            "batches_trained: 168 total_loss: tensor(0.0159) acc: tensor(0.7351)\n",
            "batches_trained: 169 total_loss: tensor(0.0159) acc: tensor(0.7352)\n",
            "batches_trained: 170 total_loss: tensor(0.0159) acc: tensor(0.7346)\n",
            "batches_trained: 171 total_loss: tensor(0.0159) acc: tensor(0.7352)\n",
            "batches_trained: 172 total_loss: tensor(0.0158) acc: tensor(0.7360)\n",
            "batches_trained: 173 total_loss: tensor(0.0158) acc: tensor(0.7366)\n",
            "batches_trained: 174 total_loss: tensor(0.0158) acc: tensor(0.7372)\n",
            "batches_trained: 175 total_loss: tensor(0.0157) acc: tensor(0.7380)\n",
            "batches_trained: 176 total_loss: tensor(0.0157) acc: tensor(0.7388)\n",
            "batches_trained: 177 total_loss: tensor(0.0157) acc: tensor(0.7398)\n",
            "batches_trained: 178 total_loss: tensor(0.0156) acc: tensor(0.7400)\n",
            "batches_trained: 179 total_loss: tensor(0.0156) acc: tensor(0.7400)\n",
            "batches_trained: 180 total_loss: tensor(0.0156) acc: tensor(0.7408)\n",
            "batches_trained: 181 total_loss: tensor(0.0156) acc: tensor(0.7407)\n",
            "batches_trained: 182 total_loss: tensor(0.0156) acc: tensor(0.7407)\n",
            "batches_trained: 183 total_loss: tensor(0.0156) acc: tensor(0.7413)\n",
            "batches_trained: 184 total_loss: tensor(0.0156) acc: tensor(0.7418)\n",
            "batches_trained: 185 total_loss: tensor(0.0156) acc: tensor(0.7421)\n",
            "batches_trained: 186 total_loss: tensor(0.0155) acc: tensor(0.7429)\n",
            "batches_trained: 187 total_loss: tensor(0.0156) acc: tensor(0.7428)\n",
            "batches_trained: 188 total_loss: tensor(0.0156) acc: tensor(0.7429)\n",
            "batches_trained: 189 total_loss: tensor(0.0155) acc: tensor(0.7431)\n",
            "batches_trained: 190 total_loss: tensor(0.0155) acc: tensor(0.7437)\n",
            "batches_trained: 191 total_loss: tensor(0.0155) acc: tensor(0.7446)\n",
            "batches_trained: 192 total_loss: tensor(0.0154) acc: tensor(0.7454)\n",
            "batches_trained: 193 total_loss: tensor(0.0154) acc: tensor(0.7455)\n",
            "batches_trained: 194 total_loss: tensor(0.0154) acc: tensor(0.7457)\n",
            "batches_trained: 195 total_loss: tensor(0.0154) acc: tensor(0.7457)\n",
            "batches_trained: 196 total_loss: tensor(0.0154) acc: tensor(0.7463)\n",
            "batches_trained: 197 total_loss: tensor(0.0153) acc: tensor(0.7471)\n",
            "batches_trained: 198 total_loss: tensor(0.0153) acc: tensor(0.7476)\n",
            "batches_trained: 199 total_loss: tensor(0.0153) acc: tensor(0.7475)\n",
            "batches_trained: 200 total_loss: tensor(0.0153) acc: tensor(0.7486)\n",
            "batches_trained: 201 total_loss: tensor(0.0153) acc: tensor(0.7489)\n",
            "batches_trained: 202 total_loss: tensor(0.0153) acc: tensor(0.7492)\n",
            "batches_trained: 203 total_loss: tensor(0.0152) acc: tensor(0.7500)\n",
            "batches_trained: 204 total_loss: tensor(0.0152) acc: tensor(0.7503)\n",
            "batches_trained: 205 total_loss: tensor(0.0152) acc: tensor(0.7509)\n",
            "batches_trained: 206 total_loss: tensor(0.0152) acc: tensor(0.7502)\n",
            "batches_trained: 207 total_loss: tensor(0.0152) acc: tensor(0.7506)\n",
            "batches_trained: 208 total_loss: tensor(0.0152) acc: tensor(0.7505)\n",
            "batches_trained: 209 total_loss: tensor(0.0152) acc: tensor(0.7507)\n",
            "batches_trained: 210 total_loss: tensor(0.0152) acc: tensor(0.7507)\n",
            "batches_trained: 211 total_loss: tensor(0.0152) acc: tensor(0.7509)\n",
            "batches_trained: 212 total_loss: tensor(0.0152) acc: tensor(0.7512)\n",
            "batches_trained: 213 total_loss: tensor(0.0151) acc: tensor(0.7515)\n",
            "batches_trained: 214 total_loss: tensor(0.0151) acc: tensor(0.7516)\n",
            "batches_trained: 215 total_loss: tensor(0.0151) acc: tensor(0.7517)\n",
            "batches_trained: 216 total_loss: tensor(0.0152) acc: tensor(0.7516)\n",
            "batches_trained: 217 total_loss: tensor(0.0151) acc: tensor(0.7520)\n",
            "batches_trained: 218 total_loss: tensor(0.0151) acc: tensor(0.7526)\n",
            "batches_trained: 219 total_loss: tensor(0.0151) acc: tensor(0.7521)\n",
            "batches_trained: 220 total_loss: tensor(0.0151) acc: tensor(0.7523)\n",
            "batches_trained: 221 total_loss: tensor(0.0151) acc: tensor(0.7531)\n",
            "batches_trained: 222 total_loss: tensor(0.0151) acc: tensor(0.7534)\n",
            "batches_trained: 223 total_loss: tensor(0.0151) acc: tensor(0.7532)\n",
            "batches_trained: 224 total_loss: tensor(0.0151) acc: tensor(0.7532)\n",
            "batches_trained: 225 total_loss: tensor(0.0151) acc: tensor(0.7536)\n",
            "batches_trained: 226 total_loss: tensor(0.0151) acc: tensor(0.7535)\n",
            "batches_trained: 227 total_loss: tensor(0.0150) acc: tensor(0.7536)\n",
            "batches_trained: 228 total_loss: tensor(0.0150) acc: tensor(0.7538)\n",
            "batches_trained: 229 total_loss: tensor(0.0150) acc: tensor(0.7541)\n",
            "batches_trained: 230 total_loss: tensor(0.0150) acc: tensor(0.7542)\n",
            "batches_trained: 231 total_loss: tensor(0.0150) acc: tensor(0.7546)\n",
            "batches_trained: 232 total_loss: tensor(0.0150) acc: tensor(0.7550)\n",
            "batches_trained: 233 total_loss: tensor(0.0150) acc: tensor(0.7550)\n",
            "batches_trained: 234 total_loss: tensor(0.0150) acc: tensor(0.7555)\n",
            "batches_trained: 235 total_loss: tensor(0.0150) acc: tensor(0.7556)\n",
            "batches_trained: 236 total_loss: tensor(0.0150) acc: tensor(0.7564)\n",
            "batches_trained: 237 total_loss: tensor(0.0149) acc: tensor(0.7567)\n",
            "batches_trained: 238 total_loss: tensor(0.0149) acc: tensor(0.7568)\n",
            "batches_trained: 239 total_loss: tensor(0.0149) acc: tensor(0.7572)\n",
            "batches_trained: 240 total_loss: tensor(0.0149) acc: tensor(0.7573)\n",
            "batches_trained: 241 total_loss: tensor(0.0149) acc: tensor(0.7575)\n",
            "batches_trained: 242 total_loss: tensor(0.0149) acc: tensor(0.7575)\n",
            "batches_trained: 243 total_loss: tensor(0.0149) acc: tensor(0.7578)\n",
            "batches_trained: 244 total_loss: tensor(0.0149) acc: tensor(0.7579)\n",
            "batches_trained: 245 total_loss: tensor(0.0149) acc: tensor(0.7583)\n",
            "batches_trained: 246 total_loss: tensor(0.0149) acc: tensor(0.7581)\n",
            "batches_trained: 247 total_loss: tensor(0.0149) acc: tensor(0.7582)\n",
            "batches_trained: 248 total_loss: tensor(0.0149) acc: tensor(0.7586)\n",
            "batches_trained: 249 total_loss: tensor(0.0149) acc: tensor(0.7589)\n",
            "batches_trained: 250 total_loss: tensor(0.0149) acc: tensor(0.7592)\n",
            "batches_trained: 251 total_loss: tensor(0.0149) acc: tensor(0.7593)\n",
            "batches_trained: 252 total_loss: tensor(0.0149) acc: tensor(0.7597)\n",
            "batches_trained: 253 total_loss: tensor(0.0148) acc: tensor(0.7600)\n",
            "batches_trained: 254 total_loss: tensor(0.0148) acc: tensor(0.7603)\n",
            "batches_trained: 255 total_loss: tensor(0.0148) acc: tensor(0.7605)\n",
            "batches_trained: 256 total_loss: tensor(0.0148) acc: tensor(0.7612)\n",
            "batches_trained: 257 total_loss: tensor(0.0148) acc: tensor(0.7613)\n",
            "batches_trained: 258 total_loss: tensor(0.0148) acc: tensor(0.7615)\n",
            "batches_trained: 259 total_loss: tensor(0.0147) acc: tensor(0.7618)\n",
            "batches_trained: 260 total_loss: tensor(0.0147) acc: tensor(0.7624)\n",
            "batches_trained: 261 total_loss: tensor(0.0147) acc: tensor(0.7625)\n",
            "batches_trained: 262 total_loss: tensor(0.0147) acc: tensor(0.7625)\n",
            "batches_trained: 263 total_loss: tensor(0.0147) acc: tensor(0.7631)\n",
            "batches_trained: 264 total_loss: tensor(0.0147) acc: tensor(0.7635)\n",
            "batches_trained: 265 total_loss: tensor(0.0147) acc: tensor(0.7639)\n",
            "batches_trained: 266 total_loss: tensor(0.0147) acc: tensor(0.7640)\n",
            "batches_trained: 267 total_loss: tensor(0.0146) acc: tensor(0.7640)\n",
            "batches_trained: 268 total_loss: tensor(0.0146) acc: tensor(0.7641)\n",
            "batches_trained: 269 total_loss: tensor(0.0146) acc: tensor(0.7643)\n",
            "batches_trained: 270 total_loss: tensor(0.0146) acc: tensor(0.7642)\n",
            "batches_trained: 271 total_loss: tensor(0.0146) acc: tensor(0.7645)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCh1S8fGm93w",
        "outputId": "4ca5703d-d9fb-49f4-ff01-1bb042bd0dbd"
      },
      "source": [
        "for n in range(1,n_epocs):\n",
        "  correct=0\n",
        "  train_loss=0\n",
        "  batch=0\n",
        "  for d in train_dataloader:\n",
        "    batch+=1\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = d[\"input_ids\"]\n",
        "    attention_mask = d[\"attention_mask\"]\n",
        "    target = d[\"targets\"]\n",
        "    target=target.to(torch.float32)\n",
        "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = criterion(output[:,0],target)\n",
        "    with torch.no_grad():\n",
        "      train_loss+=loss\n",
        "      output = (output>0.5).float()\n",
        "      correct += ((output[:,0] == target).float()).sum()\n",
        "      print(\"batches_trained: \" + str(batch)+\" total_loss: \"+str(train_loss/(batch*batch_size))+\" acc: \"+str(correct/(batch*batch_size)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch completed: \" + str(n)+\" total_loss: \"+str(train_loss)+\" acc: \"+str(correct/n_samples))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batches_trained: 1 total_loss: tensor(0.0234) acc: tensor(0.4062)\n",
            "batches_trained: 2 total_loss: tensor(0.0228) acc: tensor(0.4844)\n",
            "batches_trained: 3 total_loss: tensor(0.0229) acc: tensor(0.4271)\n",
            "batches_trained: 4 total_loss: tensor(0.0227) acc: tensor(0.4531)\n",
            "batches_trained: 5 total_loss: tensor(0.0225) acc: tensor(0.4625)\n",
            "batches_trained: 6 total_loss: tensor(0.0223) acc: tensor(0.4792)\n",
            "batches_trained: 7 total_loss: tensor(0.0223) acc: tensor(0.4777)\n",
            "batches_trained: 8 total_loss: tensor(0.0223) acc: tensor(0.4922)\n",
            "batches_trained: 9 total_loss: tensor(0.0224) acc: tensor(0.5000)\n",
            "batches_trained: 10 total_loss: tensor(0.0224) acc: tensor(0.4938)\n",
            "batches_trained: 11 total_loss: tensor(0.0224) acc: tensor(0.5000)\n",
            "batches_trained: 12 total_loss: tensor(0.0225) acc: tensor(0.4922)\n",
            "batches_trained: 13 total_loss: tensor(0.0223) acc: tensor(0.5024)\n",
            "batches_trained: 14 total_loss: tensor(0.0223) acc: tensor(0.4978)\n",
            "batches_trained: 15 total_loss: tensor(0.0223) acc: tensor(0.4958)\n",
            "batches_trained: 16 total_loss: tensor(0.0223) acc: tensor(0.4863)\n",
            "batches_trained: 17 total_loss: tensor(0.0224) acc: tensor(0.4798)\n",
            "batches_trained: 18 total_loss: tensor(0.0225) acc: tensor(0.4705)\n",
            "batches_trained: 19 total_loss: tensor(0.0224) acc: tensor(0.4770)\n",
            "batches_trained: 20 total_loss: tensor(0.0224) acc: tensor(0.4797)\n",
            "batches_trained: 21 total_loss: tensor(0.0223) acc: tensor(0.4851)\n",
            "batches_trained: 22 total_loss: tensor(0.0222) acc: tensor(0.4901)\n",
            "batches_trained: 23 total_loss: tensor(0.0222) acc: tensor(0.4918)\n",
            "batches_trained: 24 total_loss: tensor(0.0222) acc: tensor(0.4987)\n",
            "batches_trained: 25 total_loss: tensor(0.0221) acc: tensor(0.5063)\n",
            "batches_trained: 26 total_loss: tensor(0.0221) acc: tensor(0.5072)\n",
            "batches_trained: 27 total_loss: tensor(0.0220) acc: tensor(0.5174)\n",
            "batches_trained: 28 total_loss: tensor(0.0219) acc: tensor(0.5201)\n",
            "batches_trained: 29 total_loss: tensor(0.0219) acc: tensor(0.5205)\n",
            "batches_trained: 30 total_loss: tensor(0.0218) acc: tensor(0.5271)\n",
            "batches_trained: 31 total_loss: tensor(0.0217) acc: tensor(0.5353)\n",
            "batches_trained: 32 total_loss: tensor(0.0215) acc: tensor(0.5449)\n",
            "batches_trained: 33 total_loss: tensor(0.0215) acc: tensor(0.5483)\n",
            "batches_trained: 34 total_loss: tensor(0.0214) acc: tensor(0.5515)\n",
            "batches_trained: 35 total_loss: tensor(0.0213) acc: tensor(0.5562)\n",
            "batches_trained: 36 total_loss: tensor(0.0212) acc: tensor(0.5642)\n",
            "batches_trained: 37 total_loss: tensor(0.0211) acc: tensor(0.5642)\n",
            "batches_trained: 38 total_loss: tensor(0.0210) acc: tensor(0.5666)\n",
            "batches_trained: 39 total_loss: tensor(0.0210) acc: tensor(0.5697)\n",
            "batches_trained: 40 total_loss: tensor(0.0209) acc: tensor(0.5734)\n",
            "batches_trained: 41 total_loss: tensor(0.0208) acc: tensor(0.5777)\n",
            "batches_trained: 42 total_loss: tensor(0.0207) acc: tensor(0.5804)\n",
            "batches_trained: 43 total_loss: tensor(0.0208) acc: tensor(0.5807)\n",
            "batches_trained: 44 total_loss: tensor(0.0207) acc: tensor(0.5859)\n",
            "batches_trained: 45 total_loss: tensor(0.0206) acc: tensor(0.5896)\n",
            "batches_trained: 46 total_loss: tensor(0.0205) acc: tensor(0.5944)\n",
            "batches_trained: 47 total_loss: tensor(0.0204) acc: tensor(0.5984)\n",
            "batches_trained: 48 total_loss: tensor(0.0203) acc: tensor(0.5990)\n",
            "batches_trained: 49 total_loss: tensor(0.0202) acc: tensor(0.6027)\n",
            "batches_trained: 50 total_loss: tensor(0.0203) acc: tensor(0.6012)\n",
            "batches_trained: 51 total_loss: tensor(0.0202) acc: tensor(0.6048)\n",
            "batches_trained: 52 total_loss: tensor(0.0200) acc: tensor(0.6100)\n",
            "batches_trained: 53 total_loss: tensor(0.0198) acc: tensor(0.6162)\n",
            "batches_trained: 54 total_loss: tensor(0.0197) acc: tensor(0.6221)\n",
            "batches_trained: 55 total_loss: tensor(0.0196) acc: tensor(0.6233)\n",
            "batches_trained: 56 total_loss: tensor(0.0195) acc: tensor(0.6267)\n",
            "batches_trained: 57 total_loss: tensor(0.0194) acc: tensor(0.6305)\n",
            "batches_trained: 58 total_loss: tensor(0.0194) acc: tensor(0.6320)\n",
            "batches_trained: 59 total_loss: tensor(0.0193) acc: tensor(0.6351)\n",
            "batches_trained: 60 total_loss: tensor(0.0191) acc: tensor(0.6396)\n",
            "batches_trained: 61 total_loss: tensor(0.0190) acc: tensor(0.6414)\n",
            "batches_trained: 62 total_loss: tensor(0.0191) acc: tensor(0.6421)\n",
            "batches_trained: 63 total_loss: tensor(0.0190) acc: tensor(0.6443)\n",
            "batches_trained: 64 total_loss: tensor(0.0190) acc: tensor(0.6470)\n",
            "batches_trained: 65 total_loss: tensor(0.0188) acc: tensor(0.6510)\n",
            "batches_trained: 66 total_loss: tensor(0.0187) acc: tensor(0.6544)\n",
            "batches_trained: 67 total_loss: tensor(0.0186) acc: tensor(0.6567)\n",
            "batches_trained: 68 total_loss: tensor(0.0185) acc: tensor(0.6604)\n",
            "batches_trained: 69 total_loss: tensor(0.0185) acc: tensor(0.6617)\n",
            "batches_trained: 70 total_loss: tensor(0.0184) acc: tensor(0.6629)\n",
            "batches_trained: 71 total_loss: tensor(0.0184) acc: tensor(0.6642)\n",
            "batches_trained: 72 total_loss: tensor(0.0184) acc: tensor(0.6658)\n",
            "batches_trained: 73 total_loss: tensor(0.0184) acc: tensor(0.6674)\n",
            "batches_trained: 74 total_loss: tensor(0.0183) acc: tensor(0.6689)\n",
            "batches_trained: 75 total_loss: tensor(0.0182) acc: tensor(0.6696)\n",
            "batches_trained: 76 total_loss: tensor(0.0182) acc: tensor(0.6719)\n",
            "batches_trained: 77 total_loss: tensor(0.0181) acc: tensor(0.6737)\n",
            "batches_trained: 78 total_loss: tensor(0.0181) acc: tensor(0.6755)\n",
            "batches_trained: 79 total_loss: tensor(0.0181) acc: tensor(0.6752)\n",
            "batches_trained: 80 total_loss: tensor(0.0180) acc: tensor(0.6762)\n",
            "batches_trained: 81 total_loss: tensor(0.0180) acc: tensor(0.6775)\n",
            "batches_trained: 82 total_loss: tensor(0.0179) acc: tensor(0.6784)\n",
            "batches_trained: 83 total_loss: tensor(0.0179) acc: tensor(0.6796)\n",
            "batches_trained: 84 total_loss: tensor(0.0178) acc: tensor(0.6812)\n",
            "batches_trained: 85 total_loss: tensor(0.0178) acc: tensor(0.6820)\n",
            "batches_trained: 86 total_loss: tensor(0.0178) acc: tensor(0.6831)\n",
            "batches_trained: 87 total_loss: tensor(0.0178) acc: tensor(0.6839)\n",
            "batches_trained: 88 total_loss: tensor(0.0177) acc: tensor(0.6839)\n",
            "batches_trained: 89 total_loss: tensor(0.0177) acc: tensor(0.6857)\n",
            "batches_trained: 90 total_loss: tensor(0.0176) acc: tensor(0.6872)\n",
            "batches_trained: 91 total_loss: tensor(0.0175) acc: tensor(0.6892)\n",
            "batches_trained: 92 total_loss: tensor(0.0175) acc: tensor(0.6906)\n",
            "batches_trained: 93 total_loss: tensor(0.0175) acc: tensor(0.6915)\n",
            "batches_trained: 94 total_loss: tensor(0.0175) acc: tensor(0.6918)\n",
            "batches_trained: 95 total_loss: tensor(0.0174) acc: tensor(0.6934)\n",
            "batches_trained: 96 total_loss: tensor(0.0174) acc: tensor(0.6924)\n",
            "batches_trained: 97 total_loss: tensor(0.0174) acc: tensor(0.6936)\n",
            "batches_trained: 98 total_loss: tensor(0.0174) acc: tensor(0.6936)\n",
            "batches_trained: 99 total_loss: tensor(0.0174) acc: tensor(0.6941)\n",
            "batches_trained: 100 total_loss: tensor(0.0173) acc: tensor(0.6963)\n",
            "batches_trained: 101 total_loss: tensor(0.0173) acc: tensor(0.6968)\n",
            "batches_trained: 102 total_loss: tensor(0.0173) acc: tensor(0.6970)\n",
            "batches_trained: 103 total_loss: tensor(0.0172) acc: tensor(0.6972)\n",
            "batches_trained: 104 total_loss: tensor(0.0172) acc: tensor(0.6983)\n",
            "batches_trained: 105 total_loss: tensor(0.0172) acc: tensor(0.6997)\n",
            "batches_trained: 106 total_loss: tensor(0.0172) acc: tensor(0.6999)\n",
            "batches_trained: 107 total_loss: tensor(0.0171) acc: tensor(0.7009)\n",
            "batches_trained: 108 total_loss: tensor(0.0171) acc: tensor(0.7028)\n",
            "batches_trained: 109 total_loss: tensor(0.0171) acc: tensor(0.7030)\n",
            "batches_trained: 110 total_loss: tensor(0.0171) acc: tensor(0.7026)\n",
            "batches_trained: 111 total_loss: tensor(0.0171) acc: tensor(0.7033)\n",
            "batches_trained: 112 total_loss: tensor(0.0170) acc: tensor(0.7045)\n",
            "batches_trained: 113 total_loss: tensor(0.0170) acc: tensor(0.7049)\n",
            "batches_trained: 114 total_loss: tensor(0.0170) acc: tensor(0.7050)\n",
            "batches_trained: 115 total_loss: tensor(0.0170) acc: tensor(0.7054)\n",
            "batches_trained: 116 total_loss: tensor(0.0170) acc: tensor(0.7053)\n",
            "batches_trained: 117 total_loss: tensor(0.0169) acc: tensor(0.7059)\n",
            "batches_trained: 118 total_loss: tensor(0.0169) acc: tensor(0.7060)\n",
            "batches_trained: 119 total_loss: tensor(0.0169) acc: tensor(0.7067)\n",
            "batches_trained: 120 total_loss: tensor(0.0169) acc: tensor(0.7073)\n",
            "batches_trained: 121 total_loss: tensor(0.0169) acc: tensor(0.7082)\n",
            "batches_trained: 122 total_loss: tensor(0.0169) acc: tensor(0.7085)\n",
            "batches_trained: 123 total_loss: tensor(0.0168) acc: tensor(0.7099)\n",
            "batches_trained: 124 total_loss: tensor(0.0168) acc: tensor(0.7102)\n",
            "batches_trained: 125 total_loss: tensor(0.0168) acc: tensor(0.7110)\n",
            "batches_trained: 126 total_loss: tensor(0.0167) acc: tensor(0.7121)\n",
            "batches_trained: 127 total_loss: tensor(0.0167) acc: tensor(0.7131)\n",
            "batches_trained: 128 total_loss: tensor(0.0167) acc: tensor(0.7134)\n",
            "batches_trained: 129 total_loss: tensor(0.0167) acc: tensor(0.7139)\n",
            "batches_trained: 130 total_loss: tensor(0.0167) acc: tensor(0.7142)\n",
            "batches_trained: 131 total_loss: tensor(0.0166) acc: tensor(0.7149)\n",
            "batches_trained: 132 total_loss: tensor(0.0166) acc: tensor(0.7161)\n",
            "batches_trained: 133 total_loss: tensor(0.0166) acc: tensor(0.7169)\n",
            "batches_trained: 134 total_loss: tensor(0.0165) acc: tensor(0.7178)\n",
            "batches_trained: 135 total_loss: tensor(0.0165) acc: tensor(0.7185)\n",
            "batches_trained: 136 total_loss: tensor(0.0164) acc: tensor(0.7192)\n",
            "batches_trained: 137 total_loss: tensor(0.0165) acc: tensor(0.7188)\n",
            "batches_trained: 138 total_loss: tensor(0.0165) acc: tensor(0.7197)\n",
            "batches_trained: 139 total_loss: tensor(0.0164) acc: tensor(0.7212)\n",
            "batches_trained: 140 total_loss: tensor(0.0164) acc: tensor(0.7217)\n",
            "batches_trained: 141 total_loss: tensor(0.0164) acc: tensor(0.7221)\n",
            "batches_trained: 142 total_loss: tensor(0.0164) acc: tensor(0.7223)\n",
            "batches_trained: 143 total_loss: tensor(0.0164) acc: tensor(0.7225)\n",
            "batches_trained: 144 total_loss: tensor(0.0164) acc: tensor(0.7233)\n",
            "batches_trained: 145 total_loss: tensor(0.0164) acc: tensor(0.7237)\n",
            "batches_trained: 146 total_loss: tensor(0.0163) acc: tensor(0.7247)\n",
            "batches_trained: 147 total_loss: tensor(0.0163) acc: tensor(0.7247)\n",
            "batches_trained: 148 total_loss: tensor(0.0163) acc: tensor(0.7251)\n",
            "batches_trained: 149 total_loss: tensor(0.0163) acc: tensor(0.7253)\n",
            "batches_trained: 150 total_loss: tensor(0.0163) acc: tensor(0.7252)\n",
            "batches_trained: 151 total_loss: tensor(0.0162) acc: tensor(0.7256)\n",
            "batches_trained: 152 total_loss: tensor(0.0162) acc: tensor(0.7262)\n",
            "batches_trained: 153 total_loss: tensor(0.0162) acc: tensor(0.7269)\n",
            "batches_trained: 154 total_loss: tensor(0.0162) acc: tensor(0.7269)\n",
            "batches_trained: 155 total_loss: tensor(0.0162) acc: tensor(0.7272)\n",
            "batches_trained: 156 total_loss: tensor(0.0161) acc: tensor(0.7278)\n",
            "batches_trained: 157 total_loss: tensor(0.0161) acc: tensor(0.7281)\n",
            "batches_trained: 158 total_loss: tensor(0.0161) acc: tensor(0.7294)\n",
            "batches_trained: 159 total_loss: tensor(0.0160) acc: tensor(0.7298)\n",
            "batches_trained: 160 total_loss: tensor(0.0160) acc: tensor(0.7309)\n",
            "batches_trained: 161 total_loss: tensor(0.0160) acc: tensor(0.7314)\n",
            "batches_trained: 162 total_loss: tensor(0.0160) acc: tensor(0.7321)\n",
            "batches_trained: 163 total_loss: tensor(0.0160) acc: tensor(0.7327)\n",
            "batches_trained: 164 total_loss: tensor(0.0160) acc: tensor(0.7325)\n",
            "batches_trained: 165 total_loss: tensor(0.0160) acc: tensor(0.7337)\n",
            "batches_trained: 166 total_loss: tensor(0.0159) acc: tensor(0.7346)\n",
            "batches_trained: 167 total_loss: tensor(0.0159) acc: tensor(0.7348)\n",
            "batches_trained: 168 total_loss: tensor(0.0159) acc: tensor(0.7351)\n",
            "batches_trained: 169 total_loss: tensor(0.0159) acc: tensor(0.7352)\n",
            "batches_trained: 170 total_loss: tensor(0.0159) acc: tensor(0.7346)\n",
            "batches_trained: 171 total_loss: tensor(0.0159) acc: tensor(0.7352)\n",
            "batches_trained: 172 total_loss: tensor(0.0158) acc: tensor(0.7360)\n",
            "batches_trained: 173 total_loss: tensor(0.0158) acc: tensor(0.7366)\n",
            "batches_trained: 174 total_loss: tensor(0.0158) acc: tensor(0.7372)\n",
            "batches_trained: 175 total_loss: tensor(0.0157) acc: tensor(0.7380)\n",
            "batches_trained: 176 total_loss: tensor(0.0157) acc: tensor(0.7388)\n",
            "batches_trained: 177 total_loss: tensor(0.0157) acc: tensor(0.7398)\n",
            "batches_trained: 178 total_loss: tensor(0.0156) acc: tensor(0.7400)\n",
            "batches_trained: 179 total_loss: tensor(0.0156) acc: tensor(0.7400)\n",
            "batches_trained: 180 total_loss: tensor(0.0156) acc: tensor(0.7408)\n",
            "batches_trained: 181 total_loss: tensor(0.0156) acc: tensor(0.7407)\n",
            "batches_trained: 182 total_loss: tensor(0.0156) acc: tensor(0.7407)\n",
            "batches_trained: 183 total_loss: tensor(0.0156) acc: tensor(0.7413)\n",
            "batches_trained: 184 total_loss: tensor(0.0156) acc: tensor(0.7418)\n",
            "batches_trained: 185 total_loss: tensor(0.0156) acc: tensor(0.7421)\n",
            "batches_trained: 186 total_loss: tensor(0.0155) acc: tensor(0.7429)\n",
            "batches_trained: 187 total_loss: tensor(0.0156) acc: tensor(0.7428)\n",
            "batches_trained: 188 total_loss: tensor(0.0156) acc: tensor(0.7429)\n",
            "batches_trained: 189 total_loss: tensor(0.0155) acc: tensor(0.7431)\n",
            "batches_trained: 190 total_loss: tensor(0.0155) acc: tensor(0.7437)\n",
            "batches_trained: 191 total_loss: tensor(0.0155) acc: tensor(0.7446)\n",
            "batches_trained: 192 total_loss: tensor(0.0154) acc: tensor(0.7454)\n",
            "batches_trained: 193 total_loss: tensor(0.0154) acc: tensor(0.7455)\n",
            "batches_trained: 194 total_loss: tensor(0.0154) acc: tensor(0.7457)\n",
            "batches_trained: 195 total_loss: tensor(0.0154) acc: tensor(0.7457)\n",
            "batches_trained: 196 total_loss: tensor(0.0154) acc: tensor(0.7463)\n",
            "batches_trained: 197 total_loss: tensor(0.0153) acc: tensor(0.7471)\n",
            "batches_trained: 198 total_loss: tensor(0.0153) acc: tensor(0.7476)\n",
            "batches_trained: 199 total_loss: tensor(0.0153) acc: tensor(0.7475)\n",
            "batches_trained: 200 total_loss: tensor(0.0153) acc: tensor(0.7486)\n",
            "batches_trained: 201 total_loss: tensor(0.0153) acc: tensor(0.7489)\n",
            "batches_trained: 202 total_loss: tensor(0.0153) acc: tensor(0.7492)\n",
            "batches_trained: 203 total_loss: tensor(0.0152) acc: tensor(0.7500)\n",
            "batches_trained: 204 total_loss: tensor(0.0152) acc: tensor(0.7503)\n",
            "batches_trained: 205 total_loss: tensor(0.0152) acc: tensor(0.7509)\n",
            "batches_trained: 206 total_loss: tensor(0.0152) acc: tensor(0.7502)\n",
            "batches_trained: 207 total_loss: tensor(0.0152) acc: tensor(0.7506)\n",
            "batches_trained: 208 total_loss: tensor(0.0152) acc: tensor(0.7505)\n",
            "batches_trained: 209 total_loss: tensor(0.0152) acc: tensor(0.7507)\n",
            "batches_trained: 210 total_loss: tensor(0.0152) acc: tensor(0.7507)\n",
            "batches_trained: 211 total_loss: tensor(0.0152) acc: tensor(0.7509)\n",
            "batches_trained: 212 total_loss: tensor(0.0152) acc: tensor(0.7512)\n",
            "batches_trained: 213 total_loss: tensor(0.0151) acc: tensor(0.7515)\n",
            "batches_trained: 214 total_loss: tensor(0.0151) acc: tensor(0.7516)\n",
            "batches_trained: 215 total_loss: tensor(0.0151) acc: tensor(0.7517)\n",
            "batches_trained: 216 total_loss: tensor(0.0152) acc: tensor(0.7516)\n",
            "batches_trained: 217 total_loss: tensor(0.0151) acc: tensor(0.7520)\n",
            "batches_trained: 218 total_loss: tensor(0.0151) acc: tensor(0.7526)\n",
            "batches_trained: 219 total_loss: tensor(0.0151) acc: tensor(0.7521)\n",
            "batches_trained: 220 total_loss: tensor(0.0151) acc: tensor(0.7523)\n",
            "batches_trained: 221 total_loss: tensor(0.0151) acc: tensor(0.7531)\n",
            "batches_trained: 222 total_loss: tensor(0.0151) acc: tensor(0.7534)\n",
            "batches_trained: 223 total_loss: tensor(0.0151) acc: tensor(0.7532)\n",
            "batches_trained: 224 total_loss: tensor(0.0151) acc: tensor(0.7532)\n",
            "batches_trained: 225 total_loss: tensor(0.0151) acc: tensor(0.7536)\n",
            "batches_trained: 226 total_loss: tensor(0.0151) acc: tensor(0.7535)\n",
            "batches_trained: 227 total_loss: tensor(0.0150) acc: tensor(0.7536)\n",
            "batches_trained: 228 total_loss: tensor(0.0150) acc: tensor(0.7538)\n",
            "batches_trained: 229 total_loss: tensor(0.0150) acc: tensor(0.7541)\n",
            "batches_trained: 230 total_loss: tensor(0.0150) acc: tensor(0.7542)\n",
            "batches_trained: 231 total_loss: tensor(0.0150) acc: tensor(0.7546)\n",
            "batches_trained: 232 total_loss: tensor(0.0150) acc: tensor(0.7550)\n",
            "batches_trained: 233 total_loss: tensor(0.0150) acc: tensor(0.7550)\n",
            "batches_trained: 234 total_loss: tensor(0.0150) acc: tensor(0.7555)\n",
            "batches_trained: 235 total_loss: tensor(0.0150) acc: tensor(0.7556)\n",
            "batches_trained: 236 total_loss: tensor(0.0150) acc: tensor(0.7564)\n",
            "batches_trained: 237 total_loss: tensor(0.0149) acc: tensor(0.7567)\n",
            "batches_trained: 238 total_loss: tensor(0.0149) acc: tensor(0.7568)\n",
            "batches_trained: 239 total_loss: tensor(0.0149) acc: tensor(0.7572)\n",
            "batches_trained: 240 total_loss: tensor(0.0149) acc: tensor(0.7573)\n",
            "batches_trained: 241 total_loss: tensor(0.0149) acc: tensor(0.7575)\n",
            "batches_trained: 242 total_loss: tensor(0.0149) acc: tensor(0.7575)\n",
            "batches_trained: 243 total_loss: tensor(0.0149) acc: tensor(0.7578)\n",
            "batches_trained: 244 total_loss: tensor(0.0149) acc: tensor(0.7579)\n",
            "batches_trained: 245 total_loss: tensor(0.0149) acc: tensor(0.7583)\n",
            "batches_trained: 246 total_loss: tensor(0.0149) acc: tensor(0.7581)\n",
            "batches_trained: 247 total_loss: tensor(0.0149) acc: tensor(0.7582)\n",
            "batches_trained: 248 total_loss: tensor(0.0149) acc: tensor(0.7586)\n",
            "batches_trained: 249 total_loss: tensor(0.0149) acc: tensor(0.7589)\n",
            "batches_trained: 250 total_loss: tensor(0.0149) acc: tensor(0.7592)\n",
            "batches_trained: 251 total_loss: tensor(0.0149) acc: tensor(0.7593)\n",
            "batches_trained: 252 total_loss: tensor(0.0149) acc: tensor(0.7597)\n",
            "batches_trained: 253 total_loss: tensor(0.0148) acc: tensor(0.7600)\n",
            "batches_trained: 254 total_loss: tensor(0.0148) acc: tensor(0.7603)\n",
            "batches_trained: 255 total_loss: tensor(0.0148) acc: tensor(0.7605)\n",
            "batches_trained: 256 total_loss: tensor(0.0148) acc: tensor(0.7612)\n",
            "batches_trained: 257 total_loss: tensor(0.0148) acc: tensor(0.7613)\n",
            "batches_trained: 258 total_loss: tensor(0.0148) acc: tensor(0.7615)\n",
            "batches_trained: 259 total_loss: tensor(0.0147) acc: tensor(0.7618)\n",
            "batches_trained: 260 total_loss: tensor(0.0147) acc: tensor(0.7624)\n",
            "batches_trained: 261 total_loss: tensor(0.0147) acc: tensor(0.7625)\n",
            "batches_trained: 262 total_loss: tensor(0.0147) acc: tensor(0.7625)\n",
            "batches_trained: 263 total_loss: tensor(0.0147) acc: tensor(0.7631)\n",
            "batches_trained: 264 total_loss: tensor(0.0147) acc: tensor(0.7635)\n",
            "batches_trained: 265 total_loss: tensor(0.0147) acc: tensor(0.7639)\n",
            "batches_trained: 266 total_loss: tensor(0.0147) acc: tensor(0.7640)\n",
            "batches_trained: 267 total_loss: tensor(0.0146) acc: tensor(0.7640)\n",
            "batches_trained: 268 total_loss: tensor(0.0146) acc: tensor(0.7641)\n",
            "batches_trained: 269 total_loss: tensor(0.0146) acc: tensor(0.7643)\n",
            "batches_trained: 270 total_loss: tensor(0.0146) acc: tensor(0.7642)\n",
            "batches_trained: 271 total_loss: tensor(0.0146) acc: tensor(0.7645)\n",
            "batches_trained: 272 total_loss: tensor(0.0146) acc: tensor(0.7651)\n",
            "batches_trained: 273 total_loss: tensor(0.0146) acc: tensor(0.7649)\n",
            "batches_trained: 274 total_loss: tensor(0.0146) acc: tensor(0.7655)\n",
            "batches_trained: 275 total_loss: tensor(0.0146) acc: tensor(0.7656)\n",
            "batches_trained: 276 total_loss: tensor(0.0146) acc: tensor(0.7659)\n",
            "batches_trained: 277 total_loss: tensor(0.0146) acc: tensor(0.7664)\n",
            "batches_trained: 278 total_loss: tensor(0.0145) acc: tensor(0.7664)\n",
            "batches_trained: 279 total_loss: tensor(0.0145) acc: tensor(0.7668)\n",
            "batches_trained: 280 total_loss: tensor(0.0145) acc: tensor(0.7670)\n",
            "batches_trained: 281 total_loss: tensor(0.0145) acc: tensor(0.7671)\n",
            "batches_trained: 282 total_loss: tensor(0.0145) acc: tensor(0.7673)\n",
            "batches_trained: 283 total_loss: tensor(0.0145) acc: tensor(0.7674)\n",
            "batches_trained: 284 total_loss: tensor(0.0145) acc: tensor(0.7676)\n",
            "batches_trained: 285 total_loss: tensor(0.0145) acc: tensor(0.7678)\n",
            "batches_trained: 286 total_loss: tensor(0.0145) acc: tensor(0.7685)\n",
            "batches_trained: 287 total_loss: tensor(0.0145) acc: tensor(0.7685)\n",
            "batches_trained: 288 total_loss: tensor(0.0145) acc: tensor(0.7686)\n",
            "batches_trained: 289 total_loss: tensor(0.0144) acc: tensor(0.7691)\n",
            "batches_trained: 290 total_loss: tensor(0.0144) acc: tensor(0.7692)\n",
            "batches_trained: 291 total_loss: tensor(0.0144) acc: tensor(0.7691)\n",
            "batches_trained: 292 total_loss: tensor(0.0144) acc: tensor(0.7688)\n",
            "batches_trained: 293 total_loss: tensor(0.0144) acc: tensor(0.7690)\n",
            "batches_trained: 294 total_loss: tensor(0.0144) acc: tensor(0.7691)\n",
            "batches_trained: 295 total_loss: tensor(0.0144) acc: tensor(0.7691)\n",
            "batches_trained: 296 total_loss: tensor(0.0144) acc: tensor(0.7692)\n",
            "batches_trained: 297 total_loss: tensor(0.0144) acc: tensor(0.7697)\n",
            "batches_trained: 298 total_loss: tensor(0.0144) acc: tensor(0.7699)\n",
            "batches_trained: 299 total_loss: tensor(0.0144) acc: tensor(0.7700)\n",
            "batches_trained: 300 total_loss: tensor(0.0144) acc: tensor(0.7703)\n",
            "batches_trained: 301 total_loss: tensor(0.0144) acc: tensor(0.7706)\n",
            "batches_trained: 302 total_loss: tensor(0.0144) acc: tensor(0.7706)\n",
            "batches_trained: 303 total_loss: tensor(0.0144) acc: tensor(0.7705)\n",
            "batches_trained: 304 total_loss: tensor(0.0144) acc: tensor(0.7709)\n",
            "batches_trained: 305 total_loss: tensor(0.0144) acc: tensor(0.7712)\n",
            "batches_trained: 306 total_loss: tensor(0.0144) acc: tensor(0.7711)\n",
            "batches_trained: 307 total_loss: tensor(0.0144) acc: tensor(0.7709)\n",
            "batches_trained: 308 total_loss: tensor(0.0144) acc: tensor(0.7712)\n",
            "batches_trained: 309 total_loss: tensor(0.0144) acc: tensor(0.7712)\n",
            "batches_trained: 310 total_loss: tensor(0.0143) acc: tensor(0.7717)\n",
            "batches_trained: 311 total_loss: tensor(0.0143) acc: tensor(0.7719)\n",
            "batches_trained: 312 total_loss: tensor(0.0143) acc: tensor(0.7719)\n",
            "batches_trained: 313 total_loss: tensor(0.0143) acc: tensor(0.7723)\n",
            "batches_trained: 314 total_loss: tensor(0.0143) acc: tensor(0.7726)\n",
            "batches_trained: 315 total_loss: tensor(0.0143) acc: tensor(0.7727)\n",
            "batches_trained: 316 total_loss: tensor(0.0143) acc: tensor(0.7730)\n",
            "batches_trained: 317 total_loss: tensor(0.0143) acc: tensor(0.7736)\n",
            "batches_trained: 318 total_loss: tensor(0.0143) acc: tensor(0.7738)\n",
            "batches_trained: 319 total_loss: tensor(0.0143) acc: tensor(0.7739)\n",
            "batches_trained: 320 total_loss: tensor(0.0142) acc: tensor(0.7741)\n",
            "batches_trained: 321 total_loss: tensor(0.0142) acc: tensor(0.7742)\n",
            "batches_trained: 322 total_loss: tensor(0.0142) acc: tensor(0.7747)\n",
            "batches_trained: 323 total_loss: tensor(0.0142) acc: tensor(0.7748)\n",
            "batches_trained: 324 total_loss: tensor(0.0142) acc: tensor(0.7749)\n",
            "batches_trained: 325 total_loss: tensor(0.0142) acc: tensor(0.7754)\n",
            "batches_trained: 326 total_loss: tensor(0.0142) acc: tensor(0.7754)\n",
            "batches_trained: 327 total_loss: tensor(0.0141) acc: tensor(0.7759)\n",
            "batches_trained: 328 total_loss: tensor(0.0141) acc: tensor(0.7760)\n",
            "batches_trained: 329 total_loss: tensor(0.0142) acc: tensor(0.7758)\n",
            "batches_trained: 330 total_loss: tensor(0.0141) acc: tensor(0.7759)\n",
            "batches_trained: 331 total_loss: tensor(0.0142) acc: tensor(0.7757)\n",
            "batches_trained: 332 total_loss: tensor(0.0142) acc: tensor(0.7758)\n",
            "batches_trained: 333 total_loss: tensor(0.0142) acc: tensor(0.7762)\n",
            "batches_trained: 334 total_loss: tensor(0.0142) acc: tensor(0.7762)\n",
            "batches_trained: 335 total_loss: tensor(0.0142) acc: tensor(0.7760)\n",
            "batches_trained: 336 total_loss: tensor(0.0142) acc: tensor(0.7760)\n",
            "batches_trained: 337 total_loss: tensor(0.0142) acc: tensor(0.7761)\n",
            "batches_trained: 338 total_loss: tensor(0.0142) acc: tensor(0.7762)\n",
            "batches_trained: 339 total_loss: tensor(0.0142) acc: tensor(0.7762)\n",
            "batches_trained: 340 total_loss: tensor(0.0142) acc: tensor(0.7760)\n",
            "batches_trained: 341 total_loss: tensor(0.0142) acc: tensor(0.7759)\n",
            "batches_trained: 342 total_loss: tensor(0.0142) acc: tensor(0.7760)\n",
            "batches_trained: 343 total_loss: tensor(0.0142) acc: tensor(0.7761)\n",
            "batches_trained: 344 total_loss: tensor(0.0142) acc: tensor(0.7763)\n",
            "batches_trained: 345 total_loss: tensor(0.0142) acc: tensor(0.7767)\n",
            "batches_trained: 346 total_loss: tensor(0.0142) acc: tensor(0.7767)\n",
            "batches_trained: 347 total_loss: tensor(0.0142) acc: tensor(0.7770)\n",
            "batches_trained: 348 total_loss: tensor(0.0141) acc: tensor(0.7774)\n",
            "batches_trained: 349 total_loss: tensor(0.0141) acc: tensor(0.7775)\n",
            "batches_trained: 350 total_loss: tensor(0.0141) acc: tensor(0.7775)\n",
            "batches_trained: 351 total_loss: tensor(0.0141) acc: tensor(0.7776)\n",
            "batches_trained: 352 total_loss: tensor(0.0141) acc: tensor(0.7779)\n",
            "batches_trained: 353 total_loss: tensor(0.0141) acc: tensor(0.7782)\n",
            "batches_trained: 354 total_loss: tensor(0.0141) acc: tensor(0.7779)\n",
            "batches_trained: 355 total_loss: tensor(0.0141) acc: tensor(0.7780)\n",
            "batches_trained: 356 total_loss: tensor(0.0141) acc: tensor(0.7782)\n",
            "batches_trained: 357 total_loss: tensor(0.0141) acc: tensor(0.7785)\n",
            "batches_trained: 358 total_loss: tensor(0.0141) acc: tensor(0.7784)\n",
            "batches_trained: 359 total_loss: tensor(0.0141) acc: tensor(0.7786)\n",
            "batches_trained: 360 total_loss: tensor(0.0141) acc: tensor(0.7784)\n",
            "batches_trained: 361 total_loss: tensor(0.0141) acc: tensor(0.7786)\n",
            "batches_trained: 362 total_loss: tensor(0.0141) acc: tensor(0.7787)\n",
            "batches_trained: 363 total_loss: tensor(0.0141) acc: tensor(0.7788)\n",
            "batches_trained: 364 total_loss: tensor(0.0141) acc: tensor(0.7788)\n",
            "batches_trained: 365 total_loss: tensor(0.0141) acc: tensor(0.7786)\n",
            "batches_trained: 366 total_loss: tensor(0.0141) acc: tensor(0.7787)\n",
            "batches_trained: 367 total_loss: tensor(0.0141) acc: tensor(0.7787)\n",
            "batches_trained: 368 total_loss: tensor(0.0141) acc: tensor(0.7787)\n",
            "batches_trained: 369 total_loss: tensor(0.0141) acc: tensor(0.7790)\n",
            "batches_trained: 370 total_loss: tensor(0.0140) acc: tensor(0.7793)\n",
            "batches_trained: 371 total_loss: tensor(0.0140) acc: tensor(0.7793)\n",
            "batches_trained: 372 total_loss: tensor(0.0140) acc: tensor(0.7798)\n",
            "batches_trained: 373 total_loss: tensor(0.0140) acc: tensor(0.7799)\n",
            "batches_trained: 374 total_loss: tensor(0.0140) acc: tensor(0.7798)\n",
            "batches_trained: 375 total_loss: tensor(0.0140) acc: tensor(0.7801)\n",
            "batches_trained: 376 total_loss: tensor(0.0140) acc: tensor(0.7801)\n",
            "batches_trained: 377 total_loss: tensor(0.0140) acc: tensor(0.7803)\n",
            "batches_trained: 378 total_loss: tensor(0.0140) acc: tensor(0.7805)\n",
            "batches_trained: 379 total_loss: tensor(0.0140) acc: tensor(0.7808)\n",
            "batches_trained: 380 total_loss: tensor(0.0140) acc: tensor(0.7807)\n",
            "batches_trained: 381 total_loss: tensor(0.0140) acc: tensor(0.7807)\n",
            "batches_trained: 382 total_loss: tensor(0.0140) acc: tensor(0.7804)\n",
            "batches_trained: 383 total_loss: tensor(0.0140) acc: tensor(0.7806)\n",
            "batches_trained: 384 total_loss: tensor(0.0140) acc: tensor(0.7808)\n",
            "batches_trained: 385 total_loss: tensor(0.0140) acc: tensor(0.7807)\n",
            "batches_trained: 386 total_loss: tensor(0.0140) acc: tensor(0.7807)\n",
            "batches_trained: 387 total_loss: tensor(0.0140) acc: tensor(0.7810)\n",
            "batches_trained: 388 total_loss: tensor(0.0139) acc: tensor(0.7814)\n",
            "batches_trained: 389 total_loss: tensor(0.0140) acc: tensor(0.7812)\n",
            "batches_trained: 390 total_loss: tensor(0.0139) acc: tensor(0.7812)\n",
            "batches_trained: 391 total_loss: tensor(0.0139) acc: tensor(0.7815)\n",
            "batches_trained: 392 total_loss: tensor(0.0139) acc: tensor(0.7816)\n",
            "batches_trained: 393 total_loss: tensor(0.0139) acc: tensor(0.7817)\n",
            "batches_trained: 394 total_loss: tensor(0.0139) acc: tensor(0.7820)\n",
            "batches_trained: 395 total_loss: tensor(0.0139) acc: tensor(0.7820)\n",
            "batches_trained: 396 total_loss: tensor(0.0139) acc: tensor(0.7824)\n",
            "batches_trained: 397 total_loss: tensor(0.0139) acc: tensor(0.7821)\n",
            "batches_trained: 398 total_loss: tensor(0.0139) acc: tensor(0.7820)\n",
            "batches_trained: 399 total_loss: tensor(0.0139) acc: tensor(0.7819)\n",
            "batches_trained: 400 total_loss: tensor(0.0139) acc: tensor(0.7819)\n",
            "batches_trained: 401 total_loss: tensor(0.0139) acc: tensor(0.7819)\n",
            "batches_trained: 402 total_loss: tensor(0.0139) acc: tensor(0.7822)\n",
            "batches_trained: 403 total_loss: tensor(0.0139) acc: tensor(0.7822)\n",
            "batches_trained: 404 total_loss: tensor(0.0139) acc: tensor(0.7823)\n",
            "batches_trained: 405 total_loss: tensor(0.0139) acc: tensor(0.7823)\n",
            "batches_trained: 406 total_loss: tensor(0.0139) acc: tensor(0.7823)\n",
            "batches_trained: 407 total_loss: tensor(0.0139) acc: tensor(0.7826)\n",
            "batches_trained: 408 total_loss: tensor(0.0139) acc: tensor(0.7824)\n",
            "batches_trained: 409 total_loss: tensor(0.0139) acc: tensor(0.7824)\n",
            "batches_trained: 410 total_loss: tensor(0.0139) acc: tensor(0.7825)\n",
            "batches_trained: 411 total_loss: tensor(0.0139) acc: tensor(0.7825)\n",
            "batches_trained: 412 total_loss: tensor(0.0139) acc: tensor(0.7826)\n",
            "batches_trained: 413 total_loss: tensor(0.0139) acc: tensor(0.7827)\n",
            "batches_trained: 414 total_loss: tensor(0.0139) acc: tensor(0.7824)\n",
            "batches_trained: 415 total_loss: tensor(0.0139) acc: tensor(0.7825)\n",
            "batches_trained: 416 total_loss: tensor(0.0139) acc: tensor(0.7826)\n",
            "batches_trained: 417 total_loss: tensor(0.0139) acc: tensor(0.7828)\n",
            "batches_trained: 418 total_loss: tensor(0.0139) acc: tensor(0.7828)\n",
            "batches_trained: 419 total_loss: tensor(0.0138) acc: tensor(0.7830)\n",
            "batches_trained: 420 total_loss: tensor(0.0138) acc: tensor(0.7830)\n",
            "batches_trained: 421 total_loss: tensor(0.0138) acc: tensor(0.7833)\n",
            "batches_trained: 422 total_loss: tensor(0.0138) acc: tensor(0.7831)\n",
            "batches_trained: 423 total_loss: tensor(0.0138) acc: tensor(0.7832)\n",
            "batches_trained: 424 total_loss: tensor(0.0138) acc: tensor(0.7835)\n",
            "batches_trained: 425 total_loss: tensor(0.0138) acc: tensor(0.7836)\n",
            "batches_trained: 426 total_loss: tensor(0.0138) acc: tensor(0.7839)\n",
            "batches_trained: 427 total_loss: tensor(0.0138) acc: tensor(0.7841)\n",
            "batches_trained: 428 total_loss: tensor(0.0138) acc: tensor(0.7842)\n",
            "batches_trained: 429 total_loss: tensor(0.0138) acc: tensor(0.7845)\n",
            "batches_trained: 430 total_loss: tensor(0.0138) acc: tensor(0.7847)\n",
            "batches_trained: 431 total_loss: tensor(0.0138) acc: tensor(0.7849)\n",
            "batches_trained: 432 total_loss: tensor(0.0138) acc: tensor(0.7849)\n",
            "batches_trained: 433 total_loss: tensor(0.0138) acc: tensor(0.7851)\n",
            "batches_trained: 434 total_loss: tensor(0.0137) acc: tensor(0.7852)\n",
            "batches_trained: 435 total_loss: tensor(0.0138) acc: tensor(0.7853)\n",
            "batches_trained: 436 total_loss: tensor(0.0137) acc: tensor(0.7856)\n",
            "batches_trained: 437 total_loss: tensor(0.0137) acc: tensor(0.7858)\n",
            "batches_trained: 438 total_loss: tensor(0.0137) acc: tensor(0.7855)\n",
            "batches_trained: 439 total_loss: tensor(0.0137) acc: tensor(0.7857)\n",
            "batches_trained: 440 total_loss: tensor(0.0137) acc: tensor(0.7859)\n",
            "batches_trained: 441 total_loss: tensor(0.0137) acc: tensor(0.7862)\n",
            "batches_trained: 442 total_loss: tensor(0.0137) acc: tensor(0.7863)\n",
            "batches_trained: 443 total_loss: tensor(0.0137) acc: tensor(0.7859)\n",
            "batches_trained: 444 total_loss: tensor(0.0137) acc: tensor(0.7862)\n",
            "batches_trained: 445 total_loss: tensor(0.0137) acc: tensor(0.7860)\n",
            "batches_trained: 446 total_loss: tensor(0.0137) acc: tensor(0.7861)\n",
            "batches_trained: 447 total_loss: tensor(0.0137) acc: tensor(0.7863)\n",
            "batches_trained: 448 total_loss: tensor(0.0137) acc: tensor(0.7866)\n",
            "batches_trained: 449 total_loss: tensor(0.0137) acc: tensor(0.7867)\n",
            "batches_trained: 450 total_loss: tensor(0.0137) acc: tensor(0.7869)\n",
            "batches_trained: 451 total_loss: tensor(0.0137) acc: tensor(0.7873)\n",
            "batches_trained: 452 total_loss: tensor(0.0137) acc: tensor(0.7875)\n",
            "batches_trained: 453 total_loss: tensor(0.0136) acc: tensor(0.7878)\n",
            "batches_trained: 454 total_loss: tensor(0.0136) acc: tensor(0.7880)\n",
            "batches_trained: 455 total_loss: tensor(0.0136) acc: tensor(0.7877)\n",
            "batches_trained: 456 total_loss: tensor(0.0137) acc: tensor(0.7874)\n",
            "batches_trained: 457 total_loss: tensor(0.0136) acc: tensor(0.7875)\n",
            "batches_trained: 458 total_loss: tensor(0.0136) acc: tensor(0.7877)\n",
            "batches_trained: 459 total_loss: tensor(0.0137) acc: tensor(0.7874)\n",
            "batches_trained: 460 total_loss: tensor(0.0137) acc: tensor(0.7875)\n",
            "batches_trained: 461 total_loss: tensor(0.0137) acc: tensor(0.7876)\n",
            "batches_trained: 462 total_loss: tensor(0.0136) acc: tensor(0.7879)\n",
            "batches_trained: 463 total_loss: tensor(0.0136) acc: tensor(0.7883)\n",
            "batches_trained: 464 total_loss: tensor(0.0136) acc: tensor(0.7883)\n",
            "batches_trained: 465 total_loss: tensor(0.0136) acc: tensor(0.7881)\n",
            "batches_trained: 466 total_loss: tensor(0.0136) acc: tensor(0.7882)\n",
            "batches_trained: 467 total_loss: tensor(0.0136) acc: tensor(0.7883)\n",
            "batches_trained: 468 total_loss: tensor(0.0136) acc: tensor(0.7885)\n",
            "batches_trained: 469 total_loss: tensor(0.0136) acc: tensor(0.7886)\n",
            "batches_trained: 470 total_loss: tensor(0.0136) acc: tensor(0.7883)\n",
            "batches_trained: 471 total_loss: tensor(0.0136) acc: tensor(0.7884)\n",
            "batches_trained: 472 total_loss: tensor(0.0136) acc: tensor(0.7883)\n",
            "batches_trained: 473 total_loss: tensor(0.0136) acc: tensor(0.7887)\n",
            "batches_trained: 474 total_loss: tensor(0.0136) acc: tensor(0.7886)\n",
            "batches_trained: 475 total_loss: tensor(0.0136) acc: tensor(0.7885)\n",
            "batches_trained: 476 total_loss: tensor(0.0136) acc: tensor(0.7885)\n",
            "batches_trained: 477 total_loss: tensor(0.0136) acc: tensor(0.7886)\n",
            "batches_trained: 478 total_loss: tensor(0.0136) acc: tensor(0.7886)\n",
            "batches_trained: 479 total_loss: tensor(0.0136) acc: tensor(0.7888)\n",
            "batches_trained: 480 total_loss: tensor(0.0136) acc: tensor(0.7887)\n",
            "batches_trained: 481 total_loss: tensor(0.0136) acc: tensor(0.7885)\n",
            "batches_trained: 482 total_loss: tensor(0.0136) acc: tensor(0.7885)\n",
            "batches_trained: 483 total_loss: tensor(0.0136) acc: tensor(0.7885)\n",
            "batches_trained: 484 total_loss: tensor(0.0136) acc: tensor(0.7884)\n",
            "batches_trained: 485 total_loss: tensor(0.0136) acc: tensor(0.7886)\n",
            "batches_trained: 486 total_loss: tensor(0.0136) acc: tensor(0.7887)\n",
            "batches_trained: 487 total_loss: tensor(0.0136) acc: tensor(0.7887)\n",
            "batches_trained: 488 total_loss: tensor(0.0136) acc: tensor(0.7889)\n",
            "batches_trained: 489 total_loss: tensor(0.0136) acc: tensor(0.7889)\n",
            "batches_trained: 490 total_loss: tensor(0.0136) acc: tensor(0.7892)\n",
            "batches_trained: 491 total_loss: tensor(0.0136) acc: tensor(0.7893)\n",
            "batches_trained: 492 total_loss: tensor(0.0136) acc: tensor(0.7893)\n",
            "batches_trained: 493 total_loss: tensor(0.0136) acc: tensor(0.7893)\n",
            "batches_trained: 494 total_loss: tensor(0.0136) acc: tensor(0.7896)\n",
            "batches_trained: 495 total_loss: tensor(0.0136) acc: tensor(0.7897)\n",
            "batches_trained: 496 total_loss: tensor(0.0136) acc: tensor(0.7899)\n",
            "batches_trained: 497 total_loss: tensor(0.0136) acc: tensor(0.7899)\n",
            "batches_trained: 498 total_loss: tensor(0.0136) acc: tensor(0.7900)\n",
            "batches_trained: 499 total_loss: tensor(0.0135) acc: tensor(0.7901)\n",
            "batches_trained: 500 total_loss: tensor(0.0135) acc: tensor(0.7903)\n",
            "batches_trained: 501 total_loss: tensor(0.0135) acc: tensor(0.7902)\n",
            "batches_trained: 502 total_loss: tensor(0.0136) acc: tensor(0.7900)\n",
            "batches_trained: 503 total_loss: tensor(0.0136) acc: tensor(0.7899)\n",
            "batches_trained: 504 total_loss: tensor(0.0135) acc: tensor(0.7899)\n",
            "batches_trained: 505 total_loss: tensor(0.0135) acc: tensor(0.7900)\n",
            "batches_trained: 506 total_loss: tensor(0.0135) acc: tensor(0.7901)\n",
            "batches_trained: 507 total_loss: tensor(0.0135) acc: tensor(0.7901)\n",
            "batches_trained: 508 total_loss: tensor(0.0135) acc: tensor(0.7903)\n",
            "batches_trained: 509 total_loss: tensor(0.0135) acc: tensor(0.7905)\n",
            "batches_trained: 510 total_loss: tensor(0.0135) acc: tensor(0.7906)\n",
            "batches_trained: 511 total_loss: tensor(0.0135) acc: tensor(0.7907)\n",
            "batches_trained: 512 total_loss: tensor(0.0135) acc: tensor(0.7908)\n",
            "batches_trained: 513 total_loss: tensor(0.0135) acc: tensor(0.7908)\n",
            "batches_trained: 514 total_loss: tensor(0.0135) acc: tensor(0.7907)\n",
            "batches_trained: 515 total_loss: tensor(0.0135) acc: tensor(0.7907)\n",
            "batches_trained: 516 total_loss: tensor(0.0135) acc: tensor(0.7908)\n",
            "batches_trained: 517 total_loss: tensor(0.0135) acc: tensor(0.7907)\n",
            "batches_trained: 518 total_loss: tensor(0.0135) acc: tensor(0.7910)\n",
            "batches_trained: 519 total_loss: tensor(0.0135) acc: tensor(0.7910)\n",
            "batches_trained: 520 total_loss: tensor(0.0135) acc: tensor(0.7913)\n",
            "batches_trained: 521 total_loss: tensor(0.0135) acc: tensor(0.7913)\n",
            "batches_trained: 522 total_loss: tensor(0.0135) acc: tensor(0.7912)\n",
            "batches_trained: 523 total_loss: tensor(0.0135) acc: tensor(0.7912)\n",
            "batches_trained: 524 total_loss: tensor(0.0135) acc: tensor(0.7911)\n",
            "batches_trained: 525 total_loss: tensor(0.0135) acc: tensor(0.7913)\n",
            "batches_trained: 526 total_loss: tensor(0.0135) acc: tensor(0.7915)\n",
            "batches_trained: 527 total_loss: tensor(0.0135) acc: tensor(0.7914)\n",
            "batches_trained: 528 total_loss: tensor(0.0135) acc: tensor(0.7914)\n",
            "batches_trained: 529 total_loss: tensor(0.0135) acc: tensor(0.7916)\n",
            "batches_trained: 530 total_loss: tensor(0.0135) acc: tensor(0.7916)\n",
            "batches_trained: 531 total_loss: tensor(0.0135) acc: tensor(0.7917)\n",
            "batches_trained: 532 total_loss: tensor(0.0135) acc: tensor(0.7916)\n",
            "batches_trained: 533 total_loss: tensor(0.0135) acc: tensor(0.7918)\n",
            "batches_trained: 534 total_loss: tensor(0.0135) acc: tensor(0.7919)\n",
            "batches_trained: 535 total_loss: tensor(0.0135) acc: tensor(0.7920)\n",
            "batches_trained: 536 total_loss: tensor(0.0135) acc: tensor(0.7922)\n",
            "batches_trained: 537 total_loss: tensor(0.0134) acc: tensor(0.7925)\n",
            "batches_trained: 538 total_loss: tensor(0.0134) acc: tensor(0.7926)\n",
            "batches_trained: 539 total_loss: tensor(0.0134) acc: tensor(0.7926)\n",
            "batches_trained: 540 total_loss: tensor(0.0134) acc: tensor(0.7925)\n",
            "batches_trained: 541 total_loss: tensor(0.0134) acc: tensor(0.7928)\n",
            "batches_trained: 542 total_loss: tensor(0.0134) acc: tensor(0.7929)\n",
            "batches_trained: 543 total_loss: tensor(0.0134) acc: tensor(0.7928)\n",
            "batches_trained: 544 total_loss: tensor(0.0134) acc: tensor(0.7929)\n",
            "batches_trained: 545 total_loss: tensor(0.0134) acc: tensor(0.7929)\n",
            "batches_trained: 546 total_loss: tensor(0.0134) acc: tensor(0.7929)\n",
            "batches_trained: 547 total_loss: tensor(0.0134) acc: tensor(0.7928)\n",
            "batches_trained: 548 total_loss: tensor(0.0134) acc: tensor(0.7929)\n",
            "batches_trained: 549 total_loss: tensor(0.0134) acc: tensor(0.7929)\n",
            "batches_trained: 550 total_loss: tensor(0.0134) acc: tensor(0.7931)\n",
            "batches_trained: 551 total_loss: tensor(0.0134) acc: tensor(0.7932)\n",
            "batches_trained: 552 total_loss: tensor(0.0134) acc: tensor(0.7933)\n",
            "batches_trained: 553 total_loss: tensor(0.0134) acc: tensor(0.7932)\n",
            "batches_trained: 554 total_loss: tensor(0.0134) acc: tensor(0.7933)\n",
            "batches_trained: 555 total_loss: tensor(0.0134) acc: tensor(0.7935)\n",
            "batches_trained: 556 total_loss: tensor(0.0134) acc: tensor(0.7932)\n",
            "batches_trained: 557 total_loss: tensor(0.0134) acc: tensor(0.7933)\n",
            "batches_trained: 558 total_loss: tensor(0.0134) acc: tensor(0.7933)\n",
            "batches_trained: 559 total_loss: tensor(0.0134) acc: tensor(0.7934)\n",
            "batches_trained: 560 total_loss: tensor(0.0134) acc: tensor(0.7937)\n",
            "batches_trained: 561 total_loss: tensor(0.0134) acc: tensor(0.7938)\n",
            "batches_trained: 562 total_loss: tensor(0.0134) acc: tensor(0.7940)\n",
            "batches_trained: 563 total_loss: tensor(0.0134) acc: tensor(0.7939)\n",
            "batches_trained: 564 total_loss: tensor(0.0134) acc: tensor(0.7940)\n",
            "batches_trained: 565 total_loss: tensor(0.0134) acc: tensor(0.7941)\n",
            "batches_trained: 566 total_loss: tensor(0.0134) acc: tensor(0.7941)\n",
            "batches_trained: 567 total_loss: tensor(0.0134) acc: tensor(0.7942)\n",
            "batches_trained: 568 total_loss: tensor(0.0134) acc: tensor(0.7942)\n",
            "batches_trained: 569 total_loss: tensor(0.0134) acc: tensor(0.7939)\n",
            "batches_trained: 570 total_loss: tensor(0.0134) acc: tensor(0.7940)\n",
            "batches_trained: 571 total_loss: tensor(0.0134) acc: tensor(0.7942)\n",
            "batches_trained: 572 total_loss: tensor(0.0134) acc: tensor(0.7943)\n",
            "batches_trained: 573 total_loss: tensor(0.0134) acc: tensor(0.7942)\n",
            "batches_trained: 574 total_loss: tensor(0.0134) acc: tensor(0.7944)\n",
            "batches_trained: 575 total_loss: tensor(0.0134) acc: tensor(0.7943)\n",
            "batches_trained: 576 total_loss: tensor(0.0133) acc: tensor(0.7945)\n",
            "batches_trained: 577 total_loss: tensor(0.0133) acc: tensor(0.7946)\n",
            "batches_trained: 578 total_loss: tensor(0.0133) acc: tensor(0.7946)\n",
            "batches_trained: 579 total_loss: tensor(0.0133) acc: tensor(0.7946)\n",
            "batches_trained: 580 total_loss: tensor(0.0133) acc: tensor(0.7948)\n",
            "batches_trained: 581 total_loss: tensor(0.0133) acc: tensor(0.7950)\n",
            "batches_trained: 582 total_loss: tensor(0.0133) acc: tensor(0.7952)\n",
            "batches_trained: 583 total_loss: tensor(0.0133) acc: tensor(0.7950)\n",
            "batches_trained: 584 total_loss: tensor(0.0133) acc: tensor(0.7952)\n",
            "batches_trained: 585 total_loss: tensor(0.0133) acc: tensor(0.7952)\n",
            "batches_trained: 586 total_loss: tensor(0.0133) acc: tensor(0.7953)\n",
            "batches_trained: 587 total_loss: tensor(0.0133) acc: tensor(0.7953)\n",
            "batches_trained: 588 total_loss: tensor(0.0133) acc: tensor(0.7952)\n",
            "batches_trained: 589 total_loss: tensor(0.0133) acc: tensor(0.7954)\n",
            "batches_trained: 590 total_loss: tensor(0.0133) acc: tensor(0.7954)\n",
            "batches_trained: 591 total_loss: tensor(0.0133) acc: tensor(0.7957)\n",
            "batches_trained: 592 total_loss: tensor(0.0133) acc: tensor(0.7958)\n",
            "batches_trained: 593 total_loss: tensor(0.0133) acc: tensor(0.7958)\n",
            "batches_trained: 594 total_loss: tensor(0.0133) acc: tensor(0.7960)\n",
            "batches_trained: 595 total_loss: tensor(0.0133) acc: tensor(0.7961)\n",
            "batches_trained: 596 total_loss: tensor(0.0133) acc: tensor(0.7960)\n",
            "batches_trained: 597 total_loss: tensor(0.0133) acc: tensor(0.7961)\n",
            "batches_trained: 598 total_loss: tensor(0.0133) acc: tensor(0.7962)\n",
            "batches_trained: 599 total_loss: tensor(0.0133) acc: tensor(0.7963)\n",
            "batches_trained: 600 total_loss: tensor(0.0133) acc: tensor(0.7963)\n",
            "batches_trained: 601 total_loss: tensor(0.0133) acc: tensor(0.7963)\n",
            "batches_trained: 602 total_loss: tensor(0.0133) acc: tensor(0.7964)\n",
            "batches_trained: 603 total_loss: tensor(0.0133) acc: tensor(0.7965)\n",
            "batches_trained: 604 total_loss: tensor(0.0133) acc: tensor(0.7966)\n",
            "batches_trained: 605 total_loss: tensor(0.0133) acc: tensor(0.7967)\n",
            "batches_trained: 606 total_loss: tensor(0.0132) acc: tensor(0.7969)\n",
            "batches_trained: 607 total_loss: tensor(0.0132) acc: tensor(0.7970)\n",
            "batches_trained: 608 total_loss: tensor(0.0132) acc: tensor(0.7971)\n",
            "batches_trained: 609 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 610 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 611 total_loss: tensor(0.0132) acc: tensor(0.7973)\n",
            "batches_trained: 612 total_loss: tensor(0.0132) acc: tensor(0.7973)\n",
            "batches_trained: 613 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 614 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 615 total_loss: tensor(0.0132) acc: tensor(0.7970)\n",
            "batches_trained: 616 total_loss: tensor(0.0132) acc: tensor(0.7969)\n",
            "batches_trained: 617 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 618 total_loss: tensor(0.0132) acc: tensor(0.7971)\n",
            "batches_trained: 619 total_loss: tensor(0.0132) acc: tensor(0.7971)\n",
            "batches_trained: 620 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 621 total_loss: tensor(0.0132) acc: tensor(0.7972)\n",
            "batches_trained: 622 total_loss: tensor(0.0132) acc: tensor(0.7973)\n",
            "batches_trained: 623 total_loss: tensor(0.0132) acc: tensor(0.7973)\n",
            "batches_trained: 624 total_loss: tensor(0.0132) acc: tensor(0.7973)\n",
            "batches_trained: 625 total_loss: tensor(0.0132) acc: tensor(0.7974)\n",
            "batches_trained: 626 total_loss: tensor(0.0132) acc: tensor(0.7973)\n",
            "batches_trained: 627 total_loss: tensor(0.0132) acc: tensor(0.7974)\n",
            "batches_trained: 628 total_loss: tensor(0.0132) acc: tensor(0.7975)\n",
            "batches_trained: 629 total_loss: tensor(0.0132) acc: tensor(0.7977)\n",
            "batches_trained: 630 total_loss: tensor(0.0132) acc: tensor(0.7978)\n",
            "batches_trained: 631 total_loss: tensor(0.0132) acc: tensor(0.7978)\n",
            "batches_trained: 632 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 633 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 634 total_loss: tensor(0.0132) acc: tensor(0.7979)\n",
            "batches_trained: 635 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 636 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 637 total_loss: tensor(0.0132) acc: tensor(0.7979)\n",
            "batches_trained: 638 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 639 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 640 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 641 total_loss: tensor(0.0132) acc: tensor(0.7980)\n",
            "batches_trained: 642 total_loss: tensor(0.0132) acc: tensor(0.7981)\n",
            "batches_trained: 643 total_loss: tensor(0.0132) acc: tensor(0.7982)\n",
            "batches_trained: 644 total_loss: tensor(0.0132) acc: tensor(0.7982)\n",
            "batches_trained: 645 total_loss: tensor(0.0132) acc: tensor(0.7983)\n",
            "batches_trained: 646 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 647 total_loss: tensor(0.0132) acc: tensor(0.7985)\n",
            "batches_trained: 648 total_loss: tensor(0.0132) acc: tensor(0.7985)\n",
            "batches_trained: 649 total_loss: tensor(0.0132) acc: tensor(0.7983)\n",
            "batches_trained: 650 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 651 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 652 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 653 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 654 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 655 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 656 total_loss: tensor(0.0132) acc: tensor(0.7985)\n",
            "batches_trained: 657 total_loss: tensor(0.0132) acc: tensor(0.7983)\n",
            "batches_trained: 658 total_loss: tensor(0.0132) acc: tensor(0.7984)\n",
            "batches_trained: 659 total_loss: tensor(0.0132) acc: tensor(0.7985)\n",
            "batches_trained: 660 total_loss: tensor(0.0132) acc: tensor(0.7986)\n",
            "batches_trained: 661 total_loss: tensor(0.0132) acc: tensor(0.7986)\n",
            "batches_trained: 662 total_loss: tensor(0.0131) acc: tensor(0.7990)\n",
            "batches_trained: 663 total_loss: tensor(0.0131) acc: tensor(0.7990)\n",
            "batches_trained: 664 total_loss: tensor(0.0131) acc: tensor(0.7990)\n",
            "batches_trained: 665 total_loss: tensor(0.0131) acc: tensor(0.7992)\n",
            "batches_trained: 666 total_loss: tensor(0.0131) acc: tensor(0.7993)\n",
            "batches_trained: 667 total_loss: tensor(0.0131) acc: tensor(0.7993)\n",
            "batches_trained: 668 total_loss: tensor(0.0131) acc: tensor(0.7991)\n",
            "batches_trained: 669 total_loss: tensor(0.0131) acc: tensor(0.7991)\n",
            "batches_trained: 670 total_loss: tensor(0.0131) acc: tensor(0.7992)\n",
            "batches_trained: 671 total_loss: tensor(0.0131) acc: tensor(0.7991)\n",
            "batches_trained: 672 total_loss: tensor(0.0131) acc: tensor(0.7991)\n",
            "batches_trained: 673 total_loss: tensor(0.0131) acc: tensor(0.7992)\n",
            "batches_trained: 674 total_loss: tensor(0.0131) acc: tensor(0.7993)\n",
            "batches_trained: 675 total_loss: tensor(0.0131) acc: tensor(0.7994)\n",
            "batches_trained: 676 total_loss: tensor(0.0131) acc: tensor(0.7995)\n",
            "batches_trained: 677 total_loss: tensor(0.0131) acc: tensor(0.7996)\n",
            "batches_trained: 678 total_loss: tensor(0.0131) acc: tensor(0.7997)\n",
            "batches_trained: 679 total_loss: tensor(0.0131) acc: tensor(0.7997)\n",
            "batches_trained: 680 total_loss: tensor(0.0131) acc: tensor(0.7998)\n",
            "batches_trained: 681 total_loss: tensor(0.0131) acc: tensor(0.7999)\n",
            "batches_trained: 682 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 683 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 684 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 685 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 686 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 687 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 688 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 689 total_loss: tensor(0.0131) acc: tensor(0.7999)\n",
            "batches_trained: 690 total_loss: tensor(0.0131) acc: tensor(0.8000)\n",
            "batches_trained: 691 total_loss: tensor(0.0131) acc: tensor(0.8001)\n",
            "batches_trained: 692 total_loss: tensor(0.0131) acc: tensor(0.8003)\n",
            "batches_trained: 693 total_loss: tensor(0.0131) acc: tensor(0.8003)\n",
            "batches_trained: 694 total_loss: tensor(0.0131) acc: tensor(0.8003)\n",
            "batches_trained: 695 total_loss: tensor(0.0131) acc: tensor(0.8004)\n",
            "batches_trained: 696 total_loss: tensor(0.0131) acc: tensor(0.8005)\n",
            "batches_trained: 697 total_loss: tensor(0.0131) acc: tensor(0.8005)\n",
            "batches_trained: 698 total_loss: tensor(0.0131) acc: tensor(0.8006)\n",
            "batches_trained: 699 total_loss: tensor(0.0131) acc: tensor(0.8007)\n",
            "batches_trained: 700 total_loss: tensor(0.0131) acc: tensor(0.8008)\n",
            "batches_trained: 701 total_loss: tensor(0.0131) acc: tensor(0.8008)\n",
            "batches_trained: 702 total_loss: tensor(0.0131) acc: tensor(0.8007)\n",
            "batches_trained: 703 total_loss: tensor(0.0131) acc: tensor(0.8008)\n",
            "batches_trained: 704 total_loss: tensor(0.0131) acc: tensor(0.8008)\n",
            "batches_trained: 705 total_loss: tensor(0.0131) acc: tensor(0.8006)\n",
            "batches_trained: 706 total_loss: tensor(0.0131) acc: tensor(0.8005)\n",
            "batches_trained: 707 total_loss: tensor(0.0131) acc: tensor(0.8006)\n",
            "batches_trained: 708 total_loss: tensor(0.0131) acc: tensor(0.8005)\n",
            "batches_trained: 709 total_loss: tensor(0.0131) acc: tensor(0.8005)\n",
            "batches_trained: 710 total_loss: tensor(0.0131) acc: tensor(0.8007)\n",
            "batches_trained: 711 total_loss: tensor(0.0130) acc: tensor(0.8006)\n",
            "batches_trained: 712 total_loss: tensor(0.0130) acc: tensor(0.8007)\n",
            "batches_trained: 713 total_loss: tensor(0.0130) acc: tensor(0.8008)\n",
            "batches_trained: 714 total_loss: tensor(0.0130) acc: tensor(0.8008)\n",
            "batches_trained: 715 total_loss: tensor(0.0130) acc: tensor(0.8009)\n",
            "batches_trained: 716 total_loss: tensor(0.0130) acc: tensor(0.8009)\n",
            "batches_trained: 717 total_loss: tensor(0.0130) acc: tensor(0.8010)\n",
            "batches_trained: 718 total_loss: tensor(0.0130) acc: tensor(0.8011)\n",
            "batches_trained: 719 total_loss: tensor(0.0130) acc: tensor(0.8010)\n",
            "batches_trained: 720 total_loss: tensor(0.0130) acc: tensor(0.8011)\n",
            "batches_trained: 721 total_loss: tensor(0.0130) acc: tensor(0.8012)\n",
            "batches_trained: 722 total_loss: tensor(0.0130) acc: tensor(0.8013)\n",
            "batches_trained: 723 total_loss: tensor(0.0130) acc: tensor(0.8012)\n",
            "batches_trained: 724 total_loss: tensor(0.0130) acc: tensor(0.8011)\n",
            "batches_trained: 725 total_loss: tensor(0.0130) acc: tensor(0.8012)\n",
            "batches_trained: 726 total_loss: tensor(0.0130) acc: tensor(0.8013)\n",
            "batches_trained: 727 total_loss: tensor(0.0130) acc: tensor(0.8013)\n",
            "batches_trained: 728 total_loss: tensor(0.0130) acc: tensor(0.8015)\n",
            "batches_trained: 729 total_loss: tensor(0.0130) acc: tensor(0.8015)\n",
            "batches_trained: 730 total_loss: tensor(0.0130) acc: tensor(0.8017)\n",
            "batches_trained: 731 total_loss: tensor(0.0130) acc: tensor(0.8017)\n",
            "batches_trained: 732 total_loss: tensor(0.0130) acc: tensor(0.8019)\n",
            "batches_trained: 733 total_loss: tensor(0.0130) acc: tensor(0.8020)\n",
            "batches_trained: 734 total_loss: tensor(0.0130) acc: tensor(0.8020)\n",
            "batches_trained: 735 total_loss: tensor(0.0130) acc: tensor(0.8021)\n",
            "batches_trained: 736 total_loss: tensor(0.0130) acc: tensor(0.8021)\n",
            "batches_trained: 737 total_loss: tensor(0.0130) acc: tensor(0.8023)\n",
            "batches_trained: 738 total_loss: tensor(0.0130) acc: tensor(0.8023)\n",
            "batches_trained: 739 total_loss: tensor(0.0130) acc: tensor(0.8024)\n",
            "batches_trained: 740 total_loss: tensor(0.0130) acc: tensor(0.8023)\n",
            "batches_trained: 741 total_loss: tensor(0.0130) acc: tensor(0.8026)\n",
            "batches_trained: 742 total_loss: tensor(0.0130) acc: tensor(0.8026)\n",
            "batches_trained: 743 total_loss: tensor(0.0130) acc: tensor(0.8027)\n",
            "batches_trained: 744 total_loss: tensor(0.0130) acc: tensor(0.8027)\n",
            "batches_trained: 745 total_loss: tensor(0.0130) acc: tensor(0.8027)\n",
            "batches_trained: 746 total_loss: tensor(0.0130) acc: tensor(0.8027)\n",
            "batches_trained: 747 total_loss: tensor(0.0130) acc: tensor(0.8028)\n",
            "batches_trained: 748 total_loss: tensor(0.0130) acc: tensor(0.8028)\n",
            "batches_trained: 749 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 750 total_loss: tensor(0.0130) acc: tensor(0.8030)\n",
            "batches_trained: 751 total_loss: tensor(0.0130) acc: tensor(0.8030)\n",
            "batches_trained: 752 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 753 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 754 total_loss: tensor(0.0130) acc: tensor(0.8028)\n",
            "batches_trained: 755 total_loss: tensor(0.0130) acc: tensor(0.8026)\n",
            "batches_trained: 756 total_loss: tensor(0.0130) acc: tensor(0.8026)\n",
            "batches_trained: 757 total_loss: tensor(0.0130) acc: tensor(0.8028)\n",
            "batches_trained: 758 total_loss: tensor(0.0130) acc: tensor(0.8026)\n",
            "batches_trained: 759 total_loss: tensor(0.0130) acc: tensor(0.8027)\n",
            "batches_trained: 760 total_loss: tensor(0.0130) acc: tensor(0.8028)\n",
            "batches_trained: 761 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 762 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 763 total_loss: tensor(0.0130) acc: tensor(0.8030)\n",
            "batches_trained: 764 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 765 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 766 total_loss: tensor(0.0130) acc: tensor(0.8030)\n",
            "batches_trained: 767 total_loss: tensor(0.0130) acc: tensor(0.8028)\n",
            "batches_trained: 768 total_loss: tensor(0.0130) acc: tensor(0.8029)\n",
            "batches_trained: 769 total_loss: tensor(0.0129) acc: tensor(0.8030)\n",
            "batches_trained: 770 total_loss: tensor(0.0129) acc: tensor(0.8030)\n",
            "batches_trained: 771 total_loss: tensor(0.0129) acc: tensor(0.8031)\n",
            "batches_trained: 772 total_loss: tensor(0.0129) acc: tensor(0.8031)\n",
            "batches_trained: 773 total_loss: tensor(0.0129) acc: tensor(0.8030)\n",
            "batches_trained: 774 total_loss: tensor(0.0129) acc: tensor(0.8032)\n",
            "batches_trained: 775 total_loss: tensor(0.0129) acc: tensor(0.8031)\n",
            "batches_trained: 776 total_loss: tensor(0.0129) acc: tensor(0.8031)\n",
            "batches_trained: 777 total_loss: tensor(0.0129) acc: tensor(0.8033)\n",
            "batches_trained: 778 total_loss: tensor(0.0129) acc: tensor(0.8034)\n",
            "batches_trained: 779 total_loss: tensor(0.0129) acc: tensor(0.8036)\n",
            "batches_trained: 780 total_loss: tensor(0.0129) acc: tensor(0.8036)\n",
            "batches_trained: 781 total_loss: tensor(0.0129) acc: tensor(0.8037)\n",
            "batches_trained: 782 total_loss: tensor(0.0129) acc: tensor(0.8038)\n",
            "batches_trained: 783 total_loss: tensor(0.0129) acc: tensor(0.8039)\n",
            "batches_trained: 784 total_loss: tensor(0.0129) acc: tensor(0.8039)\n",
            "batches_trained: 785 total_loss: tensor(0.0129) acc: tensor(0.8040)\n",
            "batches_trained: 786 total_loss: tensor(0.0129) acc: tensor(0.8040)\n",
            "batches_trained: 787 total_loss: tensor(0.0129) acc: tensor(0.8040)\n",
            "batches_trained: 788 total_loss: tensor(0.0129) acc: tensor(0.8040)\n",
            "batches_trained: 789 total_loss: tensor(0.0129) acc: tensor(0.8039)\n",
            "batches_trained: 790 total_loss: tensor(0.0129) acc: tensor(0.8040)\n",
            "batches_trained: 791 total_loss: tensor(0.0129) acc: tensor(0.8040)\n",
            "batches_trained: 792 total_loss: tensor(0.0129) acc: tensor(0.8041)\n",
            "batches_trained: 793 total_loss: tensor(0.0129) acc: tensor(0.8042)\n",
            "batches_trained: 794 total_loss: tensor(0.0129) acc: tensor(0.8044)\n",
            "batches_trained: 795 total_loss: tensor(0.0129) acc: tensor(0.8046)\n",
            "batches_trained: 796 total_loss: tensor(0.0129) acc: tensor(0.8046)\n",
            "batches_trained: 797 total_loss: tensor(0.0129) acc: tensor(0.8047)\n",
            "batches_trained: 798 total_loss: tensor(0.0128) acc: tensor(0.8047)\n",
            "batches_trained: 799 total_loss: tensor(0.0128) acc: tensor(0.8048)\n",
            "batches_trained: 800 total_loss: tensor(0.0128) acc: tensor(0.8048)\n",
            "batches_trained: 801 total_loss: tensor(0.0128) acc: tensor(0.8047)\n",
            "batches_trained: 802 total_loss: tensor(0.0128) acc: tensor(0.8048)\n",
            "batches_trained: 803 total_loss: tensor(0.0128) acc: tensor(0.8047)\n",
            "batches_trained: 804 total_loss: tensor(0.0128) acc: tensor(0.8049)\n",
            "batches_trained: 805 total_loss: tensor(0.0128) acc: tensor(0.8049)\n",
            "batches_trained: 806 total_loss: tensor(0.0128) acc: tensor(0.8047)\n",
            "batches_trained: 807 total_loss: tensor(0.0128) acc: tensor(0.8049)\n",
            "batches_trained: 808 total_loss: tensor(0.0128) acc: tensor(0.8050)\n",
            "batches_trained: 809 total_loss: tensor(0.0128) acc: tensor(0.8051)\n",
            "batches_trained: 810 total_loss: tensor(0.0128) acc: tensor(0.8051)\n",
            "batches_trained: 811 total_loss: tensor(0.0128) acc: tensor(0.8051)\n",
            "batches_trained: 812 total_loss: tensor(0.0128) acc: tensor(0.8051)\n",
            "batches_trained: 813 total_loss: tensor(0.0128) acc: tensor(0.8052)\n",
            "batches_trained: 814 total_loss: tensor(0.0128) acc: tensor(0.8052)\n",
            "batches_trained: 815 total_loss: tensor(0.0128) acc: tensor(0.8053)\n",
            "batches_trained: 816 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 817 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 818 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 819 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 820 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 821 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 822 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 823 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 824 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 825 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 826 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 827 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 828 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 829 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 830 total_loss: tensor(0.0128) acc: tensor(0.8055)\n",
            "batches_trained: 831 total_loss: tensor(0.0128) acc: tensor(0.8054)\n",
            "batches_trained: 832 total_loss: tensor(0.0128) acc: tensor(0.8056)\n",
            "batches_trained: 833 total_loss: tensor(0.0128) acc: tensor(0.8056)\n",
            "batches_trained: 834 total_loss: tensor(0.0128) acc: tensor(0.8057)\n",
            "batches_trained: 835 total_loss: tensor(0.0128) acc: tensor(0.8057)\n",
            "batches_trained: 836 total_loss: tensor(0.0128) acc: tensor(0.8058)\n",
            "batches_trained: 837 total_loss: tensor(0.0128) acc: tensor(0.8060)\n",
            "batches_trained: 838 total_loss: tensor(0.0128) acc: tensor(0.8059)\n",
            "batches_trained: 839 total_loss: tensor(0.0128) acc: tensor(0.8061)\n",
            "batches_trained: 840 total_loss: tensor(0.0128) acc: tensor(0.8060)\n",
            "batches_trained: 841 total_loss: tensor(0.0128) acc: tensor(0.8061)\n",
            "batches_trained: 842 total_loss: tensor(0.0128) acc: tensor(0.8061)\n",
            "batches_trained: 843 total_loss: tensor(0.0128) acc: tensor(0.8062)\n",
            "batches_trained: 844 total_loss: tensor(0.0128) acc: tensor(0.8062)\n",
            "batches_trained: 845 total_loss: tensor(0.0128) acc: tensor(0.8062)\n",
            "batches_trained: 846 total_loss: tensor(0.0128) acc: tensor(0.8062)\n",
            "batches_trained: 847 total_loss: tensor(0.0128) acc: tensor(0.8062)\n",
            "batches_trained: 848 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 849 total_loss: tensor(0.0128) acc: tensor(0.8062)\n",
            "batches_trained: 850 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 851 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 852 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 853 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 854 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 855 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 856 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 857 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 858 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 859 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 860 total_loss: tensor(0.0128) acc: tensor(0.8063)\n",
            "batches_trained: 861 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 862 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 863 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 864 total_loss: tensor(0.0128) acc: tensor(0.8065)\n",
            "batches_trained: 865 total_loss: tensor(0.0128) acc: tensor(0.8065)\n",
            "batches_trained: 866 total_loss: tensor(0.0128) acc: tensor(0.8064)\n",
            "batches_trained: 867 total_loss: tensor(0.0128) acc: tensor(0.8066)\n",
            "batches_trained: 868 total_loss: tensor(0.0128) acc: tensor(0.8067)\n",
            "batches_trained: 869 total_loss: tensor(0.0128) acc: tensor(0.8067)\n",
            "batches_trained: 870 total_loss: tensor(0.0128) acc: tensor(0.8067)\n",
            "batches_trained: 871 total_loss: tensor(0.0128) acc: tensor(0.8068)\n",
            "batches_trained: 872 total_loss: tensor(0.0128) acc: tensor(0.8068)\n",
            "batches_trained: 873 total_loss: tensor(0.0127) acc: tensor(0.8068)\n",
            "batches_trained: 874 total_loss: tensor(0.0127) acc: tensor(0.8069)\n",
            "batches_trained: 875 total_loss: tensor(0.0128) acc: tensor(0.8069)\n",
            "batches_trained: 876 total_loss: tensor(0.0127) acc: tensor(0.8070)\n",
            "batches_trained: 877 total_loss: tensor(0.0127) acc: tensor(0.8070)\n",
            "batches_trained: 878 total_loss: tensor(0.0127) acc: tensor(0.8071)\n",
            "batches_trained: 879 total_loss: tensor(0.0127) acc: tensor(0.8071)\n",
            "batches_trained: 880 total_loss: tensor(0.0127) acc: tensor(0.8071)\n",
            "batches_trained: 881 total_loss: tensor(0.0127) acc: tensor(0.8070)\n",
            "batches_trained: 882 total_loss: tensor(0.0127) acc: tensor(0.8070)\n",
            "batches_trained: 883 total_loss: tensor(0.0127) acc: tensor(0.8071)\n",
            "batches_trained: 884 total_loss: tensor(0.0127) acc: tensor(0.8071)\n",
            "batches_trained: 885 total_loss: tensor(0.0127) acc: tensor(0.8071)\n",
            "batches_trained: 886 total_loss: tensor(0.0127) acc: tensor(0.8072)\n",
            "batches_trained: 887 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 888 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 889 total_loss: tensor(0.0127) acc: tensor(0.8074)\n",
            "batches_trained: 890 total_loss: tensor(0.0127) acc: tensor(0.8074)\n",
            "batches_trained: 891 total_loss: tensor(0.0127) acc: tensor(0.8074)\n",
            "batches_trained: 892 total_loss: tensor(0.0127) acc: tensor(0.8072)\n",
            "batches_trained: 893 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 894 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 895 total_loss: tensor(0.0127) acc: tensor(0.8072)\n",
            "batches_trained: 896 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 897 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 898 total_loss: tensor(0.0127) acc: tensor(0.8072)\n",
            "batches_trained: 899 total_loss: tensor(0.0127) acc: tensor(0.8072)\n",
            "batches_trained: 900 total_loss: tensor(0.0127) acc: tensor(0.8073)\n",
            "batches_trained: 901 total_loss: tensor(0.0127) acc: tensor(0.8074)\n",
            "batches_trained: 902 total_loss: tensor(0.0127) acc: tensor(0.8075)\n",
            "batches_trained: 903 total_loss: tensor(0.0127) acc: tensor(0.8075)\n",
            "batches_trained: 904 total_loss: tensor(0.0127) acc: tensor(0.8074)\n",
            "batches_trained: 905 total_loss: tensor(0.0127) acc: tensor(0.8075)\n",
            "batches_trained: 906 total_loss: tensor(0.0127) acc: tensor(0.8076)\n",
            "batches_trained: 907 total_loss: tensor(0.0127) acc: tensor(0.8075)\n",
            "batches_trained: 908 total_loss: tensor(0.0127) acc: tensor(0.8075)\n",
            "batches_trained: 909 total_loss: tensor(0.0127) acc: tensor(0.8077)\n",
            "batches_trained: 910 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 911 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 912 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 913 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 914 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 915 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 916 total_loss: tensor(0.0127) acc: tensor(0.8077)\n",
            "batches_trained: 917 total_loss: tensor(0.0127) acc: tensor(0.8078)\n",
            "batches_trained: 918 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 919 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 920 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 921 total_loss: tensor(0.0127) acc: tensor(0.8079)\n",
            "batches_trained: 922 total_loss: tensor(0.0127) acc: tensor(0.8080)\n",
            "batches_trained: 923 total_loss: tensor(0.0127) acc: tensor(0.8080)\n",
            "batches_trained: 924 total_loss: tensor(0.0127) acc: tensor(0.8080)\n",
            "batches_trained: 925 total_loss: tensor(0.0127) acc: tensor(0.8080)\n",
            "batches_trained: 926 total_loss: tensor(0.0127) acc: tensor(0.8081)\n",
            "batches_trained: 927 total_loss: tensor(0.0127) acc: tensor(0.8082)\n",
            "batches_trained: 928 total_loss: tensor(0.0127) acc: tensor(0.8083)\n",
            "batches_trained: 929 total_loss: tensor(0.0127) acc: tensor(0.8082)\n",
            "batches_trained: 930 total_loss: tensor(0.0127) acc: tensor(0.8081)\n",
            "batches_trained: 931 total_loss: tensor(0.0127) acc: tensor(0.8082)\n",
            "batches_trained: 932 total_loss: tensor(0.0127) acc: tensor(0.8082)\n",
            "batches_trained: 933 total_loss: tensor(0.0127) acc: tensor(0.8083)\n",
            "batches_trained: 934 total_loss: tensor(0.0127) acc: tensor(0.8083)\n",
            "batches_trained: 935 total_loss: tensor(0.0127) acc: tensor(0.8083)\n",
            "batches_trained: 936 total_loss: tensor(0.0127) acc: tensor(0.8084)\n",
            "batches_trained: 937 total_loss: tensor(0.0127) acc: tensor(0.8085)\n",
            "batches_trained: 938 total_loss: tensor(0.0127) acc: tensor(0.8085)\n",
            "batches_trained: 939 total_loss: tensor(0.0127) acc: tensor(0.8086)\n",
            "batches_trained: 940 total_loss: tensor(0.0127) acc: tensor(0.8086)\n",
            "batches_trained: 941 total_loss: tensor(0.0127) acc: tensor(0.8087)\n",
            "batches_trained: 942 total_loss: tensor(0.0127) acc: tensor(0.8088)\n",
            "batches_trained: 943 total_loss: tensor(0.0127) acc: tensor(0.8088)\n",
            "batches_trained: 944 total_loss: tensor(0.0127) acc: tensor(0.8089)\n",
            "batches_trained: 945 total_loss: tensor(0.0126) acc: tensor(0.8090)\n",
            "batches_trained: 946 total_loss: tensor(0.0126) acc: tensor(0.8090)\n",
            "batches_trained: 947 total_loss: tensor(0.0126) acc: tensor(0.8089)\n",
            "batches_trained: 948 total_loss: tensor(0.0126) acc: tensor(0.8089)\n",
            "batches_trained: 949 total_loss: tensor(0.0126) acc: tensor(0.8090)\n",
            "batches_trained: 950 total_loss: tensor(0.0126) acc: tensor(0.8090)\n",
            "batches_trained: 951 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 952 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 953 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 954 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 955 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 956 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 957 total_loss: tensor(0.0126) acc: tensor(0.8093)\n",
            "batches_trained: 958 total_loss: tensor(0.0126) acc: tensor(0.8093)\n",
            "batches_trained: 959 total_loss: tensor(0.0126) acc: tensor(0.8093)\n",
            "batches_trained: 960 total_loss: tensor(0.0126) acc: tensor(0.8093)\n",
            "batches_trained: 961 total_loss: tensor(0.0126) acc: tensor(0.8093)\n",
            "batches_trained: 962 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 963 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 964 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 965 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 966 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 967 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 968 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 969 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 970 total_loss: tensor(0.0126) acc: tensor(0.8091)\n",
            "batches_trained: 971 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 972 total_loss: tensor(0.0126) acc: tensor(0.8092)\n",
            "batches_trained: 973 total_loss: tensor(0.0126) acc: tensor(0.8094)\n",
            "batches_trained: 974 total_loss: tensor(0.0126) acc: tensor(0.8093)\n",
            "batches_trained: 975 total_loss: tensor(0.0126) acc: tensor(0.8094)\n",
            "batches_trained: 976 total_loss: tensor(0.0126) acc: tensor(0.8094)\n",
            "batches_trained: 977 total_loss: tensor(0.0126) acc: tensor(0.8094)\n",
            "batches_trained: 978 total_loss: tensor(0.0126) acc: tensor(0.8094)\n",
            "batches_trained: 979 total_loss: tensor(0.0126) acc: tensor(0.8095)\n",
            "batches_trained: 980 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 981 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 982 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 983 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 984 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 985 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 986 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 987 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 988 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 989 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 990 total_loss: tensor(0.0126) acc: tensor(0.8095)\n",
            "batches_trained: 991 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 992 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 993 total_loss: tensor(0.0126) acc: tensor(0.8095)\n",
            "batches_trained: 994 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 995 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 996 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 997 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 998 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 999 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1000 total_loss: tensor(0.0126) acc: tensor(0.8096)\n",
            "batches_trained: 1001 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1002 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1003 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1004 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1005 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1006 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1007 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1008 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1009 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1010 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1011 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1012 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1013 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1014 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1015 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1016 total_loss: tensor(0.0126) acc: tensor(0.8097)\n",
            "batches_trained: 1017 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1018 total_loss: tensor(0.0126) acc: tensor(0.8098)\n",
            "batches_trained: 1019 total_loss: tensor(0.0125) acc: tensor(0.8099)\n",
            "batches_trained: 1020 total_loss: tensor(0.0125) acc: tensor(0.8099)\n",
            "batches_trained: 1021 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1022 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1023 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1024 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1025 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1026 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1027 total_loss: tensor(0.0125) acc: tensor(0.8100)\n",
            "batches_trained: 1028 total_loss: tensor(0.0125) acc: tensor(0.8102)\n",
            "batches_trained: 1029 total_loss: tensor(0.0125) acc: tensor(0.8103)\n",
            "batches_trained: 1030 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1031 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1032 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1033 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1034 total_loss: tensor(0.0125) acc: tensor(0.8105)\n",
            "batches_trained: 1035 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1036 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1037 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1038 total_loss: tensor(0.0125) acc: tensor(0.8104)\n",
            "batches_trained: 1039 total_loss: tensor(0.0125) acc: tensor(0.8105)\n",
            "batches_trained: 1040 total_loss: tensor(0.0125) acc: tensor(0.8106)\n",
            "batches_trained: 1041 total_loss: tensor(0.0125) acc: tensor(0.8105)\n",
            "batches_trained: 1042 total_loss: tensor(0.0125) acc: tensor(0.8106)\n",
            "batches_trained: 1043 total_loss: tensor(0.0125) acc: tensor(0.8108)\n",
            "batches_trained: 1044 total_loss: tensor(0.0125) acc: tensor(0.8108)\n",
            "batches_trained: 1045 total_loss: tensor(0.0125) acc: tensor(0.8109)\n",
            "batches_trained: 1046 total_loss: tensor(0.0125) acc: tensor(0.8108)\n",
            "batches_trained: 1047 total_loss: tensor(0.0125) acc: tensor(0.8109)\n",
            "batches_trained: 1048 total_loss: tensor(0.0125) acc: tensor(0.8109)\n",
            "batches_trained: 1049 total_loss: tensor(0.0125) acc: tensor(0.8109)\n",
            "batches_trained: 1050 total_loss: tensor(0.0125) acc: tensor(0.8108)\n",
            "batches_trained: 1051 total_loss: tensor(0.0125) acc: tensor(0.8108)\n",
            "batches_trained: 1052 total_loss: tensor(0.0125) acc: tensor(0.8108)\n",
            "batches_trained: 1053 total_loss: tensor(0.0125) acc: tensor(0.8109)\n",
            "batches_trained: 1054 total_loss: tensor(0.0125) acc: tensor(0.8109)\n",
            "batches_trained: 1055 total_loss: tensor(0.0125) acc: tensor(0.8111)\n",
            "batches_trained: 1056 total_loss: tensor(0.0125) acc: tensor(0.8112)\n",
            "batches_trained: 1057 total_loss: tensor(0.0125) acc: tensor(0.8112)\n",
            "batches_trained: 1058 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1059 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1060 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1061 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1062 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1063 total_loss: tensor(0.0124) acc: tensor(0.8114)\n",
            "batches_trained: 1064 total_loss: tensor(0.0124) acc: tensor(0.8114)\n",
            "batches_trained: 1065 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1066 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1067 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1068 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1069 total_loss: tensor(0.0125) acc: tensor(0.8112)\n",
            "batches_trained: 1070 total_loss: tensor(0.0125) acc: tensor(0.8112)\n",
            "batches_trained: 1071 total_loss: tensor(0.0125) acc: tensor(0.8112)\n",
            "batches_trained: 1072 total_loss: tensor(0.0125) acc: tensor(0.8112)\n",
            "batches_trained: 1073 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1074 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1075 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1076 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1077 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1078 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1079 total_loss: tensor(0.0125) acc: tensor(0.8115)\n",
            "batches_trained: 1080 total_loss: tensor(0.0125) acc: tensor(0.8115)\n",
            "batches_trained: 1081 total_loss: tensor(0.0125) acc: tensor(0.8115)\n",
            "batches_trained: 1082 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1083 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1084 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1085 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1086 total_loss: tensor(0.0125) acc: tensor(0.8115)\n",
            "batches_trained: 1087 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1088 total_loss: tensor(0.0125) acc: tensor(0.8113)\n",
            "batches_trained: 1089 total_loss: tensor(0.0125) acc: tensor(0.8114)\n",
            "batches_trained: 1090 total_loss: tensor(0.0124) acc: tensor(0.8115)\n",
            "batches_trained: 1091 total_loss: tensor(0.0124) acc: tensor(0.8115)\n",
            "batches_trained: 1092 total_loss: tensor(0.0124) acc: tensor(0.8116)\n",
            "batches_trained: 1093 total_loss: tensor(0.0125) acc: tensor(0.8116)\n",
            "batches_trained: 1094 total_loss: tensor(0.0124) acc: tensor(0.8116)\n",
            "batches_trained: 1095 total_loss: tensor(0.0124) acc: tensor(0.8116)\n",
            "batches_trained: 1096 total_loss: tensor(0.0124) acc: tensor(0.8116)\n",
            "batches_trained: 1097 total_loss: tensor(0.0124) acc: tensor(0.8116)\n",
            "batches_trained: 1098 total_loss: tensor(0.0124) acc: tensor(0.8117)\n",
            "batches_trained: 1099 total_loss: tensor(0.0124) acc: tensor(0.8117)\n",
            "batches_trained: 1100 total_loss: tensor(0.0124) acc: tensor(0.8117)\n",
            "batches_trained: 1101 total_loss: tensor(0.0124) acc: tensor(0.8117)\n",
            "batches_trained: 1102 total_loss: tensor(0.0124) acc: tensor(0.8117)\n",
            "batches_trained: 1103 total_loss: tensor(0.0124) acc: tensor(0.8118)\n",
            "batches_trained: 1104 total_loss: tensor(0.0124) acc: tensor(0.8118)\n",
            "batches_trained: 1105 total_loss: tensor(0.0124) acc: tensor(0.8119)\n",
            "batches_trained: 1106 total_loss: tensor(0.0124) acc: tensor(0.8118)\n",
            "batches_trained: 1107 total_loss: tensor(0.0124) acc: tensor(0.8119)\n",
            "batches_trained: 1108 total_loss: tensor(0.0124) acc: tensor(0.8119)\n",
            "batches_trained: 1109 total_loss: tensor(0.0124) acc: tensor(0.8119)\n",
            "batches_trained: 1110 total_loss: tensor(0.0124) acc: tensor(0.8120)\n",
            "batches_trained: 1111 total_loss: tensor(0.0124) acc: tensor(0.8119)\n",
            "batches_trained: 1112 total_loss: tensor(0.0124) acc: tensor(0.8120)\n",
            "batches_trained: 1113 total_loss: tensor(0.0124) acc: tensor(0.8120)\n",
            "batches_trained: 1114 total_loss: tensor(0.0124) acc: tensor(0.8121)\n",
            "batches_trained: 1115 total_loss: tensor(0.0124) acc: tensor(0.8122)\n",
            "batches_trained: 1116 total_loss: tensor(0.0124) acc: tensor(0.8123)\n",
            "batches_trained: 1117 total_loss: tensor(0.0124) acc: tensor(0.8122)\n",
            "batches_trained: 1118 total_loss: tensor(0.0124) acc: tensor(0.8122)\n",
            "batches_trained: 1119 total_loss: tensor(0.0124) acc: tensor(0.8123)\n",
            "batches_trained: 1120 total_loss: tensor(0.0124) acc: tensor(0.8124)\n",
            "batches_trained: 1121 total_loss: tensor(0.0124) acc: tensor(0.8125)\n",
            "batches_trained: 1122 total_loss: tensor(0.0124) acc: tensor(0.8126)\n",
            "batches_trained: 1123 total_loss: tensor(0.0124) acc: tensor(0.8126)\n",
            "batches_trained: 1124 total_loss: tensor(0.0124) acc: tensor(0.8126)\n",
            "batches_trained: 1125 total_loss: tensor(0.0124) acc: tensor(0.8126)\n",
            "batches_trained: 1126 total_loss: tensor(0.0124) acc: tensor(0.8127)\n",
            "batches_trained: 1127 total_loss: tensor(0.0124) acc: tensor(0.8128)\n",
            "batches_trained: 1128 total_loss: tensor(0.0124) acc: tensor(0.8129)\n",
            "batches_trained: 1129 total_loss: tensor(0.0124) acc: tensor(0.8129)\n",
            "batches_trained: 1130 total_loss: tensor(0.0124) acc: tensor(0.8129)\n",
            "batches_trained: 1131 total_loss: tensor(0.0124) acc: tensor(0.8129)\n",
            "batches_trained: 1132 total_loss: tensor(0.0124) acc: tensor(0.8129)\n",
            "batches_trained: 1133 total_loss: tensor(0.0124) acc: tensor(0.8130)\n",
            "batches_trained: 1134 total_loss: tensor(0.0124) acc: tensor(0.8130)\n",
            "batches_trained: 1135 total_loss: tensor(0.0124) acc: tensor(0.8130)\n",
            "batches_trained: 1136 total_loss: tensor(0.0124) acc: tensor(0.8131)\n",
            "batches_trained: 1137 total_loss: tensor(0.0124) acc: tensor(0.8131)\n",
            "batches_trained: 1138 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1139 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1140 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1141 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1142 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1143 total_loss: tensor(0.0124) acc: tensor(0.8131)\n",
            "batches_trained: 1144 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1145 total_loss: tensor(0.0124) acc: tensor(0.8133)\n",
            "batches_trained: 1146 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1147 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1148 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1149 total_loss: tensor(0.0124) acc: tensor(0.8133)\n",
            "batches_trained: 1150 total_loss: tensor(0.0123) acc: tensor(0.8133)\n",
            "batches_trained: 1151 total_loss: tensor(0.0124) acc: tensor(0.8132)\n",
            "batches_trained: 1152 total_loss: tensor(0.0124) acc: tensor(0.8133)\n",
            "batches_trained: 1153 total_loss: tensor(0.0123) acc: tensor(0.8133)\n",
            "batches_trained: 1154 total_loss: tensor(0.0123) acc: tensor(0.8133)\n",
            "batches_trained: 1155 total_loss: tensor(0.0124) acc: tensor(0.8133)\n",
            "batches_trained: 1156 total_loss: tensor(0.0123) acc: tensor(0.8133)\n",
            "batches_trained: 1157 total_loss: tensor(0.0123) acc: tensor(0.8134)\n",
            "batches_trained: 1158 total_loss: tensor(0.0123) acc: tensor(0.8134)\n",
            "batches_trained: 1159 total_loss: tensor(0.0123) acc: tensor(0.8134)\n",
            "batches_trained: 1160 total_loss: tensor(0.0123) acc: tensor(0.8135)\n",
            "batches_trained: 1161 total_loss: tensor(0.0123) acc: tensor(0.8135)\n",
            "batches_trained: 1162 total_loss: tensor(0.0123) acc: tensor(0.8135)\n",
            "batches_trained: 1163 total_loss: tensor(0.0123) acc: tensor(0.8136)\n",
            "batches_trained: 1164 total_loss: tensor(0.0123) acc: tensor(0.8136)\n",
            "batches_trained: 1165 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1166 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1167 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1168 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1169 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1170 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1171 total_loss: tensor(0.0123) acc: tensor(0.8138)\n",
            "batches_trained: 1172 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1173 total_loss: tensor(0.0123) acc: tensor(0.8138)\n",
            "batches_trained: 1174 total_loss: tensor(0.0123) acc: tensor(0.8137)\n",
            "batches_trained: 1175 total_loss: tensor(0.0123) acc: tensor(0.8138)\n",
            "batches_trained: 1176 total_loss: tensor(0.0123) acc: tensor(0.8139)\n",
            "batches_trained: 1177 total_loss: tensor(0.0123) acc: tensor(0.8139)\n",
            "batches_trained: 1178 total_loss: tensor(0.0123) acc: tensor(0.8139)\n",
            "batches_trained: 1179 total_loss: tensor(0.0123) acc: tensor(0.8140)\n",
            "batches_trained: 1180 total_loss: tensor(0.0123) acc: tensor(0.8140)\n",
            "batches_trained: 1181 total_loss: tensor(0.0123) acc: tensor(0.8139)\n",
            "batches_trained: 1182 total_loss: tensor(0.0123) acc: tensor(0.8139)\n",
            "batches_trained: 1183 total_loss: tensor(0.0123) acc: tensor(0.8140)\n",
            "batches_trained: 1184 total_loss: tensor(0.0123) acc: tensor(0.8140)\n",
            "batches_trained: 1185 total_loss: tensor(0.0123) acc: tensor(0.8142)\n",
            "batches_trained: 1186 total_loss: tensor(0.0123) acc: tensor(0.8142)\n",
            "batches_trained: 1187 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1188 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1189 total_loss: tensor(0.0123) acc: tensor(0.8142)\n",
            "batches_trained: 1190 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1191 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1192 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1193 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1194 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1195 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1196 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1197 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1198 total_loss: tensor(0.0123) acc: tensor(0.8143)\n",
            "batches_trained: 1199 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1200 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1201 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1202 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1203 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1204 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1205 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1206 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1207 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1208 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1209 total_loss: tensor(0.0123) acc: tensor(0.8144)\n",
            "batches_trained: 1210 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1211 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1212 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1213 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1214 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1215 total_loss: tensor(0.0123) acc: tensor(0.8145)\n",
            "batches_trained: 1216 total_loss: tensor(0.0123) acc: tensor(0.8146)\n",
            "batches_trained: 1217 total_loss: tensor(0.0123) acc: tensor(0.8146)\n",
            "batches_trained: 1218 total_loss: tensor(0.0123) acc: tensor(0.8147)\n",
            "batches_trained: 1219 total_loss: tensor(0.0123) acc: tensor(0.8146)\n",
            "batches_trained: 1220 total_loss: tensor(0.0123) acc: tensor(0.8147)\n",
            "batches_trained: 1221 total_loss: tensor(0.0123) acc: tensor(0.8147)\n",
            "batches_trained: 1222 total_loss: tensor(0.0123) acc: tensor(0.8147)\n",
            "batches_trained: 1223 total_loss: tensor(0.0123) acc: tensor(0.8147)\n",
            "batches_trained: 1224 total_loss: tensor(0.0123) acc: tensor(0.8148)\n",
            "batches_trained: 1225 total_loss: tensor(0.0123) acc: tensor(0.8148)\n",
            "batches_trained: 1226 total_loss: tensor(0.0123) acc: tensor(0.8148)\n",
            "batches_trained: 1227 total_loss: tensor(0.0123) acc: tensor(0.8148)\n",
            "batches_trained: 1228 total_loss: tensor(0.0123) acc: tensor(0.8148)\n",
            "batches_trained: 1229 total_loss: tensor(0.0123) acc: tensor(0.8148)\n",
            "batches_trained: 1230 total_loss: tensor(0.0123) acc: tensor(0.8149)\n",
            "batches_trained: 1231 total_loss: tensor(0.0122) acc: tensor(0.8149)\n",
            "batches_trained: 1232 total_loss: tensor(0.0122) acc: tensor(0.8150)\n",
            "batches_trained: 1233 total_loss: tensor(0.0122) acc: tensor(0.8150)\n",
            "batches_trained: 1234 total_loss: tensor(0.0122) acc: tensor(0.8150)\n",
            "batches_trained: 1235 total_loss: tensor(0.0122) acc: tensor(0.8150)\n",
            "batches_trained: 1236 total_loss: tensor(0.0122) acc: tensor(0.8151)\n",
            "batches_trained: 1237 total_loss: tensor(0.0122) acc: tensor(0.8151)\n",
            "batches_trained: 1238 total_loss: tensor(0.0122) acc: tensor(0.8152)\n",
            "batches_trained: 1239 total_loss: tensor(0.0122) acc: tensor(0.8152)\n",
            "batches_trained: 1240 total_loss: tensor(0.0122) acc: tensor(0.8152)\n",
            "batches_trained: 1241 total_loss: tensor(0.0122) acc: tensor(0.8152)\n",
            "batches_trained: 1242 total_loss: tensor(0.0122) acc: tensor(0.8153)\n",
            "batches_trained: 1243 total_loss: tensor(0.0122) acc: tensor(0.8154)\n",
            "batches_trained: 1244 total_loss: tensor(0.0122) acc: tensor(0.8154)\n",
            "batches_trained: 1245 total_loss: tensor(0.0122) acc: tensor(0.8155)\n",
            "batches_trained: 1246 total_loss: tensor(0.0122) acc: tensor(0.8155)\n",
            "batches_trained: 1247 total_loss: tensor(0.0122) acc: tensor(0.8155)\n",
            "batches_trained: 1248 total_loss: tensor(0.0122) acc: tensor(0.8156)\n",
            "batches_trained: 1249 total_loss: tensor(0.0122) acc: tensor(0.8155)\n",
            "batches_trained: 1250 total_loss: tensor(0.0122) acc: tensor(0.8155)\n",
            "Epoch completed: 1 total_loss: tensor(488.9027) acc: tensor(0.8155)\n",
            "batches_trained: 1 total_loss: tensor(0.0050) acc: tensor(0.9688)\n",
            "batches_trained: 2 total_loss: tensor(0.0051) acc: tensor(0.9531)\n",
            "batches_trained: 3 total_loss: tensor(0.0057) acc: tensor(0.9375)\n",
            "batches_trained: 4 total_loss: tensor(0.0067) acc: tensor(0.9062)\n",
            "batches_trained: 5 total_loss: tensor(0.0072) acc: tensor(0.9000)\n",
            "batches_trained: 6 total_loss: tensor(0.0073) acc: tensor(0.8958)\n",
            "batches_trained: 7 total_loss: tensor(0.0070) acc: tensor(0.9018)\n",
            "batches_trained: 8 total_loss: tensor(0.0070) acc: tensor(0.9062)\n",
            "batches_trained: 9 total_loss: tensor(0.0071) acc: tensor(0.9062)\n",
            "batches_trained: 10 total_loss: tensor(0.0067) acc: tensor(0.9125)\n",
            "batches_trained: 11 total_loss: tensor(0.0066) acc: tensor(0.9119)\n",
            "batches_trained: 12 total_loss: tensor(0.0065) acc: tensor(0.9115)\n",
            "batches_trained: 13 total_loss: tensor(0.0066) acc: tensor(0.9087)\n",
            "batches_trained: 14 total_loss: tensor(0.0064) acc: tensor(0.9129)\n",
            "batches_trained: 15 total_loss: tensor(0.0065) acc: tensor(0.9125)\n",
            "batches_trained: 16 total_loss: tensor(0.0064) acc: tensor(0.9102)\n",
            "batches_trained: 17 total_loss: tensor(0.0066) acc: tensor(0.9081)\n",
            "batches_trained: 18 total_loss: tensor(0.0065) acc: tensor(0.9097)\n",
            "batches_trained: 19 total_loss: tensor(0.0063) acc: tensor(0.9145)\n",
            "batches_trained: 20 total_loss: tensor(0.0062) acc: tensor(0.9172)\n",
            "batches_trained: 21 total_loss: tensor(0.0061) acc: tensor(0.9196)\n",
            "batches_trained: 22 total_loss: tensor(0.0061) acc: tensor(0.9176)\n",
            "batches_trained: 23 total_loss: tensor(0.0061) acc: tensor(0.9185)\n",
            "batches_trained: 24 total_loss: tensor(0.0061) acc: tensor(0.9180)\n",
            "batches_trained: 25 total_loss: tensor(0.0061) acc: tensor(0.9187)\n",
            "batches_trained: 26 total_loss: tensor(0.0060) acc: tensor(0.9195)\n",
            "batches_trained: 27 total_loss: tensor(0.0060) acc: tensor(0.9190)\n",
            "batches_trained: 28 total_loss: tensor(0.0059) acc: tensor(0.9196)\n",
            "batches_trained: 29 total_loss: tensor(0.0058) acc: tensor(0.9213)\n",
            "batches_trained: 30 total_loss: tensor(0.0060) acc: tensor(0.9198)\n",
            "batches_trained: 31 total_loss: tensor(0.0062) acc: tensor(0.9163)\n",
            "batches_trained: 32 total_loss: tensor(0.0061) acc: tensor(0.9189)\n",
            "batches_trained: 33 total_loss: tensor(0.0064) acc: tensor(0.9157)\n",
            "batches_trained: 34 total_loss: tensor(0.0065) acc: tensor(0.9136)\n",
            "batches_trained: 35 total_loss: tensor(0.0066) acc: tensor(0.9143)\n",
            "batches_trained: 36 total_loss: tensor(0.0067) acc: tensor(0.9115)\n",
            "batches_trained: 37 total_loss: tensor(0.0067) acc: tensor(0.9113)\n",
            "batches_trained: 38 total_loss: tensor(0.0067) acc: tensor(0.9112)\n",
            "batches_trained: 39 total_loss: tensor(0.0066) acc: tensor(0.9135)\n",
            "batches_trained: 40 total_loss: tensor(0.0067) acc: tensor(0.9141)\n",
            "batches_trained: 41 total_loss: tensor(0.0067) acc: tensor(0.9123)\n",
            "batches_trained: 42 total_loss: tensor(0.0067) acc: tensor(0.9115)\n",
            "batches_trained: 43 total_loss: tensor(0.0066) acc: tensor(0.9128)\n",
            "batches_trained: 44 total_loss: tensor(0.0066) acc: tensor(0.9119)\n",
            "batches_trained: 45 total_loss: tensor(0.0066) acc: tensor(0.9118)\n",
            "batches_trained: 46 total_loss: tensor(0.0066) acc: tensor(0.9130)\n",
            "batches_trained: 47 total_loss: tensor(0.0065) acc: tensor(0.9136)\n",
            "batches_trained: 48 total_loss: tensor(0.0065) acc: tensor(0.9134)\n",
            "batches_trained: 49 total_loss: tensor(0.0065) acc: tensor(0.9139)\n",
            "batches_trained: 50 total_loss: tensor(0.0065) acc: tensor(0.9144)\n",
            "batches_trained: 51 total_loss: tensor(0.0064) acc: tensor(0.9154)\n",
            "batches_trained: 52 total_loss: tensor(0.0064) acc: tensor(0.9159)\n",
            "batches_trained: 53 total_loss: tensor(0.0063) acc: tensor(0.9175)\n",
            "batches_trained: 54 total_loss: tensor(0.0063) acc: tensor(0.9184)\n",
            "batches_trained: 55 total_loss: tensor(0.0063) acc: tensor(0.9170)\n",
            "batches_trained: 56 total_loss: tensor(0.0063) acc: tensor(0.9169)\n",
            "batches_trained: 57 total_loss: tensor(0.0063) acc: tensor(0.9178)\n",
            "batches_trained: 58 total_loss: tensor(0.0062) acc: tensor(0.9192)\n",
            "batches_trained: 59 total_loss: tensor(0.0062) acc: tensor(0.9190)\n",
            "batches_trained: 60 total_loss: tensor(0.0062) acc: tensor(0.9193)\n",
            "batches_trained: 61 total_loss: tensor(0.0062) acc: tensor(0.9185)\n",
            "batches_trained: 62 total_loss: tensor(0.0062) acc: tensor(0.9189)\n",
            "batches_trained: 63 total_loss: tensor(0.0062) acc: tensor(0.9187)\n",
            "batches_trained: 64 total_loss: tensor(0.0063) acc: tensor(0.9185)\n",
            "batches_trained: 65 total_loss: tensor(0.0062) acc: tensor(0.9197)\n",
            "batches_trained: 66 total_loss: tensor(0.0061) acc: tensor(0.9205)\n",
            "batches_trained: 67 total_loss: tensor(0.0061) acc: tensor(0.9212)\n",
            "batches_trained: 68 total_loss: tensor(0.0062) acc: tensor(0.9210)\n",
            "batches_trained: 69 total_loss: tensor(0.0062) acc: tensor(0.9216)\n",
            "batches_trained: 70 total_loss: tensor(0.0062) acc: tensor(0.9214)\n",
            "batches_trained: 71 total_loss: tensor(0.0062) acc: tensor(0.9212)\n",
            "batches_trained: 72 total_loss: tensor(0.0062) acc: tensor(0.9206)\n",
            "batches_trained: 73 total_loss: tensor(0.0062) acc: tensor(0.9212)\n",
            "batches_trained: 74 total_loss: tensor(0.0062) acc: tensor(0.9210)\n",
            "batches_trained: 75 total_loss: tensor(0.0062) acc: tensor(0.9204)\n",
            "batches_trained: 76 total_loss: tensor(0.0062) acc: tensor(0.9211)\n",
            "batches_trained: 77 total_loss: tensor(0.0062) acc: tensor(0.9209)\n",
            "batches_trained: 78 total_loss: tensor(0.0062) acc: tensor(0.9211)\n",
            "batches_trained: 79 total_loss: tensor(0.0062) acc: tensor(0.9217)\n",
            "batches_trained: 80 total_loss: tensor(0.0062) acc: tensor(0.9219)\n",
            "batches_trained: 81 total_loss: tensor(0.0062) acc: tensor(0.9225)\n",
            "batches_trained: 82 total_loss: tensor(0.0061) acc: tensor(0.9230)\n",
            "batches_trained: 83 total_loss: tensor(0.0061) acc: tensor(0.9232)\n",
            "batches_trained: 84 total_loss: tensor(0.0061) acc: tensor(0.9234)\n",
            "batches_trained: 85 total_loss: tensor(0.0061) acc: tensor(0.9239)\n",
            "batches_trained: 86 total_loss: tensor(0.0061) acc: tensor(0.9237)\n",
            "batches_trained: 87 total_loss: tensor(0.0061) acc: tensor(0.9239)\n",
            "batches_trained: 88 total_loss: tensor(0.0061) acc: tensor(0.9233)\n",
            "batches_trained: 89 total_loss: tensor(0.0061) acc: tensor(0.9235)\n",
            "batches_trained: 90 total_loss: tensor(0.0060) acc: tensor(0.9236)\n",
            "batches_trained: 91 total_loss: tensor(0.0060) acc: tensor(0.9241)\n",
            "batches_trained: 92 total_loss: tensor(0.0060) acc: tensor(0.9239)\n",
            "batches_trained: 93 total_loss: tensor(0.0060) acc: tensor(0.9244)\n",
            "batches_trained: 94 total_loss: tensor(0.0060) acc: tensor(0.9242)\n",
            "batches_trained: 95 total_loss: tensor(0.0060) acc: tensor(0.9247)\n",
            "batches_trained: 96 total_loss: tensor(0.0060) acc: tensor(0.9248)\n",
            "batches_trained: 97 total_loss: tensor(0.0060) acc: tensor(0.9253)\n",
            "batches_trained: 98 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 99 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 100 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 101 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 102 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 103 total_loss: tensor(0.0060) acc: tensor(0.9248)\n",
            "batches_trained: 104 total_loss: tensor(0.0060) acc: tensor(0.9246)\n",
            "batches_trained: 105 total_loss: tensor(0.0060) acc: tensor(0.9244)\n",
            "batches_trained: 106 total_loss: tensor(0.0060) acc: tensor(0.9236)\n",
            "batches_trained: 107 total_loss: tensor(0.0060) acc: tensor(0.9232)\n",
            "batches_trained: 108 total_loss: tensor(0.0060) acc: tensor(0.9225)\n",
            "batches_trained: 109 total_loss: tensor(0.0059) acc: tensor(0.9232)\n",
            "batches_trained: 110 total_loss: tensor(0.0059) acc: tensor(0.9236)\n",
            "batches_trained: 111 total_loss: tensor(0.0059) acc: tensor(0.9237)\n",
            "batches_trained: 112 total_loss: tensor(0.0059) acc: tensor(0.9241)\n",
            "batches_trained: 113 total_loss: tensor(0.0059) acc: tensor(0.9234)\n",
            "batches_trained: 114 total_loss: tensor(0.0059) acc: tensor(0.9235)\n",
            "batches_trained: 115 total_loss: tensor(0.0060) acc: tensor(0.9234)\n",
            "batches_trained: 116 total_loss: tensor(0.0060) acc: tensor(0.9230)\n",
            "batches_trained: 117 total_loss: tensor(0.0060) acc: tensor(0.9228)\n",
            "batches_trained: 118 total_loss: tensor(0.0060) acc: tensor(0.9232)\n",
            "batches_trained: 119 total_loss: tensor(0.0060) acc: tensor(0.9225)\n",
            "batches_trained: 120 total_loss: tensor(0.0060) acc: tensor(0.9227)\n",
            "batches_trained: 121 total_loss: tensor(0.0060) acc: tensor(0.9223)\n",
            "batches_trained: 122 total_loss: tensor(0.0060) acc: tensor(0.9219)\n",
            "batches_trained: 123 total_loss: tensor(0.0060) acc: tensor(0.9220)\n",
            "batches_trained: 124 total_loss: tensor(0.0060) acc: tensor(0.9221)\n",
            "batches_trained: 125 total_loss: tensor(0.0059) acc: tensor(0.9220)\n",
            "batches_trained: 126 total_loss: tensor(0.0059) acc: tensor(0.9221)\n",
            "batches_trained: 127 total_loss: tensor(0.0060) acc: tensor(0.9218)\n",
            "batches_trained: 128 total_loss: tensor(0.0059) acc: tensor(0.9221)\n",
            "batches_trained: 129 total_loss: tensor(0.0059) acc: tensor(0.9225)\n",
            "batches_trained: 130 total_loss: tensor(0.0059) acc: tensor(0.9228)\n",
            "batches_trained: 131 total_loss: tensor(0.0059) acc: tensor(0.9232)\n",
            "batches_trained: 132 total_loss: tensor(0.0059) acc: tensor(0.9233)\n",
            "batches_trained: 133 total_loss: tensor(0.0059) acc: tensor(0.9234)\n",
            "batches_trained: 134 total_loss: tensor(0.0059) acc: tensor(0.9235)\n",
            "batches_trained: 135 total_loss: tensor(0.0059) acc: tensor(0.9236)\n",
            "batches_trained: 136 total_loss: tensor(0.0058) acc: tensor(0.9237)\n",
            "batches_trained: 137 total_loss: tensor(0.0058) acc: tensor(0.9238)\n",
            "batches_trained: 138 total_loss: tensor(0.0058) acc: tensor(0.9237)\n",
            "batches_trained: 139 total_loss: tensor(0.0058) acc: tensor(0.9240)\n",
            "batches_trained: 140 total_loss: tensor(0.0058) acc: tensor(0.9243)\n",
            "batches_trained: 141 total_loss: tensor(0.0058) acc: tensor(0.9246)\n",
            "batches_trained: 142 total_loss: tensor(0.0059) acc: tensor(0.9236)\n",
            "batches_trained: 143 total_loss: tensor(0.0059) acc: tensor(0.9224)\n",
            "batches_trained: 144 total_loss: tensor(0.0059) acc: tensor(0.9223)\n",
            "batches_trained: 145 total_loss: tensor(0.0059) acc: tensor(0.9228)\n",
            "batches_trained: 146 total_loss: tensor(0.0059) acc: tensor(0.9229)\n",
            "batches_trained: 147 total_loss: tensor(0.0059) acc: tensor(0.9233)\n",
            "batches_trained: 148 total_loss: tensor(0.0058) acc: tensor(0.9238)\n",
            "batches_trained: 149 total_loss: tensor(0.0059) acc: tensor(0.9239)\n",
            "batches_trained: 150 total_loss: tensor(0.0059) acc: tensor(0.9240)\n",
            "batches_trained: 151 total_loss: tensor(0.0059) acc: tensor(0.9240)\n",
            "batches_trained: 152 total_loss: tensor(0.0058) acc: tensor(0.9243)\n",
            "batches_trained: 153 total_loss: tensor(0.0058) acc: tensor(0.9246)\n",
            "batches_trained: 154 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 155 total_loss: tensor(0.0058) acc: tensor(0.9250)\n",
            "batches_trained: 156 total_loss: tensor(0.0059) acc: tensor(0.9247)\n",
            "batches_trained: 157 total_loss: tensor(0.0059) acc: tensor(0.9248)\n",
            "batches_trained: 158 total_loss: tensor(0.0059) acc: tensor(0.9248)\n",
            "batches_trained: 159 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 160 total_loss: tensor(0.0059) acc: tensor(0.9246)\n",
            "batches_trained: 161 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 162 total_loss: tensor(0.0059) acc: tensor(0.9250)\n",
            "batches_trained: 163 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 164 total_loss: tensor(0.0059) acc: tensor(0.9253)\n",
            "batches_trained: 165 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 166 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 167 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 168 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 169 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 170 total_loss: tensor(0.0058) acc: tensor(0.9257)\n",
            "batches_trained: 171 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 172 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 173 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 174 total_loss: tensor(0.0058) acc: tensor(0.9264)\n",
            "batches_trained: 175 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 176 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 177 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 178 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 179 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 180 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 181 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 182 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 183 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 184 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 185 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 186 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 187 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 188 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 189 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 190 total_loss: tensor(0.0058) acc: tensor(0.9248)\n",
            "batches_trained: 191 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 192 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 193 total_loss: tensor(0.0058) acc: tensor(0.9250)\n",
            "batches_trained: 194 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 195 total_loss: tensor(0.0058) acc: tensor(0.9248)\n",
            "batches_trained: 196 total_loss: tensor(0.0058) acc: tensor(0.9246)\n",
            "batches_trained: 197 total_loss: tensor(0.0059) acc: tensor(0.9242)\n",
            "batches_trained: 198 total_loss: tensor(0.0059) acc: tensor(0.9241)\n",
            "batches_trained: 199 total_loss: tensor(0.0059) acc: tensor(0.9240)\n",
            "batches_trained: 200 total_loss: tensor(0.0059) acc: tensor(0.9241)\n",
            "batches_trained: 201 total_loss: tensor(0.0059) acc: tensor(0.9243)\n",
            "batches_trained: 202 total_loss: tensor(0.0059) acc: tensor(0.9244)\n",
            "batches_trained: 203 total_loss: tensor(0.0059) acc: tensor(0.9244)\n",
            "batches_trained: 204 total_loss: tensor(0.0059) acc: tensor(0.9246)\n",
            "batches_trained: 205 total_loss: tensor(0.0059) acc: tensor(0.9244)\n",
            "batches_trained: 206 total_loss: tensor(0.0059) acc: tensor(0.9245)\n",
            "batches_trained: 207 total_loss: tensor(0.0059) acc: tensor(0.9239)\n",
            "batches_trained: 208 total_loss: tensor(0.0059) acc: tensor(0.9241)\n",
            "batches_trained: 209 total_loss: tensor(0.0059) acc: tensor(0.9243)\n",
            "batches_trained: 210 total_loss: tensor(0.0059) acc: tensor(0.9246)\n",
            "batches_trained: 211 total_loss: tensor(0.0059) acc: tensor(0.9246)\n",
            "batches_trained: 212 total_loss: tensor(0.0059) acc: tensor(0.9248)\n",
            "batches_trained: 213 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 214 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 215 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 216 total_loss: tensor(0.0059) acc: tensor(0.9251)\n",
            "batches_trained: 217 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 218 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 219 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 220 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 221 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 222 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 223 total_loss: tensor(0.0058) acc: tensor(0.9257)\n",
            "batches_trained: 224 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 225 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 226 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 227 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 228 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 229 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 230 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 231 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 232 total_loss: tensor(0.0058) acc: tensor(0.9250)\n",
            "batches_trained: 233 total_loss: tensor(0.0058) acc: tensor(0.9250)\n",
            "batches_trained: 234 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 235 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 236 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 237 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 238 total_loss: tensor(0.0058) acc: tensor(0.9248)\n",
            "batches_trained: 239 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 240 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 241 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 242 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 243 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 244 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 245 total_loss: tensor(0.0058) acc: tensor(0.9258)\n",
            "batches_trained: 246 total_loss: tensor(0.0057) acc: tensor(0.9259)\n",
            "batches_trained: 247 total_loss: tensor(0.0058) acc: tensor(0.9259)\n",
            "batches_trained: 248 total_loss: tensor(0.0057) acc: tensor(0.9260)\n",
            "batches_trained: 249 total_loss: tensor(0.0057) acc: tensor(0.9260)\n",
            "batches_trained: 250 total_loss: tensor(0.0057) acc: tensor(0.9259)\n",
            "batches_trained: 251 total_loss: tensor(0.0057) acc: tensor(0.9259)\n",
            "batches_trained: 252 total_loss: tensor(0.0057) acc: tensor(0.9258)\n",
            "batches_trained: 253 total_loss: tensor(0.0057) acc: tensor(0.9259)\n",
            "batches_trained: 254 total_loss: tensor(0.0057) acc: tensor(0.9259)\n",
            "batches_trained: 255 total_loss: tensor(0.0057) acc: tensor(0.9259)\n",
            "batches_trained: 256 total_loss: tensor(0.0057) acc: tensor(0.9257)\n",
            "batches_trained: 257 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 258 total_loss: tensor(0.0058) acc: tensor(0.9253)\n",
            "batches_trained: 259 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 260 total_loss: tensor(0.0058) acc: tensor(0.9256)\n",
            "batches_trained: 261 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 262 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 263 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 264 total_loss: tensor(0.0058) acc: tensor(0.9254)\n",
            "batches_trained: 265 total_loss: tensor(0.0058) acc: tensor(0.9251)\n",
            "batches_trained: 266 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 267 total_loss: tensor(0.0058) acc: tensor(0.9247)\n",
            "batches_trained: 268 total_loss: tensor(0.0058) acc: tensor(0.9248)\n",
            "batches_trained: 269 total_loss: tensor(0.0058) acc: tensor(0.9246)\n",
            "batches_trained: 270 total_loss: tensor(0.0058) acc: tensor(0.9245)\n",
            "batches_trained: 271 total_loss: tensor(0.0058) acc: tensor(0.9246)\n",
            "batches_trained: 272 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 273 total_loss: tensor(0.0058) acc: tensor(0.9246)\n",
            "batches_trained: 274 total_loss: tensor(0.0058) acc: tensor(0.9247)\n",
            "batches_trained: 275 total_loss: tensor(0.0058) acc: tensor(0.9249)\n",
            "batches_trained: 276 total_loss: tensor(0.0058) acc: tensor(0.9248)\n",
            "batches_trained: 277 total_loss: tensor(0.0058) acc: tensor(0.9248)\n",
            "batches_trained: 278 total_loss: tensor(0.0059) acc: tensor(0.9247)\n",
            "batches_trained: 279 total_loss: tensor(0.0059) acc: tensor(0.9247)\n",
            "batches_trained: 280 total_loss: tensor(0.0059) acc: tensor(0.9250)\n",
            "batches_trained: 281 total_loss: tensor(0.0058) acc: tensor(0.9250)\n",
            "batches_trained: 282 total_loss: tensor(0.0058) acc: tensor(0.9252)\n",
            "batches_trained: 283 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 284 total_loss: tensor(0.0058) acc: tensor(0.9255)\n",
            "batches_trained: 285 total_loss: tensor(0.0058) acc: tensor(0.9258)\n",
            "batches_trained: 286 total_loss: tensor(0.0058) acc: tensor(0.9257)\n",
            "batches_trained: 287 total_loss: tensor(0.0058) acc: tensor(0.9256)\n",
            "batches_trained: 288 total_loss: tensor(0.0058) acc: tensor(0.9259)\n",
            "batches_trained: 289 total_loss: tensor(0.0058) acc: tensor(0.9259)\n",
            "batches_trained: 290 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 291 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 292 total_loss: tensor(0.0058) acc: tensor(0.9258)\n",
            "batches_trained: 293 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 294 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 295 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 296 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 297 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 298 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 299 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 300 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 301 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 302 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 303 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 304 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 305 total_loss: tensor(0.0058) acc: tensor(0.9259)\n",
            "batches_trained: 306 total_loss: tensor(0.0058) acc: tensor(0.9258)\n",
            "batches_trained: 307 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 308 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 309 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 310 total_loss: tensor(0.0058) acc: tensor(0.9257)\n",
            "batches_trained: 311 total_loss: tensor(0.0058) acc: tensor(0.9258)\n",
            "batches_trained: 312 total_loss: tensor(0.0058) acc: tensor(0.9258)\n",
            "batches_trained: 313 total_loss: tensor(0.0058) acc: tensor(0.9259)\n",
            "batches_trained: 314 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 315 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 316 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 317 total_loss: tensor(0.0058) acc: tensor(0.9266)\n",
            "batches_trained: 318 total_loss: tensor(0.0058) acc: tensor(0.9267)\n",
            "batches_trained: 319 total_loss: tensor(0.0058) acc: tensor(0.9266)\n",
            "batches_trained: 320 total_loss: tensor(0.0058) acc: tensor(0.9269)\n",
            "batches_trained: 321 total_loss: tensor(0.0058) acc: tensor(0.9268)\n",
            "batches_trained: 322 total_loss: tensor(0.0058) acc: tensor(0.9266)\n",
            "batches_trained: 323 total_loss: tensor(0.0058) acc: tensor(0.9266)\n",
            "batches_trained: 324 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 325 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 326 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 327 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 328 total_loss: tensor(0.0059) acc: tensor(0.9263)\n",
            "batches_trained: 329 total_loss: tensor(0.0059) acc: tensor(0.9263)\n",
            "batches_trained: 330 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 331 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 332 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 333 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 334 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 335 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 336 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 337 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 338 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 339 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 340 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 341 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 342 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 343 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 344 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 345 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 346 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 347 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 348 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 349 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 350 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 351 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 352 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 353 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 354 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 355 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 356 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 357 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 358 total_loss: tensor(0.0059) acc: tensor(0.9255)\n",
            "batches_trained: 359 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 360 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 361 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 362 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 363 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 364 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 365 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 366 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 367 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 368 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 369 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 370 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 371 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 372 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 373 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 374 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 375 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 376 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 377 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 378 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 379 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 380 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 381 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 382 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 383 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 384 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 385 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 386 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 387 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 388 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 389 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 390 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 391 total_loss: tensor(0.0058) acc: tensor(0.9262)\n",
            "batches_trained: 392 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 393 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 394 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 395 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 396 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 397 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 398 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 399 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 400 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 401 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 402 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 403 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 404 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 405 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 406 total_loss: tensor(0.0059) acc: tensor(0.9261)\n",
            "batches_trained: 407 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 408 total_loss: tensor(0.0058) acc: tensor(0.9264)\n",
            "batches_trained: 409 total_loss: tensor(0.0058) acc: tensor(0.9265)\n",
            "batches_trained: 410 total_loss: tensor(0.0058) acc: tensor(0.9264)\n",
            "batches_trained: 411 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 412 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 413 total_loss: tensor(0.0059) acc: tensor(0.9262)\n",
            "batches_trained: 414 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 415 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 416 total_loss: tensor(0.0058) acc: tensor(0.9264)\n",
            "batches_trained: 417 total_loss: tensor(0.0058) acc: tensor(0.9264)\n",
            "batches_trained: 418 total_loss: tensor(0.0058) acc: tensor(0.9263)\n",
            "batches_trained: 419 total_loss: tensor(0.0058) acc: tensor(0.9264)\n",
            "batches_trained: 420 total_loss: tensor(0.0058) acc: tensor(0.9261)\n",
            "batches_trained: 421 total_loss: tensor(0.0059) acc: tensor(0.9260)\n",
            "batches_trained: 422 total_loss: tensor(0.0058) acc: tensor(0.9260)\n",
            "batches_trained: 423 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 424 total_loss: tensor(0.0059) acc: tensor(0.9259)\n",
            "batches_trained: 425 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 426 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 427 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 428 total_loss: tensor(0.0059) acc: tensor(0.9255)\n",
            "batches_trained: 429 total_loss: tensor(0.0059) acc: tensor(0.9254)\n",
            "batches_trained: 430 total_loss: tensor(0.0059) acc: tensor(0.9254)\n",
            "batches_trained: 431 total_loss: tensor(0.0059) acc: tensor(0.9254)\n",
            "batches_trained: 432 total_loss: tensor(0.0059) acc: tensor(0.9254)\n",
            "batches_trained: 433 total_loss: tensor(0.0059) acc: tensor(0.9253)\n",
            "batches_trained: 434 total_loss: tensor(0.0059) acc: tensor(0.9251)\n",
            "batches_trained: 435 total_loss: tensor(0.0059) acc: tensor(0.9252)\n",
            "batches_trained: 436 total_loss: tensor(0.0059) acc: tensor(0.9252)\n",
            "batches_trained: 437 total_loss: tensor(0.0059) acc: tensor(0.9252)\n",
            "batches_trained: 438 total_loss: tensor(0.0059) acc: tensor(0.9252)\n",
            "batches_trained: 439 total_loss: tensor(0.0059) acc: tensor(0.9253)\n",
            "batches_trained: 440 total_loss: tensor(0.0059) acc: tensor(0.9253)\n",
            "batches_trained: 441 total_loss: tensor(0.0059) acc: tensor(0.9254)\n",
            "batches_trained: 442 total_loss: tensor(0.0059) acc: tensor(0.9255)\n",
            "batches_trained: 443 total_loss: tensor(0.0059) acc: tensor(0.9255)\n",
            "batches_trained: 444 total_loss: tensor(0.0059) acc: tensor(0.9255)\n",
            "batches_trained: 445 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 446 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 447 total_loss: tensor(0.0059) acc: tensor(0.9256)\n",
            "batches_trained: 448 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 449 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 450 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 451 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 452 total_loss: tensor(0.0059) acc: tensor(0.9257)\n",
            "batches_trained: 453 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 454 total_loss: tensor(0.0059) acc: tensor(0.9258)\n",
            "batches_trained: 455 total_loss: tensor(0.0059) acc: tensor(0.9257)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62A19avpl4K9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "fdb9db1859e04f2c916a6a73c9e44165",
            "a22a54fc6b904e008fc24a7e45c466db",
            "18910e15249140a9a6bd72bf0888c1d0",
            "350345db655b498a8f52657a549d2f7d",
            "9b5ca6944006431b8a1992fecbbb22b8",
            "41bb0d76461a4f2ba542fc8f2bc893f5",
            "1bb51229d50c491987e04082774ae517",
            "8b80abb9569044a6a5e29cb1b5a83fdb",
            "cb5b720549c44dcc8600c5610453698e",
            "7281155f2c4e44e2bef60364496ab4e2",
            "0407279a52e44427855b206f52e329fc",
            "144862e917ab4bf0ad038687fb3d1206",
            "cdeb0111397f4051842aafb3217ec60c",
            "cb274800880c4d479d0af97112cb6aa1",
            "bbbcc3493137464586723e4d893b268e",
            "52358a81fe31417990df31ae5fc22d73",
            "005a6935ba94461fb3573445207f12c4",
            "1a1360c60ed6417ea071d38221d8e2d1",
            "6254e90052a14ec5981998d04589cf65",
            "38b0375a328942e2b8ce56f711de2c02",
            "c8c1a732a2e14320a47a2531760f4a51",
            "38204927c1864f08a9dbf747107b4edb",
            "5cf7390048bb43cd8b726cdf61fa18fd",
            "3dc88618c1ce4442848411530d0ffdf0",
            "0bfe82ade4fe4553b054d02021ca1e70",
            "213529fe17c2432c8b65bf669cd09b8b",
            "7e74ec74405b41e1a8ef0693a8a96bc3",
            "670d5674e56e4296baa8814894088d28",
            "2fdcfbbd8897420f929d0ce1c12b3c78",
            "7181aa0425b948af8ee3ed909885e4d9",
            "db4377b557804da1b9f821cc566736a5",
            "00d3bfcbc1e448659758d0e45bc86cc5"
          ]
        },
        "outputId": "fe316a12-fc22-4e5e-a425-be3015ade37e"
      },
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "dataset=[]\n",
        "for i in range(len(X)):\n",
        "  encoding = tokenizer.encode_plus(\n",
        "  X[i],\n",
        "  truncation=True,\n",
        "  max_length=64,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "  dataset.append({\n",
        "        'input_ids': encoding['input_ids'].flatten(),\n",
        "        'attention_mask': encoding['attention_mask'].flatten(),\n",
        "        'targets': torch.tensor(Y[i], dtype=torch.long)})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdb9db1859e04f2c916a6a73c9e44165",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb5b720549c44dcc8600c5610453698e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "005a6935ba94461fb3573445207f12c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bfe82ade4fe4553b054d02021ca1e70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ_LqZTGmC7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34203a0f-9b34-46b8-dcec-2c16a7fcb2ac"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test= train_test_split(dataset, shuffle=True, random_state=60, test_size=0.20)\n",
        "print(\"Number of train samples: \", len(X_train))\n",
        "print(\"Number of test samples: \", len(X_test))\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(X_train, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(X_test, batch_size=32, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train samples:  40000\n",
            "Number of test samples:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bD9-TIqwmHME"
      },
      "source": [
        "class robertaClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(robertaClassifier, self).__init__()\n",
        "    self.roberta = AutoModel.from_pretrained(\"roberta-base\")\n",
        "    self.drop = nn.Dropout(0.3)\n",
        "    self.x= nn.Linear(self.roberta.config.hidden_size, 1)\n",
        "    self.out=nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.roberta(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,return_dict=False)\n",
        "    out1= self.drop(pooled_output)\n",
        "    out2=self.x(out1)\n",
        "    return self.out(out2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCBYW8cEmLNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1274c87-c93d-44c1-8681-bc9535b0aa08"
      },
      "source": [
        "model = robertaClassifier()\n",
        "n_epocs=3\n",
        "n_samples=40000\n",
        "batch_size=32\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWuY05a8m5PK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64d96dce-8263-4e9c-9519-b10993dd3756"
      },
      "source": [
        "for n in range(1,n_epocs):\n",
        "  correct=0\n",
        "  train_loss=0\n",
        "  batch=0\n",
        "  for d in train_dataloader:\n",
        "    batch+=1\n",
        "    optimizer.zero_grad()\n",
        "    input_ids = d[\"input_ids\"]\n",
        "    attention_mask = d[\"attention_mask\"]\n",
        "    target = d[\"targets\"]\n",
        "    target=target.to(torch.float32)\n",
        "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = criterion(output[:,0],target)\n",
        "    with torch.no_grad():\n",
        "      train_loss+=loss\n",
        "      output = (output>0.5).float()\n",
        "      correct += ((output[:,0] == target).float()).sum()\n",
        "      print(\"batches_trained: \" + str(batch)+\" total_loss: \"+str(train_loss/(batch*batch_size))+\" acc: \"+str(correct/(batch*batch_size)))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(\"Epoch completed: \" + str(n)+\" total_loss: \"+str(train_loss)+\" acc: \"+str(correct/n_samples))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batches_trained: 1 total_loss: tensor(0.0212) acc: tensor(0.7188)\n",
            "batches_trained: 2 total_loss: tensor(0.0215) acc: tensor(0.5625)\n",
            "batches_trained: 3 total_loss: tensor(0.0215) acc: tensor(0.5521)\n",
            "batches_trained: 4 total_loss: tensor(0.0215) acc: tensor(0.5625)\n",
            "batches_trained: 5 total_loss: tensor(0.0216) acc: tensor(0.5375)\n",
            "batches_trained: 6 total_loss: tensor(0.0215) acc: tensor(0.5469)\n",
            "batches_trained: 7 total_loss: tensor(0.0215) acc: tensor(0.5402)\n",
            "batches_trained: 8 total_loss: tensor(0.0215) acc: tensor(0.5391)\n",
            "batches_trained: 9 total_loss: tensor(0.0216) acc: tensor(0.5174)\n",
            "batches_trained: 10 total_loss: tensor(0.0216) acc: tensor(0.5094)\n",
            "batches_trained: 11 total_loss: tensor(0.0217) acc: tensor(0.5085)\n",
            "batches_trained: 12 total_loss: tensor(0.0216) acc: tensor(0.5052)\n",
            "batches_trained: 13 total_loss: tensor(0.0217) acc: tensor(0.5000)\n",
            "batches_trained: 14 total_loss: tensor(0.0217) acc: tensor(0.5000)\n",
            "batches_trained: 15 total_loss: tensor(0.0216) acc: tensor(0.5042)\n",
            "batches_trained: 16 total_loss: tensor(0.0216) acc: tensor(0.5000)\n",
            "batches_trained: 17 total_loss: tensor(0.0216) acc: tensor(0.5074)\n",
            "batches_trained: 18 total_loss: tensor(0.0216) acc: tensor(0.5174)\n",
            "batches_trained: 19 total_loss: tensor(0.0216) acc: tensor(0.5214)\n",
            "batches_trained: 20 total_loss: tensor(0.0215) acc: tensor(0.5344)\n",
            "batches_trained: 21 total_loss: tensor(0.0215) acc: tensor(0.5432)\n",
            "batches_trained: 22 total_loss: tensor(0.0214) acc: tensor(0.5526)\n",
            "batches_trained: 23 total_loss: tensor(0.0213) acc: tensor(0.5611)\n",
            "batches_trained: 24 total_loss: tensor(0.0212) acc: tensor(0.5664)\n",
            "batches_trained: 25 total_loss: tensor(0.0211) acc: tensor(0.5738)\n",
            "batches_trained: 26 total_loss: tensor(0.0209) acc: tensor(0.5829)\n",
            "batches_trained: 27 total_loss: tensor(0.0208) acc: tensor(0.5891)\n",
            "batches_trained: 28 total_loss: tensor(0.0206) acc: tensor(0.5960)\n",
            "batches_trained: 29 total_loss: tensor(0.0203) acc: tensor(0.6045)\n",
            "batches_trained: 30 total_loss: tensor(0.0202) acc: tensor(0.6073)\n",
            "batches_trained: 31 total_loss: tensor(0.0200) acc: tensor(0.6139)\n",
            "batches_trained: 32 total_loss: tensor(0.0199) acc: tensor(0.6191)\n",
            "batches_trained: 33 total_loss: tensor(0.0197) acc: tensor(0.6250)\n",
            "batches_trained: 34 total_loss: tensor(0.0196) acc: tensor(0.6287)\n",
            "batches_trained: 35 total_loss: tensor(0.0193) acc: tensor(0.6375)\n",
            "batches_trained: 36 total_loss: tensor(0.0191) acc: tensor(0.6415)\n",
            "batches_trained: 37 total_loss: tensor(0.0189) acc: tensor(0.6478)\n",
            "batches_trained: 38 total_loss: tensor(0.0189) acc: tensor(0.6497)\n",
            "batches_trained: 39 total_loss: tensor(0.0189) acc: tensor(0.6522)\n",
            "batches_trained: 40 total_loss: tensor(0.0189) acc: tensor(0.6539)\n",
            "batches_trained: 41 total_loss: tensor(0.0189) acc: tensor(0.6555)\n",
            "batches_trained: 42 total_loss: tensor(0.0188) acc: tensor(0.6562)\n",
            "batches_trained: 43 total_loss: tensor(0.0187) acc: tensor(0.6592)\n",
            "batches_trained: 44 total_loss: tensor(0.0186) acc: tensor(0.6598)\n",
            "batches_trained: 45 total_loss: tensor(0.0184) acc: tensor(0.6660)\n",
            "batches_trained: 46 total_loss: tensor(0.0184) acc: tensor(0.6698)\n",
            "batches_trained: 47 total_loss: tensor(0.0183) acc: tensor(0.6709)\n",
            "batches_trained: 48 total_loss: tensor(0.0182) acc: tensor(0.6751)\n",
            "batches_trained: 49 total_loss: tensor(0.0182) acc: tensor(0.6760)\n",
            "batches_trained: 50 total_loss: tensor(0.0181) acc: tensor(0.6781)\n",
            "batches_trained: 51 total_loss: tensor(0.0179) acc: tensor(0.6832)\n",
            "batches_trained: 52 total_loss: tensor(0.0178) acc: tensor(0.6857)\n",
            "batches_trained: 53 total_loss: tensor(0.0177) acc: tensor(0.6881)\n",
            "batches_trained: 54 total_loss: tensor(0.0176) acc: tensor(0.6916)\n",
            "batches_trained: 55 total_loss: tensor(0.0176) acc: tensor(0.6920)\n",
            "batches_trained: 56 total_loss: tensor(0.0176) acc: tensor(0.6936)\n",
            "batches_trained: 57 total_loss: tensor(0.0176) acc: tensor(0.6941)\n",
            "batches_trained: 58 total_loss: tensor(0.0175) acc: tensor(0.6961)\n",
            "batches_trained: 59 total_loss: tensor(0.0175) acc: tensor(0.6976)\n",
            "batches_trained: 60 total_loss: tensor(0.0174) acc: tensor(0.6974)\n",
            "batches_trained: 61 total_loss: tensor(0.0173) acc: tensor(0.7008)\n",
            "batches_trained: 62 total_loss: tensor(0.0172) acc: tensor(0.7026)\n",
            "batches_trained: 63 total_loss: tensor(0.0172) acc: tensor(0.7034)\n",
            "batches_trained: 64 total_loss: tensor(0.0172) acc: tensor(0.7051)\n",
            "batches_trained: 65 total_loss: tensor(0.0171) acc: tensor(0.7077)\n",
            "batches_trained: 66 total_loss: tensor(0.0170) acc: tensor(0.7098)\n",
            "batches_trained: 67 total_loss: tensor(0.0170) acc: tensor(0.7099)\n",
            "batches_trained: 68 total_loss: tensor(0.0169) acc: tensor(0.7109)\n",
            "batches_trained: 69 total_loss: tensor(0.0169) acc: tensor(0.7115)\n",
            "batches_trained: 70 total_loss: tensor(0.0169) acc: tensor(0.7107)\n",
            "batches_trained: 71 total_loss: tensor(0.0168) acc: tensor(0.7135)\n",
            "batches_trained: 72 total_loss: tensor(0.0167) acc: tensor(0.7148)\n",
            "batches_trained: 73 total_loss: tensor(0.0166) acc: tensor(0.7175)\n",
            "batches_trained: 74 total_loss: tensor(0.0165) acc: tensor(0.7183)\n",
            "batches_trained: 75 total_loss: tensor(0.0165) acc: tensor(0.7183)\n",
            "batches_trained: 76 total_loss: tensor(0.0165) acc: tensor(0.7200)\n",
            "batches_trained: 77 total_loss: tensor(0.0165) acc: tensor(0.7196)\n",
            "batches_trained: 78 total_loss: tensor(0.0165) acc: tensor(0.7220)\n",
            "batches_trained: 79 total_loss: tensor(0.0164) acc: tensor(0.7235)\n",
            "batches_trained: 80 total_loss: tensor(0.0164) acc: tensor(0.7250)\n",
            "batches_trained: 81 total_loss: tensor(0.0164) acc: tensor(0.7249)\n",
            "batches_trained: 82 total_loss: tensor(0.0164) acc: tensor(0.7252)\n",
            "batches_trained: 83 total_loss: tensor(0.0163) acc: tensor(0.7263)\n",
            "batches_trained: 84 total_loss: tensor(0.0163) acc: tensor(0.7269)\n",
            "batches_trained: 85 total_loss: tensor(0.0162) acc: tensor(0.7290)\n",
            "batches_trained: 86 total_loss: tensor(0.0162) acc: tensor(0.7304)\n",
            "batches_trained: 87 total_loss: tensor(0.0161) acc: tensor(0.7317)\n",
            "batches_trained: 88 total_loss: tensor(0.0160) acc: tensor(0.7330)\n",
            "batches_trained: 89 total_loss: tensor(0.0160) acc: tensor(0.7331)\n",
            "batches_trained: 90 total_loss: tensor(0.0159) acc: tensor(0.7354)\n",
            "batches_trained: 91 total_loss: tensor(0.0159) acc: tensor(0.7356)\n",
            "batches_trained: 92 total_loss: tensor(0.0158) acc: tensor(0.7371)\n",
            "batches_trained: 93 total_loss: tensor(0.0157) acc: tensor(0.7392)\n",
            "batches_trained: 94 total_loss: tensor(0.0157) acc: tensor(0.7397)\n",
            "batches_trained: 95 total_loss: tensor(0.0157) acc: tensor(0.7401)\n",
            "batches_trained: 96 total_loss: tensor(0.0156) acc: tensor(0.7415)\n",
            "batches_trained: 97 total_loss: tensor(0.0156) acc: tensor(0.7423)\n",
            "batches_trained: 98 total_loss: tensor(0.0155) acc: tensor(0.7433)\n",
            "batches_trained: 99 total_loss: tensor(0.0155) acc: tensor(0.7440)\n",
            "batches_trained: 100 total_loss: tensor(0.0155) acc: tensor(0.7450)\n",
            "batches_trained: 101 total_loss: tensor(0.0155) acc: tensor(0.7441)\n",
            "batches_trained: 102 total_loss: tensor(0.0155) acc: tensor(0.7448)\n",
            "batches_trained: 103 total_loss: tensor(0.0155) acc: tensor(0.7442)\n",
            "batches_trained: 104 total_loss: tensor(0.0155) acc: tensor(0.7443)\n",
            "batches_trained: 105 total_loss: tensor(0.0154) acc: tensor(0.7449)\n",
            "batches_trained: 106 total_loss: tensor(0.0154) acc: tensor(0.7456)\n",
            "batches_trained: 107 total_loss: tensor(0.0153) acc: tensor(0.7471)\n",
            "batches_trained: 108 total_loss: tensor(0.0152) acc: tensor(0.7488)\n",
            "batches_trained: 109 total_loss: tensor(0.0152) acc: tensor(0.7494)\n",
            "batches_trained: 110 total_loss: tensor(0.0152) acc: tensor(0.7494)\n",
            "batches_trained: 111 total_loss: tensor(0.0152) acc: tensor(0.7500)\n",
            "batches_trained: 112 total_loss: tensor(0.0152) acc: tensor(0.7506)\n",
            "batches_trained: 113 total_loss: tensor(0.0151) acc: tensor(0.7519)\n",
            "batches_trained: 114 total_loss: tensor(0.0151) acc: tensor(0.7525)\n",
            "batches_trained: 115 total_loss: tensor(0.0151) acc: tensor(0.7527)\n",
            "batches_trained: 116 total_loss: tensor(0.0151) acc: tensor(0.7527)\n",
            "batches_trained: 117 total_loss: tensor(0.0150) acc: tensor(0.7537)\n",
            "batches_trained: 118 total_loss: tensor(0.0149) acc: tensor(0.7548)\n",
            "batches_trained: 119 total_loss: tensor(0.0149) acc: tensor(0.7553)\n",
            "batches_trained: 120 total_loss: tensor(0.0149) acc: tensor(0.7549)\n",
            "batches_trained: 121 total_loss: tensor(0.0149) acc: tensor(0.7554)\n",
            "batches_trained: 122 total_loss: tensor(0.0149) acc: tensor(0.7564)\n",
            "batches_trained: 123 total_loss: tensor(0.0148) acc: tensor(0.7574)\n",
            "batches_trained: 124 total_loss: tensor(0.0148) acc: tensor(0.7576)\n",
            "batches_trained: 125 total_loss: tensor(0.0148) acc: tensor(0.7582)\n",
            "batches_trained: 126 total_loss: tensor(0.0148) acc: tensor(0.7584)\n",
            "batches_trained: 127 total_loss: tensor(0.0148) acc: tensor(0.7584)\n",
            "batches_trained: 128 total_loss: tensor(0.0147) acc: tensor(0.7583)\n",
            "batches_trained: 129 total_loss: tensor(0.0147) acc: tensor(0.7580)\n",
            "batches_trained: 130 total_loss: tensor(0.0147) acc: tensor(0.7589)\n",
            "batches_trained: 131 total_loss: tensor(0.0147) acc: tensor(0.7595)\n",
            "batches_trained: 132 total_loss: tensor(0.0147) acc: tensor(0.7595)\n",
            "batches_trained: 133 total_loss: tensor(0.0147) acc: tensor(0.7596)\n",
            "batches_trained: 134 total_loss: tensor(0.0147) acc: tensor(0.7598)\n",
            "batches_trained: 135 total_loss: tensor(0.0147) acc: tensor(0.7602)\n",
            "batches_trained: 136 total_loss: tensor(0.0147) acc: tensor(0.7603)\n",
            "batches_trained: 137 total_loss: tensor(0.0146) acc: tensor(0.7603)\n",
            "batches_trained: 138 total_loss: tensor(0.0146) acc: tensor(0.7606)\n",
            "batches_trained: 139 total_loss: tensor(0.0146) acc: tensor(0.7612)\n",
            "batches_trained: 140 total_loss: tensor(0.0146) acc: tensor(0.7612)\n",
            "batches_trained: 141 total_loss: tensor(0.0146) acc: tensor(0.7613)\n",
            "batches_trained: 142 total_loss: tensor(0.0146) acc: tensor(0.7623)\n",
            "batches_trained: 143 total_loss: tensor(0.0145) acc: tensor(0.7629)\n",
            "batches_trained: 144 total_loss: tensor(0.0145) acc: tensor(0.7635)\n",
            "batches_trained: 145 total_loss: tensor(0.0145) acc: tensor(0.7638)\n",
            "batches_trained: 146 total_loss: tensor(0.0145) acc: tensor(0.7641)\n",
            "batches_trained: 147 total_loss: tensor(0.0144) acc: tensor(0.7653)\n",
            "batches_trained: 148 total_loss: tensor(0.0144) acc: tensor(0.7660)\n",
            "batches_trained: 149 total_loss: tensor(0.0143) acc: tensor(0.7670)\n",
            "batches_trained: 150 total_loss: tensor(0.0143) acc: tensor(0.7673)\n",
            "batches_trained: 151 total_loss: tensor(0.0143) acc: tensor(0.7682)\n",
            "batches_trained: 152 total_loss: tensor(0.0142) acc: tensor(0.7691)\n",
            "batches_trained: 153 total_loss: tensor(0.0142) acc: tensor(0.7696)\n",
            "batches_trained: 154 total_loss: tensor(0.0142) acc: tensor(0.7701)\n",
            "batches_trained: 155 total_loss: tensor(0.0142) acc: tensor(0.7708)\n",
            "batches_trained: 156 total_loss: tensor(0.0142) acc: tensor(0.7710)\n",
            "batches_trained: 157 total_loss: tensor(0.0141) acc: tensor(0.7715)\n",
            "batches_trained: 158 total_loss: tensor(0.0141) acc: tensor(0.7722)\n",
            "batches_trained: 159 total_loss: tensor(0.0141) acc: tensor(0.7728)\n",
            "batches_trained: 160 total_loss: tensor(0.0141) acc: tensor(0.7729)\n",
            "batches_trained: 161 total_loss: tensor(0.0141) acc: tensor(0.7733)\n",
            "batches_trained: 162 total_loss: tensor(0.0141) acc: tensor(0.7735)\n",
            "batches_trained: 163 total_loss: tensor(0.0140) acc: tensor(0.7742)\n",
            "batches_trained: 164 total_loss: tensor(0.0140) acc: tensor(0.7750)\n",
            "batches_trained: 165 total_loss: tensor(0.0140) acc: tensor(0.7754)\n",
            "batches_trained: 166 total_loss: tensor(0.0140) acc: tensor(0.7762)\n",
            "batches_trained: 167 total_loss: tensor(0.0140) acc: tensor(0.7756)\n",
            "batches_trained: 168 total_loss: tensor(0.0140) acc: tensor(0.7760)\n",
            "batches_trained: 169 total_loss: tensor(0.0140) acc: tensor(0.7759)\n",
            "batches_trained: 170 total_loss: tensor(0.0140) acc: tensor(0.7763)\n",
            "batches_trained: 171 total_loss: tensor(0.0140) acc: tensor(0.7767)\n",
            "batches_trained: 172 total_loss: tensor(0.0140) acc: tensor(0.7769)\n",
            "batches_trained: 173 total_loss: tensor(0.0140) acc: tensor(0.7773)\n",
            "batches_trained: 174 total_loss: tensor(0.0140) acc: tensor(0.7773)\n",
            "batches_trained: 175 total_loss: tensor(0.0140) acc: tensor(0.7773)\n",
            "batches_trained: 176 total_loss: tensor(0.0140) acc: tensor(0.7777)\n",
            "batches_trained: 177 total_loss: tensor(0.0140) acc: tensor(0.7781)\n",
            "batches_trained: 178 total_loss: tensor(0.0140) acc: tensor(0.7784)\n",
            "batches_trained: 179 total_loss: tensor(0.0139) acc: tensor(0.7785)\n",
            "batches_trained: 180 total_loss: tensor(0.0139) acc: tensor(0.7792)\n",
            "batches_trained: 181 total_loss: tensor(0.0139) acc: tensor(0.7800)\n",
            "batches_trained: 182 total_loss: tensor(0.0138) acc: tensor(0.7809)\n",
            "batches_trained: 183 total_loss: tensor(0.0138) acc: tensor(0.7811)\n",
            "batches_trained: 184 total_loss: tensor(0.0138) acc: tensor(0.7814)\n",
            "batches_trained: 185 total_loss: tensor(0.0138) acc: tensor(0.7824)\n",
            "batches_trained: 186 total_loss: tensor(0.0138) acc: tensor(0.7828)\n",
            "batches_trained: 187 total_loss: tensor(0.0137) acc: tensor(0.7833)\n",
            "batches_trained: 188 total_loss: tensor(0.0137) acc: tensor(0.7836)\n",
            "batches_trained: 189 total_loss: tensor(0.0138) acc: tensor(0.7839)\n",
            "batches_trained: 190 total_loss: tensor(0.0137) acc: tensor(0.7842)\n",
            "batches_trained: 191 total_loss: tensor(0.0137) acc: tensor(0.7845)\n",
            "batches_trained: 192 total_loss: tensor(0.0137) acc: tensor(0.7847)\n",
            "batches_trained: 193 total_loss: tensor(0.0137) acc: tensor(0.7847)\n",
            "batches_trained: 194 total_loss: tensor(0.0137) acc: tensor(0.7846)\n",
            "batches_trained: 195 total_loss: tensor(0.0137) acc: tensor(0.7849)\n",
            "batches_trained: 196 total_loss: tensor(0.0137) acc: tensor(0.7854)\n",
            "batches_trained: 197 total_loss: tensor(0.0137) acc: tensor(0.7854)\n",
            "batches_trained: 198 total_loss: tensor(0.0137) acc: tensor(0.7847)\n",
            "batches_trained: 199 total_loss: tensor(0.0137) acc: tensor(0.7847)\n",
            "batches_trained: 200 total_loss: tensor(0.0137) acc: tensor(0.7847)\n",
            "batches_trained: 201 total_loss: tensor(0.0137) acc: tensor(0.7853)\n",
            "batches_trained: 202 total_loss: tensor(0.0137) acc: tensor(0.7857)\n",
            "batches_trained: 203 total_loss: tensor(0.0137) acc: tensor(0.7859)\n",
            "batches_trained: 204 total_loss: tensor(0.0137) acc: tensor(0.7866)\n",
            "batches_trained: 205 total_loss: tensor(0.0136) acc: tensor(0.7869)\n",
            "batches_trained: 206 total_loss: tensor(0.0136) acc: tensor(0.7869)\n",
            "batches_trained: 207 total_loss: tensor(0.0136) acc: tensor(0.7870)\n",
            "batches_trained: 208 total_loss: tensor(0.0136) acc: tensor(0.7871)\n",
            "batches_trained: 209 total_loss: tensor(0.0136) acc: tensor(0.7875)\n",
            "batches_trained: 210 total_loss: tensor(0.0136) acc: tensor(0.7878)\n",
            "batches_trained: 211 total_loss: tensor(0.0136) acc: tensor(0.7881)\n",
            "batches_trained: 212 total_loss: tensor(0.0135) acc: tensor(0.7880)\n",
            "batches_trained: 213 total_loss: tensor(0.0135) acc: tensor(0.7881)\n",
            "batches_trained: 214 total_loss: tensor(0.0135) acc: tensor(0.7881)\n",
            "batches_trained: 215 total_loss: tensor(0.0135) acc: tensor(0.7882)\n",
            "batches_trained: 216 total_loss: tensor(0.0135) acc: tensor(0.7878)\n",
            "batches_trained: 217 total_loss: tensor(0.0135) acc: tensor(0.7877)\n",
            "batches_trained: 218 total_loss: tensor(0.0135) acc: tensor(0.7880)\n",
            "batches_trained: 219 total_loss: tensor(0.0135) acc: tensor(0.7878)\n",
            "batches_trained: 220 total_loss: tensor(0.0135) acc: tensor(0.7881)\n",
            "batches_trained: 221 total_loss: tensor(0.0135) acc: tensor(0.7879)\n",
            "batches_trained: 222 total_loss: tensor(0.0135) acc: tensor(0.7881)\n",
            "batches_trained: 223 total_loss: tensor(0.0135) acc: tensor(0.7883)\n",
            "batches_trained: 224 total_loss: tensor(0.0135) acc: tensor(0.7881)\n",
            "batches_trained: 225 total_loss: tensor(0.0135) acc: tensor(0.7879)\n",
            "batches_trained: 226 total_loss: tensor(0.0135) acc: tensor(0.7884)\n",
            "batches_trained: 227 total_loss: tensor(0.0135) acc: tensor(0.7883)\n",
            "batches_trained: 228 total_loss: tensor(0.0135) acc: tensor(0.7884)\n",
            "batches_trained: 229 total_loss: tensor(0.0135) acc: tensor(0.7885)\n",
            "batches_trained: 230 total_loss: tensor(0.0135) acc: tensor(0.7889)\n",
            "batches_trained: 231 total_loss: tensor(0.0135) acc: tensor(0.7887)\n",
            "batches_trained: 232 total_loss: tensor(0.0135) acc: tensor(0.7885)\n",
            "batches_trained: 233 total_loss: tensor(0.0135) acc: tensor(0.7886)\n",
            "batches_trained: 234 total_loss: tensor(0.0135) acc: tensor(0.7891)\n",
            "batches_trained: 235 total_loss: tensor(0.0135) acc: tensor(0.7894)\n",
            "batches_trained: 236 total_loss: tensor(0.0135) acc: tensor(0.7892)\n",
            "batches_trained: 237 total_loss: tensor(0.0135) acc: tensor(0.7896)\n",
            "batches_trained: 238 total_loss: tensor(0.0135) acc: tensor(0.7897)\n",
            "batches_trained: 239 total_loss: tensor(0.0134) acc: tensor(0.7897)\n",
            "batches_trained: 240 total_loss: tensor(0.0134) acc: tensor(0.7897)\n",
            "batches_trained: 241 total_loss: tensor(0.0134) acc: tensor(0.7897)\n",
            "batches_trained: 242 total_loss: tensor(0.0134) acc: tensor(0.7900)\n",
            "batches_trained: 243 total_loss: tensor(0.0134) acc: tensor(0.7897)\n",
            "batches_trained: 244 total_loss: tensor(0.0134) acc: tensor(0.7902)\n",
            "batches_trained: 245 total_loss: tensor(0.0134) acc: tensor(0.7904)\n",
            "batches_trained: 246 total_loss: tensor(0.0134) acc: tensor(0.7907)\n",
            "batches_trained: 247 total_loss: tensor(0.0134) acc: tensor(0.7911)\n",
            "batches_trained: 248 total_loss: tensor(0.0134) acc: tensor(0.7916)\n",
            "batches_trained: 249 total_loss: tensor(0.0134) acc: tensor(0.7915)\n",
            "batches_trained: 250 total_loss: tensor(0.0134) acc: tensor(0.7915)\n",
            "batches_trained: 251 total_loss: tensor(0.0133) acc: tensor(0.7922)\n",
            "batches_trained: 252 total_loss: tensor(0.0133) acc: tensor(0.7923)\n",
            "batches_trained: 253 total_loss: tensor(0.0133) acc: tensor(0.7920)\n",
            "batches_trained: 254 total_loss: tensor(0.0133) acc: tensor(0.7922)\n",
            "batches_trained: 255 total_loss: tensor(0.0133) acc: tensor(0.7928)\n",
            "batches_trained: 256 total_loss: tensor(0.0133) acc: tensor(0.7928)\n",
            "batches_trained: 257 total_loss: tensor(0.0133) acc: tensor(0.7929)\n",
            "batches_trained: 258 total_loss: tensor(0.0133) acc: tensor(0.7932)\n",
            "batches_trained: 259 total_loss: tensor(0.0133) acc: tensor(0.7936)\n",
            "batches_trained: 260 total_loss: tensor(0.0133) acc: tensor(0.7940)\n",
            "batches_trained: 261 total_loss: tensor(0.0133) acc: tensor(0.7943)\n",
            "batches_trained: 262 total_loss: tensor(0.0133) acc: tensor(0.7941)\n",
            "batches_trained: 263 total_loss: tensor(0.0133) acc: tensor(0.7942)\n",
            "batches_trained: 264 total_loss: tensor(0.0132) acc: tensor(0.7947)\n",
            "batches_trained: 265 total_loss: tensor(0.0132) acc: tensor(0.7950)\n",
            "batches_trained: 266 total_loss: tensor(0.0132) acc: tensor(0.7950)\n",
            "batches_trained: 267 total_loss: tensor(0.0132) acc: tensor(0.7953)\n",
            "batches_trained: 268 total_loss: tensor(0.0132) acc: tensor(0.7954)\n",
            "batches_trained: 269 total_loss: tensor(0.0132) acc: tensor(0.7954)\n",
            "batches_trained: 270 total_loss: tensor(0.0132) acc: tensor(0.7958)\n",
            "batches_trained: 271 total_loss: tensor(0.0132) acc: tensor(0.7961)\n",
            "batches_trained: 272 total_loss: tensor(0.0132) acc: tensor(0.7961)\n",
            "batches_trained: 273 total_loss: tensor(0.0132) acc: tensor(0.7961)\n",
            "batches_trained: 274 total_loss: tensor(0.0132) acc: tensor(0.7964)\n",
            "batches_trained: 275 total_loss: tensor(0.0131) acc: tensor(0.7965)\n",
            "batches_trained: 276 total_loss: tensor(0.0131) acc: tensor(0.7965)\n",
            "batches_trained: 277 total_loss: tensor(0.0131) acc: tensor(0.7968)\n",
            "batches_trained: 278 total_loss: tensor(0.0131) acc: tensor(0.7970)\n",
            "batches_trained: 279 total_loss: tensor(0.0131) acc: tensor(0.7969)\n",
            "batches_trained: 280 total_loss: tensor(0.0131) acc: tensor(0.7971)\n",
            "batches_trained: 281 total_loss: tensor(0.0131) acc: tensor(0.7974)\n",
            "batches_trained: 282 total_loss: tensor(0.0131) acc: tensor(0.7974)\n",
            "batches_trained: 283 total_loss: tensor(0.0131) acc: tensor(0.7976)\n",
            "batches_trained: 284 total_loss: tensor(0.0131) acc: tensor(0.7979)\n",
            "batches_trained: 285 total_loss: tensor(0.0131) acc: tensor(0.7978)\n",
            "batches_trained: 286 total_loss: tensor(0.0131) acc: tensor(0.7977)\n",
            "batches_trained: 287 total_loss: tensor(0.0131) acc: tensor(0.7979)\n",
            "batches_trained: 288 total_loss: tensor(0.0131) acc: tensor(0.7981)\n",
            "batches_trained: 289 total_loss: tensor(0.0131) acc: tensor(0.7979)\n",
            "batches_trained: 290 total_loss: tensor(0.0131) acc: tensor(0.7981)\n",
            "batches_trained: 291 total_loss: tensor(0.0131) acc: tensor(0.7982)\n",
            "batches_trained: 292 total_loss: tensor(0.0131) acc: tensor(0.7984)\n",
            "batches_trained: 293 total_loss: tensor(0.0130) acc: tensor(0.7985)\n",
            "batches_trained: 294 total_loss: tensor(0.0130) acc: tensor(0.7988)\n",
            "batches_trained: 295 total_loss: tensor(0.0130) acc: tensor(0.7985)\n",
            "batches_trained: 296 total_loss: tensor(0.0130) acc: tensor(0.7986)\n",
            "batches_trained: 297 total_loss: tensor(0.0130) acc: tensor(0.7985)\n",
            "batches_trained: 298 total_loss: tensor(0.0130) acc: tensor(0.7984)\n",
            "batches_trained: 299 total_loss: tensor(0.0130) acc: tensor(0.7985)\n",
            "batches_trained: 300 total_loss: tensor(0.0130) acc: tensor(0.7987)\n",
            "batches_trained: 301 total_loss: tensor(0.0130) acc: tensor(0.7989)\n",
            "batches_trained: 302 total_loss: tensor(0.0130) acc: tensor(0.7988)\n",
            "batches_trained: 303 total_loss: tensor(0.0130) acc: tensor(0.7989)\n",
            "batches_trained: 304 total_loss: tensor(0.0130) acc: tensor(0.7989)\n",
            "batches_trained: 305 total_loss: tensor(0.0130) acc: tensor(0.7992)\n",
            "batches_trained: 306 total_loss: tensor(0.0130) acc: tensor(0.7993)\n",
            "batches_trained: 307 total_loss: tensor(0.0130) acc: tensor(0.7994)\n",
            "batches_trained: 308 total_loss: tensor(0.0130) acc: tensor(0.7994)\n",
            "batches_trained: 309 total_loss: tensor(0.0130) acc: tensor(0.7995)\n",
            "batches_trained: 310 total_loss: tensor(0.0130) acc: tensor(0.7997)\n",
            "batches_trained: 311 total_loss: tensor(0.0130) acc: tensor(0.7995)\n",
            "batches_trained: 312 total_loss: tensor(0.0130) acc: tensor(0.7995)\n",
            "batches_trained: 313 total_loss: tensor(0.0130) acc: tensor(0.7995)\n",
            "batches_trained: 314 total_loss: tensor(0.0130) acc: tensor(0.7998)\n",
            "batches_trained: 315 total_loss: tensor(0.0129) acc: tensor(0.8000)\n",
            "batches_trained: 316 total_loss: tensor(0.0129) acc: tensor(0.8000)\n",
            "batches_trained: 317 total_loss: tensor(0.0129) acc: tensor(0.7999)\n",
            "batches_trained: 318 total_loss: tensor(0.0129) acc: tensor(0.7999)\n",
            "batches_trained: 319 total_loss: tensor(0.0129) acc: tensor(0.8002)\n",
            "batches_trained: 320 total_loss: tensor(0.0129) acc: tensor(0.8006)\n",
            "batches_trained: 321 total_loss: tensor(0.0129) acc: tensor(0.8004)\n",
            "batches_trained: 322 total_loss: tensor(0.0129) acc: tensor(0.8004)\n",
            "batches_trained: 323 total_loss: tensor(0.0129) acc: tensor(0.8005)\n",
            "batches_trained: 324 total_loss: tensor(0.0129) acc: tensor(0.8008)\n",
            "batches_trained: 325 total_loss: tensor(0.0129) acc: tensor(0.8009)\n",
            "batches_trained: 326 total_loss: tensor(0.0129) acc: tensor(0.8009)\n",
            "batches_trained: 327 total_loss: tensor(0.0129) acc: tensor(0.8008)\n",
            "batches_trained: 328 total_loss: tensor(0.0129) acc: tensor(0.8009)\n",
            "batches_trained: 329 total_loss: tensor(0.0129) acc: tensor(0.8011)\n",
            "batches_trained: 330 total_loss: tensor(0.0129) acc: tensor(0.8013)\n",
            "batches_trained: 331 total_loss: tensor(0.0129) acc: tensor(0.8013)\n",
            "batches_trained: 332 total_loss: tensor(0.0128) acc: tensor(0.8015)\n",
            "batches_trained: 333 total_loss: tensor(0.0128) acc: tensor(0.8019)\n",
            "batches_trained: 334 total_loss: tensor(0.0128) acc: tensor(0.8020)\n",
            "batches_trained: 335 total_loss: tensor(0.0128) acc: tensor(0.8022)\n",
            "batches_trained: 336 total_loss: tensor(0.0128) acc: tensor(0.8025)\n",
            "batches_trained: 337 total_loss: tensor(0.0128) acc: tensor(0.8024)\n",
            "batches_trained: 338 total_loss: tensor(0.0128) acc: tensor(0.8025)\n",
            "batches_trained: 339 total_loss: tensor(0.0128) acc: tensor(0.8028)\n",
            "batches_trained: 340 total_loss: tensor(0.0128) acc: tensor(0.8028)\n",
            "batches_trained: 341 total_loss: tensor(0.0128) acc: tensor(0.8031)\n",
            "batches_trained: 342 total_loss: tensor(0.0128) acc: tensor(0.8029)\n",
            "batches_trained: 343 total_loss: tensor(0.0128) acc: tensor(0.8028)\n",
            "batches_trained: 344 total_loss: tensor(0.0128) acc: tensor(0.8029)\n",
            "batches_trained: 345 total_loss: tensor(0.0128) acc: tensor(0.8030)\n",
            "batches_trained: 346 total_loss: tensor(0.0128) acc: tensor(0.8034)\n",
            "batches_trained: 347 total_loss: tensor(0.0128) acc: tensor(0.8033)\n",
            "batches_trained: 348 total_loss: tensor(0.0128) acc: tensor(0.8036)\n",
            "batches_trained: 349 total_loss: tensor(0.0128) acc: tensor(0.8035)\n",
            "batches_trained: 350 total_loss: tensor(0.0128) acc: tensor(0.8037)\n",
            "batches_trained: 351 total_loss: tensor(0.0128) acc: tensor(0.8039)\n",
            "batches_trained: 352 total_loss: tensor(0.0128) acc: tensor(0.8037)\n",
            "batches_trained: 353 total_loss: tensor(0.0128) acc: tensor(0.8036)\n",
            "batches_trained: 354 total_loss: tensor(0.0128) acc: tensor(0.8036)\n",
            "batches_trained: 355 total_loss: tensor(0.0128) acc: tensor(0.8036)\n",
            "batches_trained: 356 total_loss: tensor(0.0128) acc: tensor(0.8038)\n",
            "batches_trained: 357 total_loss: tensor(0.0127) acc: tensor(0.8041)\n",
            "batches_trained: 358 total_loss: tensor(0.0127) acc: tensor(0.8042)\n",
            "batches_trained: 359 total_loss: tensor(0.0127) acc: tensor(0.8041)\n",
            "batches_trained: 360 total_loss: tensor(0.0127) acc: tensor(0.8045)\n",
            "batches_trained: 361 total_loss: tensor(0.0127) acc: tensor(0.8045)\n",
            "batches_trained: 362 total_loss: tensor(0.0127) acc: tensor(0.8047)\n",
            "batches_trained: 363 total_loss: tensor(0.0127) acc: tensor(0.8051)\n",
            "batches_trained: 364 total_loss: tensor(0.0127) acc: tensor(0.8051)\n",
            "batches_trained: 365 total_loss: tensor(0.0127) acc: tensor(0.8051)\n",
            "batches_trained: 366 total_loss: tensor(0.0127) acc: tensor(0.8052)\n",
            "batches_trained: 367 total_loss: tensor(0.0127) acc: tensor(0.8050)\n",
            "batches_trained: 368 total_loss: tensor(0.0127) acc: tensor(0.8050)\n",
            "batches_trained: 369 total_loss: tensor(0.0127) acc: tensor(0.8051)\n",
            "batches_trained: 370 total_loss: tensor(0.0127) acc: tensor(0.8053)\n",
            "batches_trained: 371 total_loss: tensor(0.0127) acc: tensor(0.8053)\n",
            "batches_trained: 372 total_loss: tensor(0.0126) acc: tensor(0.8054)\n",
            "batches_trained: 373 total_loss: tensor(0.0126) acc: tensor(0.8056)\n",
            "batches_trained: 374 total_loss: tensor(0.0126) acc: tensor(0.8056)\n",
            "batches_trained: 375 total_loss: tensor(0.0126) acc: tensor(0.8059)\n",
            "batches_trained: 376 total_loss: tensor(0.0126) acc: tensor(0.8058)\n",
            "batches_trained: 377 total_loss: tensor(0.0126) acc: tensor(0.8057)\n",
            "batches_trained: 378 total_loss: tensor(0.0126) acc: tensor(0.8054)\n",
            "batches_trained: 379 total_loss: tensor(0.0126) acc: tensor(0.8055)\n",
            "batches_trained: 380 total_loss: tensor(0.0126) acc: tensor(0.8054)\n",
            "batches_trained: 381 total_loss: tensor(0.0126) acc: tensor(0.8056)\n",
            "batches_trained: 382 total_loss: tensor(0.0126) acc: tensor(0.8056)\n",
            "batches_trained: 383 total_loss: tensor(0.0126) acc: tensor(0.8057)\n",
            "batches_trained: 384 total_loss: tensor(0.0126) acc: tensor(0.8057)\n",
            "batches_trained: 385 total_loss: tensor(0.0126) acc: tensor(0.8058)\n",
            "batches_trained: 386 total_loss: tensor(0.0126) acc: tensor(0.8060)\n",
            "batches_trained: 387 total_loss: tensor(0.0126) acc: tensor(0.8061)\n",
            "batches_trained: 388 total_loss: tensor(0.0126) acc: tensor(0.8061)\n",
            "batches_trained: 389 total_loss: tensor(0.0126) acc: tensor(0.8058)\n",
            "batches_trained: 390 total_loss: tensor(0.0126) acc: tensor(0.8058)\n",
            "batches_trained: 391 total_loss: tensor(0.0126) acc: tensor(0.8059)\n",
            "batches_trained: 392 total_loss: tensor(0.0126) acc: tensor(0.8059)\n",
            "batches_trained: 393 total_loss: tensor(0.0126) acc: tensor(0.8060)\n",
            "batches_trained: 394 total_loss: tensor(0.0126) acc: tensor(0.8061)\n",
            "batches_trained: 395 total_loss: tensor(0.0126) acc: tensor(0.8062)\n",
            "batches_trained: 396 total_loss: tensor(0.0126) acc: tensor(0.8063)\n",
            "batches_trained: 397 total_loss: tensor(0.0126) acc: tensor(0.8065)\n",
            "batches_trained: 398 total_loss: tensor(0.0125) acc: tensor(0.8065)\n",
            "batches_trained: 399 total_loss: tensor(0.0125) acc: tensor(0.8065)\n",
            "batches_trained: 400 total_loss: tensor(0.0125) acc: tensor(0.8066)\n",
            "batches_trained: 401 total_loss: tensor(0.0125) acc: tensor(0.8069)\n",
            "batches_trained: 402 total_loss: tensor(0.0125) acc: tensor(0.8071)\n",
            "batches_trained: 403 total_loss: tensor(0.0125) acc: tensor(0.8073)\n",
            "batches_trained: 404 total_loss: tensor(0.0125) acc: tensor(0.8076)\n",
            "batches_trained: 405 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 406 total_loss: tensor(0.0125) acc: tensor(0.8079)\n",
            "batches_trained: 407 total_loss: tensor(0.0125) acc: tensor(0.8080)\n",
            "batches_trained: 408 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 409 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 410 total_loss: tensor(0.0125) acc: tensor(0.8077)\n",
            "batches_trained: 411 total_loss: tensor(0.0125) acc: tensor(0.8077)\n",
            "batches_trained: 412 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 413 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 414 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 415 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 416 total_loss: tensor(0.0125) acc: tensor(0.8078)\n",
            "batches_trained: 417 total_loss: tensor(0.0125) acc: tensor(0.8080)\n",
            "batches_trained: 418 total_loss: tensor(0.0125) acc: tensor(0.8081)\n",
            "batches_trained: 419 total_loss: tensor(0.0125) acc: tensor(0.8082)\n",
            "batches_trained: 420 total_loss: tensor(0.0125) acc: tensor(0.8082)\n",
            "batches_trained: 421 total_loss: tensor(0.0125) acc: tensor(0.8082)\n",
            "batches_trained: 422 total_loss: tensor(0.0125) acc: tensor(0.8083)\n",
            "batches_trained: 423 total_loss: tensor(0.0125) acc: tensor(0.8083)\n",
            "batches_trained: 424 total_loss: tensor(0.0125) acc: tensor(0.8084)\n",
            "batches_trained: 425 total_loss: tensor(0.0124) acc: tensor(0.8086)\n",
            "batches_trained: 426 total_loss: tensor(0.0124) acc: tensor(0.8088)\n",
            "batches_trained: 427 total_loss: tensor(0.0124) acc: tensor(0.8088)\n",
            "batches_trained: 428 total_loss: tensor(0.0124) acc: tensor(0.8090)\n",
            "batches_trained: 429 total_loss: tensor(0.0124) acc: tensor(0.8092)\n",
            "batches_trained: 430 total_loss: tensor(0.0124) acc: tensor(0.8092)\n",
            "batches_trained: 431 total_loss: tensor(0.0124) acc: tensor(0.8091)\n",
            "batches_trained: 432 total_loss: tensor(0.0124) acc: tensor(0.8092)\n",
            "batches_trained: 433 total_loss: tensor(0.0124) acc: tensor(0.8093)\n",
            "batches_trained: 434 total_loss: tensor(0.0124) acc: tensor(0.8090)\n",
            "batches_trained: 435 total_loss: tensor(0.0124) acc: tensor(0.8091)\n",
            "batches_trained: 436 total_loss: tensor(0.0124) acc: tensor(0.8093)\n",
            "batches_trained: 437 total_loss: tensor(0.0124) acc: tensor(0.8095)\n",
            "batches_trained: 438 total_loss: tensor(0.0124) acc: tensor(0.8096)\n",
            "batches_trained: 439 total_loss: tensor(0.0124) acc: tensor(0.8095)\n",
            "batches_trained: 440 total_loss: tensor(0.0124) acc: tensor(0.8098)\n",
            "batches_trained: 441 total_loss: tensor(0.0124) acc: tensor(0.8098)\n",
            "batches_trained: 442 total_loss: tensor(0.0124) acc: tensor(0.8101)\n",
            "batches_trained: 443 total_loss: tensor(0.0124) acc: tensor(0.8101)\n",
            "batches_trained: 444 total_loss: tensor(0.0124) acc: tensor(0.8098)\n",
            "batches_trained: 445 total_loss: tensor(0.0124) acc: tensor(0.8097)\n",
            "batches_trained: 446 total_loss: tensor(0.0124) acc: tensor(0.8100)\n",
            "batches_trained: 447 total_loss: tensor(0.0124) acc: tensor(0.8099)\n",
            "batches_trained: 448 total_loss: tensor(0.0124) acc: tensor(0.8100)\n",
            "batches_trained: 449 total_loss: tensor(0.0123) acc: tensor(0.8103)\n",
            "batches_trained: 450 total_loss: tensor(0.0124) acc: tensor(0.8101)\n",
            "batches_trained: 451 total_loss: tensor(0.0123) acc: tensor(0.8103)\n",
            "batches_trained: 452 total_loss: tensor(0.0123) acc: tensor(0.8104)\n",
            "batches_trained: 453 total_loss: tensor(0.0123) acc: tensor(0.8104)\n",
            "batches_trained: 454 total_loss: tensor(0.0123) acc: tensor(0.8104)\n",
            "batches_trained: 455 total_loss: tensor(0.0123) acc: tensor(0.8106)\n",
            "batches_trained: 456 total_loss: tensor(0.0123) acc: tensor(0.8105)\n",
            "batches_trained: 457 total_loss: tensor(0.0123) acc: tensor(0.8105)\n",
            "batches_trained: 458 total_loss: tensor(0.0123) acc: tensor(0.8107)\n",
            "batches_trained: 459 total_loss: tensor(0.0123) acc: tensor(0.8105)\n",
            "batches_trained: 460 total_loss: tensor(0.0123) acc: tensor(0.8107)\n",
            "batches_trained: 461 total_loss: tensor(0.0123) acc: tensor(0.8105)\n",
            "batches_trained: 462 total_loss: tensor(0.0123) acc: tensor(0.8105)\n",
            "batches_trained: 463 total_loss: tensor(0.0123) acc: tensor(0.8107)\n",
            "batches_trained: 464 total_loss: tensor(0.0123) acc: tensor(0.8105)\n",
            "batches_trained: 465 total_loss: tensor(0.0123) acc: tensor(0.8108)\n",
            "batches_trained: 466 total_loss: tensor(0.0123) acc: tensor(0.8107)\n",
            "batches_trained: 467 total_loss: tensor(0.0123) acc: tensor(0.8108)\n",
            "batches_trained: 468 total_loss: tensor(0.0123) acc: tensor(0.8106)\n",
            "batches_trained: 469 total_loss: tensor(0.0123) acc: tensor(0.8106)\n",
            "batches_trained: 470 total_loss: tensor(0.0123) acc: tensor(0.8108)\n",
            "batches_trained: 471 total_loss: tensor(0.0123) acc: tensor(0.8106)\n",
            "batches_trained: 472 total_loss: tensor(0.0123) acc: tensor(0.8108)\n",
            "batches_trained: 473 total_loss: tensor(0.0123) acc: tensor(0.8110)\n",
            "batches_trained: 474 total_loss: tensor(0.0123) acc: tensor(0.8113)\n",
            "batches_trained: 475 total_loss: tensor(0.0123) acc: tensor(0.8114)\n",
            "batches_trained: 476 total_loss: tensor(0.0123) acc: tensor(0.8113)\n",
            "batches_trained: 477 total_loss: tensor(0.0123) acc: tensor(0.8113)\n",
            "batches_trained: 478 total_loss: tensor(0.0123) acc: tensor(0.8113)\n",
            "batches_trained: 479 total_loss: tensor(0.0123) acc: tensor(0.8113)\n",
            "batches_trained: 480 total_loss: tensor(0.0123) acc: tensor(0.8113)\n",
            "batches_trained: 481 total_loss: tensor(0.0123) acc: tensor(0.8115)\n",
            "batches_trained: 482 total_loss: tensor(0.0123) acc: tensor(0.8115)\n",
            "batches_trained: 483 total_loss: tensor(0.0123) acc: tensor(0.8116)\n",
            "batches_trained: 484 total_loss: tensor(0.0123) acc: tensor(0.8117)\n",
            "batches_trained: 485 total_loss: tensor(0.0123) acc: tensor(0.8117)\n",
            "batches_trained: 486 total_loss: tensor(0.0122) acc: tensor(0.8119)\n",
            "batches_trained: 487 total_loss: tensor(0.0122) acc: tensor(0.8121)\n",
            "batches_trained: 488 total_loss: tensor(0.0122) acc: tensor(0.8122)\n",
            "batches_trained: 489 total_loss: tensor(0.0122) acc: tensor(0.8122)\n",
            "batches_trained: 490 total_loss: tensor(0.0122) acc: tensor(0.8122)\n",
            "batches_trained: 491 total_loss: tensor(0.0122) acc: tensor(0.8123)\n",
            "batches_trained: 492 total_loss: tensor(0.0122) acc: tensor(0.8123)\n",
            "batches_trained: 493 total_loss: tensor(0.0122) acc: tensor(0.8125)\n",
            "batches_trained: 494 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 495 total_loss: tensor(0.0122) acc: tensor(0.8125)\n",
            "batches_trained: 496 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 497 total_loss: tensor(0.0122) acc: tensor(0.8128)\n",
            "batches_trained: 498 total_loss: tensor(0.0122) acc: tensor(0.8127)\n",
            "batches_trained: 499 total_loss: tensor(0.0122) acc: tensor(0.8128)\n",
            "batches_trained: 500 total_loss: tensor(0.0122) acc: tensor(0.8127)\n",
            "batches_trained: 501 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 502 total_loss: tensor(0.0122) acc: tensor(0.8124)\n",
            "batches_trained: 503 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 504 total_loss: tensor(0.0122) acc: tensor(0.8127)\n",
            "batches_trained: 505 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 506 total_loss: tensor(0.0122) acc: tensor(0.8125)\n",
            "batches_trained: 507 total_loss: tensor(0.0122) acc: tensor(0.8127)\n",
            "batches_trained: 508 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 509 total_loss: tensor(0.0122) acc: tensor(0.8125)\n",
            "batches_trained: 510 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 511 total_loss: tensor(0.0122) acc: tensor(0.8124)\n",
            "batches_trained: 512 total_loss: tensor(0.0122) acc: tensor(0.8123)\n",
            "batches_trained: 513 total_loss: tensor(0.0122) acc: tensor(0.8122)\n",
            "batches_trained: 514 total_loss: tensor(0.0122) acc: tensor(0.8123)\n",
            "batches_trained: 515 total_loss: tensor(0.0122) acc: tensor(0.8124)\n",
            "batches_trained: 516 total_loss: tensor(0.0122) acc: tensor(0.8124)\n",
            "batches_trained: 517 total_loss: tensor(0.0122) acc: tensor(0.8125)\n",
            "batches_trained: 518 total_loss: tensor(0.0122) acc: tensor(0.8125)\n",
            "batches_trained: 519 total_loss: tensor(0.0122) acc: tensor(0.8127)\n",
            "batches_trained: 520 total_loss: tensor(0.0122) acc: tensor(0.8128)\n",
            "batches_trained: 521 total_loss: tensor(0.0122) acc: tensor(0.8127)\n",
            "batches_trained: 522 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 523 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 524 total_loss: tensor(0.0122) acc: tensor(0.8126)\n",
            "batches_trained: 525 total_loss: tensor(0.0122) acc: tensor(0.8129)\n",
            "batches_trained: 526 total_loss: tensor(0.0122) acc: tensor(0.8130)\n",
            "batches_trained: 527 total_loss: tensor(0.0122) acc: tensor(0.8129)\n",
            "batches_trained: 528 total_loss: tensor(0.0122) acc: tensor(0.8129)\n",
            "batches_trained: 529 total_loss: tensor(0.0122) acc: tensor(0.8130)\n",
            "batches_trained: 530 total_loss: tensor(0.0122) acc: tensor(0.8130)\n",
            "batches_trained: 531 total_loss: tensor(0.0122) acc: tensor(0.8130)\n",
            "batches_trained: 532 total_loss: tensor(0.0122) acc: tensor(0.8130)\n",
            "batches_trained: 533 total_loss: tensor(0.0122) acc: tensor(0.8129)\n",
            "batches_trained: 534 total_loss: tensor(0.0122) acc: tensor(0.8131)\n",
            "batches_trained: 535 total_loss: tensor(0.0122) acc: tensor(0.8132)\n",
            "batches_trained: 536 total_loss: tensor(0.0122) acc: tensor(0.8133)\n",
            "batches_trained: 537 total_loss: tensor(0.0122) acc: tensor(0.8134)\n",
            "batches_trained: 538 total_loss: tensor(0.0122) acc: tensor(0.8134)\n",
            "batches_trained: 539 total_loss: tensor(0.0122) acc: tensor(0.8135)\n",
            "batches_trained: 540 total_loss: tensor(0.0122) acc: tensor(0.8135)\n",
            "batches_trained: 541 total_loss: tensor(0.0121) acc: tensor(0.8137)\n",
            "batches_trained: 542 total_loss: tensor(0.0121) acc: tensor(0.8139)\n",
            "batches_trained: 543 total_loss: tensor(0.0121) acc: tensor(0.8139)\n",
            "batches_trained: 544 total_loss: tensor(0.0121) acc: tensor(0.8142)\n",
            "batches_trained: 545 total_loss: tensor(0.0121) acc: tensor(0.8144)\n",
            "batches_trained: 546 total_loss: tensor(0.0121) acc: tensor(0.8147)\n",
            "batches_trained: 547 total_loss: tensor(0.0121) acc: tensor(0.8148)\n",
            "batches_trained: 548 total_loss: tensor(0.0121) acc: tensor(0.8149)\n",
            "batches_trained: 549 total_loss: tensor(0.0121) acc: tensor(0.8149)\n",
            "batches_trained: 550 total_loss: tensor(0.0121) acc: tensor(0.8150)\n",
            "batches_trained: 551 total_loss: tensor(0.0121) acc: tensor(0.8148)\n",
            "batches_trained: 552 total_loss: tensor(0.0121) acc: tensor(0.8147)\n",
            "batches_trained: 553 total_loss: tensor(0.0121) acc: tensor(0.8147)\n",
            "batches_trained: 554 total_loss: tensor(0.0121) acc: tensor(0.8149)\n",
            "batches_trained: 555 total_loss: tensor(0.0121) acc: tensor(0.8149)\n",
            "batches_trained: 556 total_loss: tensor(0.0121) acc: tensor(0.8149)\n",
            "batches_trained: 557 total_loss: tensor(0.0121) acc: tensor(0.8151)\n",
            "batches_trained: 558 total_loss: tensor(0.0121) acc: tensor(0.8152)\n",
            "batches_trained: 559 total_loss: tensor(0.0121) acc: tensor(0.8152)\n",
            "batches_trained: 560 total_loss: tensor(0.0121) acc: tensor(0.8154)\n",
            "batches_trained: 561 total_loss: tensor(0.0121) acc: tensor(0.8155)\n",
            "batches_trained: 562 total_loss: tensor(0.0121) acc: tensor(0.8155)\n",
            "batches_trained: 563 total_loss: tensor(0.0121) acc: tensor(0.8157)\n",
            "batches_trained: 564 total_loss: tensor(0.0120) acc: tensor(0.8158)\n",
            "batches_trained: 565 total_loss: tensor(0.0120) acc: tensor(0.8159)\n",
            "batches_trained: 566 total_loss: tensor(0.0120) acc: tensor(0.8161)\n",
            "batches_trained: 567 total_loss: tensor(0.0120) acc: tensor(0.8162)\n",
            "batches_trained: 568 total_loss: tensor(0.0120) acc: tensor(0.8162)\n",
            "batches_trained: 569 total_loss: tensor(0.0120) acc: tensor(0.8162)\n",
            "batches_trained: 570 total_loss: tensor(0.0120) acc: tensor(0.8163)\n",
            "batches_trained: 571 total_loss: tensor(0.0120) acc: tensor(0.8162)\n",
            "batches_trained: 572 total_loss: tensor(0.0120) acc: tensor(0.8163)\n",
            "batches_trained: 573 total_loss: tensor(0.0120) acc: tensor(0.8165)\n",
            "batches_trained: 574 total_loss: tensor(0.0120) acc: tensor(0.8164)\n",
            "batches_trained: 575 total_loss: tensor(0.0120) acc: tensor(0.8164)\n",
            "batches_trained: 576 total_loss: tensor(0.0120) acc: tensor(0.8166)\n",
            "batches_trained: 577 total_loss: tensor(0.0120) acc: tensor(0.8165)\n",
            "batches_trained: 578 total_loss: tensor(0.0120) acc: tensor(0.8163)\n",
            "batches_trained: 579 total_loss: tensor(0.0120) acc: tensor(0.8164)\n",
            "batches_trained: 580 total_loss: tensor(0.0120) acc: tensor(0.8165)\n",
            "batches_trained: 581 total_loss: tensor(0.0120) acc: tensor(0.8166)\n",
            "batches_trained: 582 total_loss: tensor(0.0120) acc: tensor(0.8168)\n",
            "batches_trained: 583 total_loss: tensor(0.0120) acc: tensor(0.8168)\n",
            "batches_trained: 584 total_loss: tensor(0.0120) acc: tensor(0.8170)\n",
            "batches_trained: 585 total_loss: tensor(0.0120) acc: tensor(0.8169)\n",
            "batches_trained: 586 total_loss: tensor(0.0120) acc: tensor(0.8170)\n",
            "batches_trained: 587 total_loss: tensor(0.0120) acc: tensor(0.8170)\n",
            "batches_trained: 588 total_loss: tensor(0.0120) acc: tensor(0.8170)\n",
            "batches_trained: 589 total_loss: tensor(0.0120) acc: tensor(0.8170)\n",
            "batches_trained: 590 total_loss: tensor(0.0120) acc: tensor(0.8172)\n",
            "batches_trained: 591 total_loss: tensor(0.0120) acc: tensor(0.8173)\n",
            "batches_trained: 592 total_loss: tensor(0.0120) acc: tensor(0.8173)\n",
            "batches_trained: 593 total_loss: tensor(0.0120) acc: tensor(0.8175)\n",
            "batches_trained: 594 total_loss: tensor(0.0120) acc: tensor(0.8176)\n",
            "batches_trained: 595 total_loss: tensor(0.0120) acc: tensor(0.8175)\n",
            "batches_trained: 596 total_loss: tensor(0.0120) acc: tensor(0.8175)\n",
            "batches_trained: 597 total_loss: tensor(0.0120) acc: tensor(0.8176)\n",
            "batches_trained: 598 total_loss: tensor(0.0120) acc: tensor(0.8176)\n",
            "batches_trained: 599 total_loss: tensor(0.0120) acc: tensor(0.8178)\n",
            "batches_trained: 600 total_loss: tensor(0.0120) acc: tensor(0.8180)\n",
            "batches_trained: 601 total_loss: tensor(0.0120) acc: tensor(0.8180)\n",
            "batches_trained: 602 total_loss: tensor(0.0120) acc: tensor(0.8181)\n",
            "batches_trained: 603 total_loss: tensor(0.0120) acc: tensor(0.8180)\n",
            "batches_trained: 604 total_loss: tensor(0.0120) acc: tensor(0.8179)\n",
            "batches_trained: 605 total_loss: tensor(0.0120) acc: tensor(0.8180)\n",
            "batches_trained: 606 total_loss: tensor(0.0120) acc: tensor(0.8181)\n",
            "batches_trained: 607 total_loss: tensor(0.0120) acc: tensor(0.8181)\n",
            "batches_trained: 608 total_loss: tensor(0.0120) acc: tensor(0.8181)\n",
            "batches_trained: 609 total_loss: tensor(0.0120) acc: tensor(0.8180)\n",
            "batches_trained: 610 total_loss: tensor(0.0120) acc: tensor(0.8181)\n",
            "batches_trained: 611 total_loss: tensor(0.0120) acc: tensor(0.8182)\n",
            "batches_trained: 612 total_loss: tensor(0.0120) acc: tensor(0.8183)\n",
            "batches_trained: 613 total_loss: tensor(0.0120) acc: tensor(0.8184)\n",
            "batches_trained: 614 total_loss: tensor(0.0120) acc: tensor(0.8184)\n",
            "batches_trained: 615 total_loss: tensor(0.0120) acc: tensor(0.8185)\n",
            "batches_trained: 616 total_loss: tensor(0.0120) acc: tensor(0.8186)\n",
            "batches_trained: 617 total_loss: tensor(0.0120) acc: tensor(0.8187)\n",
            "batches_trained: 618 total_loss: tensor(0.0120) acc: tensor(0.8187)\n",
            "batches_trained: 619 total_loss: tensor(0.0120) acc: tensor(0.8188)\n",
            "batches_trained: 620 total_loss: tensor(0.0120) acc: tensor(0.8189)\n",
            "batches_trained: 621 total_loss: tensor(0.0120) acc: tensor(0.8188)\n",
            "batches_trained: 622 total_loss: tensor(0.0120) acc: tensor(0.8188)\n",
            "batches_trained: 623 total_loss: tensor(0.0120) acc: tensor(0.8188)\n",
            "batches_trained: 624 total_loss: tensor(0.0120) acc: tensor(0.8188)\n",
            "batches_trained: 625 total_loss: tensor(0.0120) acc: tensor(0.8189)\n",
            "batches_trained: 626 total_loss: tensor(0.0120) acc: tensor(0.8191)\n",
            "batches_trained: 627 total_loss: tensor(0.0119) acc: tensor(0.8191)\n",
            "batches_trained: 628 total_loss: tensor(0.0119) acc: tensor(0.8192)\n",
            "batches_trained: 629 total_loss: tensor(0.0119) acc: tensor(0.8193)\n",
            "batches_trained: 630 total_loss: tensor(0.0119) acc: tensor(0.8194)\n",
            "batches_trained: 631 total_loss: tensor(0.0119) acc: tensor(0.8195)\n",
            "batches_trained: 632 total_loss: tensor(0.0119) acc: tensor(0.8197)\n",
            "batches_trained: 633 total_loss: tensor(0.0119) acc: tensor(0.8198)\n",
            "batches_trained: 634 total_loss: tensor(0.0119) acc: tensor(0.8200)\n",
            "batches_trained: 635 total_loss: tensor(0.0119) acc: tensor(0.8200)\n",
            "batches_trained: 636 total_loss: tensor(0.0119) acc: tensor(0.8201)\n",
            "batches_trained: 637 total_loss: tensor(0.0119) acc: tensor(0.8200)\n",
            "batches_trained: 638 total_loss: tensor(0.0119) acc: tensor(0.8199)\n",
            "batches_trained: 639 total_loss: tensor(0.0119) acc: tensor(0.8200)\n",
            "batches_trained: 640 total_loss: tensor(0.0119) acc: tensor(0.8199)\n",
            "batches_trained: 641 total_loss: tensor(0.0119) acc: tensor(0.8199)\n",
            "batches_trained: 642 total_loss: tensor(0.0119) acc: tensor(0.8200)\n",
            "batches_trained: 643 total_loss: tensor(0.0119) acc: tensor(0.8202)\n",
            "batches_trained: 644 total_loss: tensor(0.0119) acc: tensor(0.8204)\n",
            "batches_trained: 645 total_loss: tensor(0.0119) acc: tensor(0.8202)\n",
            "batches_trained: 646 total_loss: tensor(0.0119) acc: tensor(0.8202)\n",
            "batches_trained: 647 total_loss: tensor(0.0119) acc: tensor(0.8202)\n",
            "batches_trained: 648 total_loss: tensor(0.0119) acc: tensor(0.8203)\n",
            "batches_trained: 649 total_loss: tensor(0.0119) acc: tensor(0.8203)\n",
            "batches_trained: 650 total_loss: tensor(0.0119) acc: tensor(0.8202)\n",
            "batches_trained: 651 total_loss: tensor(0.0119) acc: tensor(0.8203)\n",
            "batches_trained: 652 total_loss: tensor(0.0119) acc: tensor(0.8204)\n",
            "batches_trained: 653 total_loss: tensor(0.0119) acc: tensor(0.8205)\n",
            "batches_trained: 654 total_loss: tensor(0.0119) acc: tensor(0.8206)\n",
            "batches_trained: 655 total_loss: tensor(0.0119) acc: tensor(0.8205)\n",
            "batches_trained: 656 total_loss: tensor(0.0119) acc: tensor(0.8206)\n",
            "batches_trained: 657 total_loss: tensor(0.0119) acc: tensor(0.8207)\n",
            "batches_trained: 658 total_loss: tensor(0.0119) acc: tensor(0.8207)\n",
            "batches_trained: 659 total_loss: tensor(0.0119) acc: tensor(0.8206)\n",
            "batches_trained: 660 total_loss: tensor(0.0119) acc: tensor(0.8207)\n",
            "batches_trained: 661 total_loss: tensor(0.0119) acc: tensor(0.8206)\n",
            "batches_trained: 662 total_loss: tensor(0.0119) acc: tensor(0.8208)\n",
            "batches_trained: 663 total_loss: tensor(0.0119) acc: tensor(0.8210)\n",
            "batches_trained: 664 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 665 total_loss: tensor(0.0119) acc: tensor(0.8208)\n",
            "batches_trained: 666 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 667 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 668 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 669 total_loss: tensor(0.0119) acc: tensor(0.8210)\n",
            "batches_trained: 670 total_loss: tensor(0.0119) acc: tensor(0.8211)\n",
            "batches_trained: 671 total_loss: tensor(0.0119) acc: tensor(0.8210)\n",
            "batches_trained: 672 total_loss: tensor(0.0119) acc: tensor(0.8211)\n",
            "batches_trained: 673 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 674 total_loss: tensor(0.0119) acc: tensor(0.8208)\n",
            "batches_trained: 675 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 676 total_loss: tensor(0.0119) acc: tensor(0.8209)\n",
            "batches_trained: 677 total_loss: tensor(0.0119) acc: tensor(0.8210)\n",
            "batches_trained: 678 total_loss: tensor(0.0119) acc: tensor(0.8210)\n",
            "batches_trained: 679 total_loss: tensor(0.0119) acc: tensor(0.8210)\n",
            "batches_trained: 680 total_loss: tensor(0.0119) acc: tensor(0.8211)\n",
            "batches_trained: 681 total_loss: tensor(0.0119) acc: tensor(0.8212)\n",
            "batches_trained: 682 total_loss: tensor(0.0119) acc: tensor(0.8212)\n",
            "batches_trained: 683 total_loss: tensor(0.0119) acc: tensor(0.8213)\n",
            "batches_trained: 684 total_loss: tensor(0.0119) acc: tensor(0.8212)\n",
            "batches_trained: 685 total_loss: tensor(0.0119) acc: tensor(0.8212)\n",
            "batches_trained: 686 total_loss: tensor(0.0119) acc: tensor(0.8212)\n",
            "batches_trained: 687 total_loss: tensor(0.0119) acc: tensor(0.8213)\n",
            "batches_trained: 688 total_loss: tensor(0.0119) acc: tensor(0.8214)\n",
            "batches_trained: 689 total_loss: tensor(0.0119) acc: tensor(0.8215)\n",
            "batches_trained: 690 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 691 total_loss: tensor(0.0119) acc: tensor(0.8217)\n",
            "batches_trained: 692 total_loss: tensor(0.0119) acc: tensor(0.8218)\n",
            "batches_trained: 693 total_loss: tensor(0.0118) acc: tensor(0.8219)\n",
            "batches_trained: 694 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 695 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 696 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 697 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 698 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 699 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 700 total_loss: tensor(0.0119) acc: tensor(0.8216)\n",
            "batches_trained: 701 total_loss: tensor(0.0119) acc: tensor(0.8217)\n",
            "batches_trained: 702 total_loss: tensor(0.0119) acc: tensor(0.8217)\n",
            "batches_trained: 703 total_loss: tensor(0.0119) acc: tensor(0.8218)\n",
            "batches_trained: 704 total_loss: tensor(0.0118) acc: tensor(0.8219)\n",
            "batches_trained: 705 total_loss: tensor(0.0119) acc: tensor(0.8219)\n",
            "batches_trained: 706 total_loss: tensor(0.0118) acc: tensor(0.8219)\n",
            "batches_trained: 707 total_loss: tensor(0.0118) acc: tensor(0.8221)\n",
            "batches_trained: 708 total_loss: tensor(0.0118) acc: tensor(0.8219)\n",
            "batches_trained: 709 total_loss: tensor(0.0118) acc: tensor(0.8218)\n",
            "batches_trained: 710 total_loss: tensor(0.0118) acc: tensor(0.8219)\n",
            "batches_trained: 711 total_loss: tensor(0.0118) acc: tensor(0.8217)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b0861f0965fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batches_trained: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" total_loss: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" acc: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch completed: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" total_loss: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" acc: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDaKkmLFhs5W"
      },
      "source": [
        "torch.save(model.state_dict(), '/content/gdrive/My Drive/NLPlay with Transformers/Sentiment analysis/model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4WoDPODijv0"
      },
      "source": [
        "batch=0\n",
        "for d in test_dataloader:\n",
        "  batch+=1\n",
        "  with torch.no_grad():\n",
        "    input_ids = d[\"input_ids\"]\n",
        "    attention_mask = d[\"attention_mask\"]\n",
        "    target = d[\"targets\"]\n",
        "    target=target.to(torch.float32)\n",
        "    output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    output = (output>0.5).float()\n",
        "    correct += ((output[:,0] == target).float()).sum()\n",
        "print(\"acc: \"+str(correct/()))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrFx3WZzjHi5"
      },
      "source": [
        "print(\"acc: \"+str(correct/(batch*batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}